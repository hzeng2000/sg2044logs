开始测试...
日志文件: /root/wmw/hzeng_vw/riscv_kernels/e2e/rvv10/out/phase_silu/benchmark_silu_20251221_082408.log
===========================================
正在处理变体目录: base
-------------------------------------------
执行时间: 2025-12-21 08:24:08
测试配置: [变体: base] [模型: Qwen3-0.6B-Q4_0.gguf] [线程: 64] [第 1 次运行]
命令: ./llama-perplexity -m /root/wmw/unsloth_riscv/Qwen3-0.6B-GGUF/Qwen3-0.6B-Q4_0.gguf -t 64 -f /root/wmw/hzeng_vw/riscv_kernels/e2e/rvv10/../wiki.test.raw
build: 0 (unknown) with riscv64-unknown-linux-gnu-gcc () 15.1.0 for riscv64-unknown-linux-gnu
llama_model_loader: loaded meta data with 32 key-value pairs and 310 tensors from /root/wmw/unsloth_riscv/Qwen3-0.6B-GGUF/Qwen3-0.6B-Q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3-0.6B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3-0.6B
llama_model_loader: - kv   4:                       general.quantized_by str              = Unsloth
llama_model_loader: - kv   5:                         general.size_label str              = 0.6B
llama_model_loader: - kv   6:                           general.repo_url str              = https://huggingface.co/unsloth
llama_model_loader: - kv   7:                          qwen3.block_count u32              = 28
llama_model_loader: - kv   8:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   9:                     qwen3.embedding_length u32              = 1024
llama_model_loader: - kv  10:                  qwen3.feed_forward_length u32              = 3072
llama_model_loader: - kv  11:                 qwen3.attention.head_count u32              = 16
llama_model_loader: - kv  12:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  13:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  14:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  15:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  16:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  22:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 151654
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 2
llama_model_loader: - kv  28:                      quantize.imatrix.file str              = Qwen3-0.6B-GGUF/imatrix_unsloth.dat
llama_model_loader: - kv  29:                   quantize.imatrix.dataset str              = unsloth_calibration_Qwen3-0.6B.txt
llama_model_loader: - kv  30:             quantize.imatrix.entries_count u32              = 196
llama_model_loader: - kv  31:              quantize.imatrix.chunks_count u32              = 688
llama_model_loader: - type  f32:  113 tensors
llama_model_loader: - type q4_0:  193 tensors
llama_model_loader: - type q4_1:    3 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 358.78 MiB (5.05 BPW) 
load: printing all EOG tokens:
load:   - 151643 ('<|endoftext|>')
load:   - 151645 ('<|im_end|>')
load:   - 151662 ('<|fim_pad|>')
load:   - 151663 ('<|repo_name|>')
load:   - 151664 ('<|file_sep|>')
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 1024
print_info: n_layer          = 28
print_info: n_head           = 16
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 2
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 3072
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: model type       = 0.6B
print_info: model params     = 596.05 M
print_info: general.name     = Qwen3-0.6B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 11 ','
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151654 '<|vision_pad|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors:   CPU_Mapped model buffer size =   358.78 MiB
....................................................................
llama_context: constructing llama_context
llama_context: n_seq_max     = 4
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 512
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (512) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     2.32 MiB
llama_kv_cache_unified:        CPU KV buffer size =   224.00 MiB
llama_kv_cache_unified: size =  224.00 MiB (   512 cells,  28 layers,  4/4 seqs), K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:        CPU compute buffer size =   298.75 MiB
llama_context: graph nodes  = 1098
llama_context: graph splits = 1
common_init_from_params: added <|endoftext|> logit bias = -inf
common_init_from_params: added <|im_end|> logit bias = -inf
common_init_from_params: added <|fim_pad|> logit bias = -inf
common_init_from_params: added <|repo_name|> logit bias = -inf
common_init_from_params: added <|file_sep|> logit bias = -inf
common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)

system_info: n_threads = 64 (n_threads_batch = 64) / 64 | CPU : LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 2996.35 ms
perplexity: calculating perplexity over 584 chunks, n_ctx=512, batch_size=2048, n_seq=4
perplexity: 30.74 seconds per pass - ETA 1 hours 14.78 minutes
[1]14.0791,[2]19.2608,[3]19.6351,[4]20.3777,[5]20.5197,[6]21.0242,[7]21.2841,[8]23.0447,[9]24.5809,[10]25.5438,[11]26.0423,[12]26.3236,[13]27.1228,[14]26.7507,[15]26.4454,[16]27.2578,[17]25.1982,[18]25.6607,[19]25.2555,[20]25.5063,[21]24.8639,[22]24.9051,[23]23.7934,[24]22.5578,[25]22.0407,[26]21.5551,[27]20.8476,[28]20.4232,[29]20.6270,[30]20.6640,[31]20.3872,[32]20.4859,[33]20.0951,[34]20.3197,[35]20.5245,[36]20.8829,[37]21.3396,[38]21.4920,[39]21.2851,[40]21.3797,[41]21.3139,[42]21.1910,[43]21.4225,[44]21.4958,[45]21.5972,[46]21.5534,[47]22.0340,[48]22.2870,[49]22.1510,[50]22.3654,[51]22.4106,[52]22.4300,[53]22.4877,[54]22.6652,[55]22.6513,[56]22.7807,[57]22.7003,[58]22.7819,[59]22.9330,[60]23.0611,[61]23.0478,[62]23.1409,[63]23.3266,[64]23.5113,[65]23.7862,[66]24.0034,[67]24.2344,[68]24.1324,[69]24.1404,[70]24.1713,[71]24.1919,[72]24.3575,[73]24.4174,[74]24.4221,[75]24.2770,[76]24.1625,[77]24.1506,[78]24.1013,[79]23.9118,[80]23.8686,[81]23.7271,[82]23.8268,[83]23.7844,[84]23.7111,[85]23.7385,[86]23.9506,[87]24.1354,[88]24.0655,[89]24.0056,[90]23.9463,[91]24.0409,[92]23.9601,[93]24.0966,[94]24.2119,[95]24.1409,[96]24.2023,[97]24.1642,[98]24.1751,[99]24.1035,[100]24.2533,[101]24.3399,[102]24.2921,[103]24.3586,[104]24.2486,[105]24.2866,[106]24.1631,[107]24.1897,[108]24.3205,[109]24.4024,[110]24.5182,[111]24.8033,[112]24.8715,[113]24.7232,[114]24.8331,[115]24.8722,[116]24.8027,[117]24.7968,[118]24.7539,[119]24.6315,[120]24.6834,[121]24.6393,[122]24.6809,[123]24.6279,[124]24.4992,[125]24.4404,[126]24.4197,[127]24.3708,[128]24.3043,[129]24.2849,[130]24.1978,[131]24.0498,[132]23.9661,[133]23.8976,[134]23.9092,[135]23.8962,[136]23.8616,[137]23.7375,[138]23.6139,[139]23.6425,[140]23.5705,[141]23.5534,[142]23.5416,[143]23.5941,[144]23.6491,[145]23.5949,[146]23.4375,[147]23.3073,[148]23.2036,[149]23.1374,[150]22.9971,[151]22.9578,[152]22.9475,[153]22.9100,[154]22.7842,[155]22.8186,[156]22.7541,[157]22.7010,[158]22.6011,[159]22.5554,[160]22.5077,[161]22.4902,[162]22.4930,[163]22.4913,[164]22.4910,[165]22.4613,[166]22.4839,[167]22.3951,[168]22.4435,[169]22.4097,[170]22.5523,[171]22.6369,[172]22.7496,[173]22.8777,[174]22.9185,[175]23.0161,[176]23.1207,[177]23.2276,[178]23.3162,[179]23.3745,[180]23.4064,[181]23.5003,[182]23.5878,[183]23.6689,[184]23.7688,[185]23.8332,[186]23.8454,[187]23.8422,[188]23.8543,[189]23.8760,[190]23.9423,[191]23.9546,[192]23.9783,[193]23.9732,[194]24.0365,[195]24.0638,[196]24.0956,[197]24.0988,[198]24.0178,[199]23.9692,[200]23.9571,[201]23.9989,[202]24.0341,[203]24.0758,[204]24.0900,[205]24.0837,[206]24.0465,[207]24.0983,[208]24.0130,[209]24.0025,[210]24.0030,[211]23.9948,[212]23.9933,[213]24.0021,[214]23.9452,[215]23.8752,[216]23.8843,[217]23.8882,[218]23.8482,[219]23.7671,[220]23.7108,[221]23.6592,[222]23.6167,[223]23.6138,[224]23.6237,[225]23.5619,[226]23.5695,[227]23.5392,[228]23.4666,[229]23.3753,[230]23.3121,[231]23.2596,[232]23.2175,[233]23.2287,[234]23.2430,[235]23.2191,[236]23.1587,[237]23.1203,[238]23.0360,[239]23.0001,[240]23.0220,[241]22.9967,[242]23.0212,[243]23.0378,[244]23.0766,[245]23.0824,[246]23.1444,[247]23.1420,[248]23.1675,[249]23.1913,[250]23.1925,[251]23.2632,[252]23.2666,[253]23.3251,[254]23.3630,[255]23.3730,[256]23.4351,[257]23.4435,[258]23.3881,[259]23.3460,[260]23.3167,[261]23.2765,[262]23.2577,[263]23.2427,[264]23.2445,[265]23.2972,[266]23.3243,[267]23.3297,[268]23.2995,[269]23.2950,[270]23.3078,[271]23.2668,[272]23.2755,[273]23.2748,[274]23.2641,[275]23.2618,[276]23.2103,[277]23.1798,[278]23.2094,[279]23.2053,[280]23.1915,[281]23.1821,[282]23.2569,[283]23.1636,[284]23.0587,[285]23.0695,[286]22.9987,[287]22.9187,[288]22.9331,[289]22.9332,[290]23.0108,[291]23.0095,[292]23.0024,[293]23.0041,[294]23.0547,[295]23.1135,[296]23.1682,[297]23.2189,[298]23.2176,[299]23.1743,[300]23.1782,[301]23.1555,[302]23.1597,[303]23.1447,[304]23.1702,[305]23.1503,[306]23.1326,[307]23.1423,[308]23.1217,[309]23.1128,[310]23.1508,[311]23.1589,[312]23.1143,[313]23.1079,[314]23.1388,[315]23.0984,[316]23.1527,[317]23.2123,[318]23.2053,[319]23.1790,[320]23.1942,[321]23.1381,[322]23.1556,[323]23.1950,[324]23.2243,[325]23.2651,[326]23.2733,[327]23.2313,[328]23.2271,[329]23.1609,[330]23.1343,[331]23.0913,[332]23.0788,[333]23.0749,[334]23.0425,[335]22.9919,[336]22.9698,[337]22.9925,[338]23.0170,[339]22.9947,[340]22.9560,[341]22.9248,[342]22.9212,[343]22.9145,[344]22.9375,[345]22.9555,[346]22.9564,[347]22.9398,[348]22.9496,[349]22.9388,[350]22.9422,[351]22.9653,[352]22.9720,[353]22.9717,[354]22.9160,[355]22.9594,[356]22.9742,[357]22.9822,[358]22.9521,[359]22.9488,[360]22.9439,[361]22.9501,[362]22.9767,[363]22.9695,[364]23.0124,[365]23.0395,[366]23.1123,[367]23.1670,[368]23.2398,[369]23.2990,[370]23.3362,[371]23.3899,[372]23.4451,[373]23.4666,[374]23.4947,[375]23.5546,[376]23.5958,[377]23.6231,[378]23.6757,[379]23.7178,[380]23.7659,[381]23.7905,[382]23.8252,[383]23.8550,[384]23.8982,[385]23.9641,[386]24.0048,[387]23.9980,[388]23.9757,[389]24.0165,[390]24.0714,[391]24.1164,[392]24.1131,[393]24.1082,[394]24.0869,[395]24.0932,[396]24.1138,[397]24.1220,[398]24.1285,[399]24.1325,[400]24.1714,[401]24.1673,[402]24.1711,[403]24.1472,[404]24.1480,[405]24.1257,[406]24.1186,[407]24.1258,[408]24.1396,[409]24.1412,[410]24.1401,[411]24.1807,[412]24.1929,[413]24.2067,[414]24.2052,[415]24.2017,[416]24.1968,[417]24.2071,[418]24.2194,[419]24.2109,[420]24.2065,[421]24.2147,[422]24.1766,[423]24.1893,[424]24.1995,[425]24.2095,[426]24.2333,[427]24.2676,[428]24.3143,[429]24.3210,[430]24.2987,[431]24.2641,[432]24.2639,[433]24.2576,[434]24.2390,[435]24.2546,[436]24.1858,[437]24.1799,[438]24.1835,[439]24.1550,[440]24.1884,[441]24.2000,[442]24.1865,[443]24.1777,[444]24.1924,[445]24.1538,[446]24.1634,[447]24.1594,[448]24.1505,[449]24.1454,[450]24.1663,[451]24.1591,[452]24.1552,[453]24.1349,[454]24.1144,[455]24.1239,[456]24.1215,[457]24.1316,[458]24.1728,[459]24.1797,[460]24.1765,[461]24.1798,[462]24.1898,[463]24.2170,[464]24.2213,[465]24.2313,[466]24.2317,[467]24.2463,[468]24.2708,[469]24.2913,[470]24.3174,[471]24.3049,[472]24.3270,[473]24.2835,[474]24.2775,[475]24.2890,[476]24.3039,[477]24.2895,[478]24.2475,[479]24.2409,[480]24.2605,[481]24.2764,[482]24.2387,[483]24.2493,[484]24.2652,[485]24.2675,[486]24.2535,[487]24.2712,[488]24.2325,[489]24.2039,[490]24.1988,[491]24.1678,[492]24.1583,[493]24.1149,[494]24.1034,[495]24.0649,[496]24.0620,[497]24.0889,[498]24.1082,[499]24.0887,[500]24.0985,[501]24.0895,[502]24.0983,[503]24.1430,[504]24.1634,[505]24.1643,[506]24.1448,[507]24.1192,[508]24.1284,[509]24.1032,[510]24.1060,[511]24.1168,[512]24.1054,[513]24.1184,[514]24.1107,[515]24.1057,[516]24.1071,[517]24.1026,[518]24.0906,[519]24.0777,[520]24.0594,[521]24.0581,[522]24.0395,[523]24.0435,[524]24.0334,[525]24.0404,[526]24.0484,[527]24.0348,[528]24.0250,[529]24.0077,[530]23.9949,[531]23.9974,[532]23.9906,[533]24.0042,[534]23.9986,[535]23.9972,[536]23.9824,[537]24.0039,[538]24.0218,[539]24.0205,[540]24.0587,[541]24.0629,[542]24.0313,[543]24.0304,[544]24.0470,[545]24.0372,[546]24.0248,[547]24.0092,[548]23.9797,[549]23.9911,[550]23.9386,[551]23.8909,[552]23.8557,[553]23.7422,[554]23.7278,[555]23.7363,[556]23.7329,[557]23.7203,[558]23.7086,[559]23.7132,[560]23.7277,[561]23.7331,[562]23.7672,[563]23.7925,[564]23.7858,[565]23.8041,[566]23.8140,[567]23.7926,[568]23.7951,[569]23.7777,[570]23.7963,[571]23.7971,[572]23.8248,[573]23.8334,[574]23.8402,[575]23.8380,[576]23.8626,[577]23.8611,[578]23.8720,[579]23.9069,[580]23.9425,[581]23.9518,[582]23.9883,[583]23.9681,[584]23.9797,
Final estimate: PPL = 23.9797 +/- 0.20740

llama_perf_context_print:        load time =    1605.51 ms
llama_perf_context_print: prompt eval time = 4343695.94 ms / 299008 tokens (   14.53 ms per token,    68.84 tokens per second)
llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_perf_context_print:       total time = 4404049.25 ms / 299009 tokens
llama_perf_context_print:    graphs reused =          0
状态: 完成
-------------------------------------------
执行时间: 2025-12-21 09:37:34
测试配置: [变体: base] [模型: Qwen3-0.6B-Q8_0.gguf] [线程: 64] [第 1 次运行]
命令: ./llama-perplexity -m /root/wmw/unsloth_riscv/Qwen3-0.6B-GGUF/Qwen3-0.6B-Q8_0.gguf -t 64 -f /root/wmw/hzeng_vw/riscv_kernels/e2e/rvv10/../wiki.test.raw
build: 0 (unknown) with riscv64-unknown-linux-gnu-gcc () 15.1.0 for riscv64-unknown-linux-gnu
llama_model_loader: loaded meta data with 32 key-value pairs and 310 tensors from /root/wmw/unsloth_riscv/Qwen3-0.6B-GGUF/Qwen3-0.6B-Q8_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3-0.6B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3-0.6B
llama_model_loader: - kv   4:                       general.quantized_by str              = Unsloth
llama_model_loader: - kv   5:                         general.size_label str              = 0.6B
llama_model_loader: - kv   6:                           general.repo_url str              = https://huggingface.co/unsloth
llama_model_loader: - kv   7:                          qwen3.block_count u32              = 28
llama_model_loader: - kv   8:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   9:                     qwen3.embedding_length u32              = 1024
llama_model_loader: - kv  10:                  qwen3.feed_forward_length u32              = 3072
llama_model_loader: - kv  11:                 qwen3.attention.head_count u32              = 16
llama_model_loader: - kv  12:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  13:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  14:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  15:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  16:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  22:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 151654
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 7
llama_model_loader: - kv  28:                      quantize.imatrix.file str              = Qwen3-0.6B-GGUF/imatrix_unsloth.dat
llama_model_loader: - kv  29:                   quantize.imatrix.dataset str              = unsloth_calibration_Qwen3-0.6B.txt
llama_model_loader: - kv  30:             quantize.imatrix.entries_count u32              = 196
llama_model_loader: - kv  31:              quantize.imatrix.chunks_count u32              = 688
llama_model_loader: - type  f32:  113 tensors
llama_model_loader: - type q8_0:  197 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q8_0
print_info: file size   = 604.15 MiB (8.50 BPW) 
load: printing all EOG tokens:
load:   - 151643 ('<|endoftext|>')
load:   - 151645 ('<|im_end|>')
load:   - 151662 ('<|fim_pad|>')
load:   - 151663 ('<|repo_name|>')
load:   - 151664 ('<|file_sep|>')
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 1024
print_info: n_layer          = 28
print_info: n_head           = 16
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 2
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 3072
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: model type       = 0.6B
print_info: model params     = 596.05 M
print_info: general.name     = Qwen3-0.6B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 11 ','
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151654 '<|vision_pad|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors:   CPU_Mapped model buffer size =   604.15 MiB
...........................................................................
llama_context: constructing llama_context
llama_context: n_seq_max     = 4
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 512
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (512) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     2.32 MiB
llama_kv_cache_unified:        CPU KV buffer size =   224.00 MiB
llama_kv_cache_unified: size =  224.00 MiB (   512 cells,  28 layers,  4/4 seqs), K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:        CPU compute buffer size =   298.75 MiB
llama_context: graph nodes  = 1098
llama_context: graph splits = 1
common_init_from_params: added <|endoftext|> logit bias = -inf
common_init_from_params: added <|im_end|> logit bias = -inf
common_init_from_params: added <|fim_pad|> logit bias = -inf
common_init_from_params: added <|repo_name|> logit bias = -inf
common_init_from_params: added <|file_sep|> logit bias = -inf
common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)

system_info: n_threads = 64 (n_threads_batch = 64) / 64 | CPU : LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 2852.22 ms
perplexity: calculating perplexity over 584 chunks, n_ctx=512, batch_size=2048, n_seq=4
perplexity: 33.60 seconds per pass - ETA 1 hours 21.75 minutes
[1]12.9049,[2]18.2255,[3]18.3677,[4]19.3176,[5]19.5931,[6]20.0124,[7]20.1813,[8]21.7131,[9]22.9996,[10]23.7721,[11]24.1951,[12]24.4437,[13]25.2557,[14]24.8450,[15]24.5444,[16]25.1469,[17]23.3096,[18]23.7820,[19]23.4676,[20]23.6525,[21]23.0978,[22]23.0624,[23]22.0541,[24]20.8408,[25]20.3838,[26]19.8970,[27]19.2036,[28]18.7937,[29]18.9730,[30]18.9905,[31]18.7540,[32]18.8258,[33]18.4690,[34]18.6471,[35]18.7935,[36]19.1124,[37]19.5056,[38]19.6484,[39]19.4641,[40]19.5772,[41]19.5521,[42]19.4127,[43]19.6091,[44]19.6627,[45]19.7421,[46]19.6900,[47]20.1092,[48]20.3356,[49]20.1969,[50]20.3962,[51]20.4521,[52]20.4848,[53]20.5602,[54]20.7116,[55]20.6766,[56]20.8052,[57]20.7310,[58]20.8003,[59]20.9383,[60]21.0561,[61]21.0399,[62]21.1036,[63]21.2643,[64]21.4259,[65]21.6696,[66]21.8553,[67]22.0673,[68]21.9821,[69]21.9916,[70]22.0176,[71]22.0401,[72]22.1909,[73]22.2385,[74]22.2467,[75]22.1151,[76]22.0184,[77]22.0359,[78]21.9851,[79]21.8178,[80]21.7814,[81]21.6514,[82]21.7487,[83]21.7022,[84]21.6169,[85]21.6221,[86]21.8206,[87]21.9839,[88]21.9289,[89]21.8584,[90]21.7919,[91]21.8871,[92]21.8167,[93]21.9514,[94]22.0621,[95]21.9942,[96]22.0289,[97]21.9838,[98]22.0009,[99]21.9558,[100]22.1099,[101]22.1991,[102]22.1676,[103]22.2350,[104]22.1417,[105]22.1772,[106]22.0786,[107]22.1063,[108]22.2289,[109]22.3068,[110]22.4265,[111]22.6983,[112]22.7445,[113]22.6043,[114]22.6991,[115]22.7464,[116]22.6789,[117]22.6637,[118]22.6117,[119]22.4801,[120]22.5271,[121]22.4859,[122]22.5401,[123]22.4884,[124]22.3738,[125]22.3296,[126]22.3120,[127]22.2808,[128]22.2113,[129]22.1820,[130]22.0967,[131]21.9649,[132]21.8758,[133]21.8106,[134]21.8190,[135]21.8055,[136]21.7746,[137]21.6757,[138]21.5688,[139]21.5918,[140]21.5352,[141]21.5365,[142]21.5313,[143]21.5830,[144]21.6334,[145]21.5820,[146]21.4404,[147]21.3221,[148]21.2254,[149]21.1610,[150]21.0275,[151]20.9862,[152]20.9807,[153]20.9462,[154]20.8430,[155]20.8845,[156]20.8231,[157]20.7761,[158]20.7030,[159]20.6539,[160]20.6175,[161]20.5961,[162]20.5945,[163]20.5933,[164]20.5829,[165]20.5549,[166]20.5706,[167]20.4945,[168]20.5321,[169]20.4991,[170]20.6300,[171]20.7073,[172]20.8090,[173]20.9257,[174]20.9546,[175]21.0459,[176]21.1389,[177]21.2396,[178]21.3148,[179]21.3779,[180]21.3999,[181]21.4899,[182]21.5729,[183]21.6508,[184]21.7454,[185]21.8018,[186]21.8150,[187]21.8148,[188]21.8192,[189]21.8321,[190]21.8946,[191]21.8988,[192]21.9099,[193]21.8954,[194]21.9487,[195]21.9786,[196]22.0066,[197]22.0122,[198]21.9362,[199]21.8940,[200]21.8842,[201]21.9256,[202]21.9574,[203]21.9939,[204]22.0111,[205]22.0078,[206]21.9754,[207]22.0273,[208]21.9489,[209]21.9375,[210]21.9351,[211]21.9303,[212]21.9306,[213]21.9356,[214]21.8914,[215]21.8245,[216]21.8248,[217]21.8215,[218]21.7893,[219]21.7209,[220]21.6726,[221]21.6308,[222]21.5844,[223]21.5850,[224]21.5932,[225]21.5312,[226]21.5377,[227]21.5125,[228]21.4446,[229]21.3643,[230]21.2985,[231]21.2502,[232]21.2138,[233]21.2159,[234]21.2235,[235]21.2078,[236]21.1555,[237]21.1226,[238]21.0472,[239]21.0126,[240]21.0364,[241]21.0142,[242]21.0357,[243]21.0522,[244]21.0883,[245]21.0862,[246]21.1484,[247]21.1422,[248]21.1551,[249]21.1873,[250]21.1885,[251]21.2482,[252]21.2487,[253]21.3056,[254]21.3377,[255]21.3458,[256]21.3951,[257]21.4012,[258]21.3516,[259]21.3046,[260]21.2711,[261]21.2378,[262]21.2172,[263]21.2020,[264]21.2051,[265]21.2514,[266]21.2689,[267]21.2769,[268]21.2512,[269]21.2495,[270]21.2604,[271]21.2247,[272]21.2362,[273]21.2306,[274]21.2257,[275]21.2204,[276]21.1653,[277]21.1383,[278]21.1568,[279]21.1522,[280]21.1416,[281]21.1318,[282]21.1935,[283]21.0992,[284]21.0066,[285]21.0192,[286]20.9604,[287]20.8883,[288]20.8982,[289]20.9033,[290]20.9771,[291]20.9773,[292]20.9730,[293]20.9790,[294]21.0244,[295]21.0750,[296]21.1267,[297]21.1729,[298]21.1655,[299]21.1226,[300]21.1190,[301]21.0966,[302]21.1031,[303]21.0925,[304]21.1167,[305]21.0969,[306]21.0828,[307]21.0929,[308]21.0756,[309]21.0630,[310]21.0965,[311]21.0983,[312]21.0566,[313]21.0474,[314]21.0640,[315]21.0245,[316]21.0772,[317]21.1347,[318]21.1309,[319]21.1085,[320]21.1258,[321]21.0809,[322]21.1015,[323]21.1350,[324]21.1671,[325]21.2013,[326]21.2158,[327]21.1780,[328]21.1717,[329]21.1124,[330]21.0911,[331]21.0487,[332]21.0369,[333]21.0330,[334]21.0004,[335]20.9572,[336]20.9416,[337]20.9568,[338]20.9846,[339]20.9619,[340]20.9265,[341]20.8980,[342]20.8948,[343]20.8895,[344]20.9131,[345]20.9252,[346]20.9290,[347]20.9197,[348]20.9258,[349]20.9131,[350]20.9128,[351]20.9369,[352]20.9426,[353]20.9421,[354]20.8916,[355]20.9323,[356]20.9483,[357]20.9563,[358]20.9323,[359]20.9294,[360]20.9247,[361]20.9310,[362]20.9537,[363]20.9492,[364]20.9858,[365]21.0167,[366]21.0832,[367]21.1329,[368]21.1995,[369]21.2535,[370]21.2927,[371]21.3428,[372]21.3929,[373]21.4149,[374]21.4419,[375]21.4981,[376]21.5366,[377]21.5578,[378]21.5972,[379]21.6336,[380]21.6756,[381]21.7027,[382]21.7344,[383]21.7624,[384]21.8092,[385]21.8688,[386]21.9063,[387]21.9056,[388]21.8845,[389]21.9224,[390]21.9712,[391]22.0114,[392]22.0110,[393]22.0082,[394]21.9857,[395]21.9901,[396]22.0089,[397]22.0145,[398]22.0209,[399]22.0226,[400]22.0624,[401]22.0581,[402]22.0635,[403]22.0402,[404]22.0360,[405]22.0157,[406]22.0130,[407]22.0166,[408]22.0269,[409]22.0285,[410]22.0292,[411]22.0655,[412]22.0758,[413]22.0852,[414]22.0784,[415]22.0755,[416]22.0653,[417]22.0752,[418]22.0854,[419]22.0784,[420]22.0697,[421]22.0786,[422]22.0485,[423]22.0611,[424]22.0714,[425]22.0802,[426]22.0978,[427]22.1304,[428]22.1741,[429]22.1771,[430]22.1545,[431]22.1257,[432]22.1264,[433]22.1198,[434]22.1060,[435]22.1215,[436]22.0598,[437]22.0524,[438]22.0553,[439]22.0294,[440]22.0559,[441]22.0642,[442]22.0520,[443]22.0423,[444]22.0538,[445]22.0201,[446]22.0275,[447]22.0219,[448]22.0103,[449]21.9999,[450]22.0159,[451]22.0078,[452]22.0011,[453]21.9813,[454]21.9633,[455]21.9674,[456]21.9618,[457]21.9761,[458]22.0112,[459]22.0188,[460]22.0148,[461]22.0200,[462]22.0289,[463]22.0540,[464]22.0557,[465]22.0626,[466]22.0644,[467]22.0768,[468]22.0997,[469]22.1154,[470]22.1372,[471]22.1276,[472]22.1486,[473]22.1078,[474]22.1035,[475]22.1146,[476]22.1303,[477]22.1183,[478]22.0800,[479]22.0767,[480]22.0991,[481]22.1168,[482]22.0800,[483]22.0907,[484]22.1102,[485]22.1115,[486]22.0988,[487]22.1144,[488]22.0801,[489]22.0539,[490]22.0500,[491]22.0238,[492]22.0158,[493]21.9776,[494]21.9670,[495]21.9318,[496]21.9298,[497]21.9507,[498]21.9690,[499]21.9480,[500]21.9542,[501]21.9444,[502]21.9511,[503]21.9913,[504]22.0109,[505]22.0135,[506]21.9989,[507]21.9777,[508]21.9843,[509]21.9597,[510]21.9612,[511]21.9710,[512]21.9596,[513]21.9746,[514]21.9712,[515]21.9681,[516]21.9676,[517]21.9635,[518]21.9487,[519]21.9364,[520]21.9210,[521]21.9166,[522]21.8963,[523]21.8973,[524]21.8896,[525]21.8972,[526]21.9068,[527]21.8947,[528]21.8852,[529]21.8729,[530]21.8620,[531]21.8654,[532]21.8605,[533]21.8717,[534]21.8707,[535]21.8708,[536]21.8580,[537]21.8774,[538]21.8935,[539]21.8927,[540]21.9267,[541]21.9310,[542]21.9031,[543]21.9037,[544]21.9193,[545]21.9088,[546]21.8992,[547]21.8840,[548]21.8580,[549]21.8684,[550]21.8225,[551]21.7793,[552]21.7472,[553]21.6456,[554]21.6317,[555]21.6413,[556]21.6386,[557]21.6279,[558]21.6138,[559]21.6158,[560]21.6295,[561]21.6331,[562]21.6619,[563]21.6847,[564]21.6771,[565]21.6962,[566]21.7037,[567]21.6830,[568]21.6831,[569]21.6660,[570]21.6846,[571]21.6875,[572]21.7139,[573]21.7223,[574]21.7289,[575]21.7254,[576]21.7478,[577]21.7466,[578]21.7535,[579]21.7866,[580]21.8215,[581]21.8317,[582]21.8666,[583]21.8466,[584]21.8581,
Final estimate: PPL = 21.8581 +/- 0.19091

llama_perf_context_print:        load time =    1613.46 ms
llama_perf_context_print: prompt eval time = 4774095.44 ms / 299008 tokens (   15.97 ms per token,    62.63 tokens per second)
llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_perf_context_print:       total time = 4834199.62 ms / 299009 tokens
llama_perf_context_print:    graphs reused =          0
状态: 完成
-------------------------------------------
执行时间: 2025-12-21 10:58:11
测试配置: [变体: base] [模型: Qwen3-1.7B-Q4_0.gguf] [线程: 64] [第 1 次运行]
命令: ./llama-perplexity -m /root/wmw/unsloth_riscv/Qwen3-1.7B-GGUF/Qwen3-1.7B-Q4_0.gguf -t 64 -f /root/wmw/hzeng_vw/riscv_kernels/e2e/rvv10/../wiki.test.raw
build: 0 (unknown) with riscv64-unknown-linux-gnu-gcc () 15.1.0 for riscv64-unknown-linux-gnu
llama_model_loader: loaded meta data with 32 key-value pairs and 310 tensors from /root/wmw/unsloth_riscv/Qwen3-1.7B-GGUF/Qwen3-1.7B-Q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3-1.7B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3-1.7B
llama_model_loader: - kv   4:                       general.quantized_by str              = Unsloth
llama_model_loader: - kv   5:                         general.size_label str              = 1.7B
llama_model_loader: - kv   6:                           general.repo_url str              = https://huggingface.co/unsloth
llama_model_loader: - kv   7:                          qwen3.block_count u32              = 28
llama_model_loader: - kv   8:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   9:                     qwen3.embedding_length u32              = 2048
llama_model_loader: - kv  10:                  qwen3.feed_forward_length u32              = 6144
llama_model_loader: - kv  11:                 qwen3.attention.head_count u32              = 16
llama_model_loader: - kv  12:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  13:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  14:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  15:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  16:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  22:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 151654
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 2
llama_model_loader: - kv  28:                      quantize.imatrix.file str              = Qwen3-1.7B-GGUF/imatrix_unsloth.dat
llama_model_loader: - kv  29:                   quantize.imatrix.dataset str              = unsloth_calibration_Qwen3-1.7B.txt
llama_model_loader: - kv  30:             quantize.imatrix.entries_count i32              = 196
llama_model_loader: - kv  31:              quantize.imatrix.chunks_count i32              = 685
llama_model_loader: - type  f32:  113 tensors
llama_model_loader: - type q4_0:  193 tensors
llama_model_loader: - type q4_1:    3 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 1002.15 MiB (4.89 BPW) 
load: printing all EOG tokens:
load:   - 151643 ('<|endoftext|>')
load:   - 151645 ('<|im_end|>')
load:   - 151662 ('<|fim_pad|>')
load:   - 151663 ('<|repo_name|>')
load:   - 151664 ('<|file_sep|>')
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 2048
print_info: n_layer          = 28
print_info: n_head           = 16
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 2
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 6144
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: model type       = 1.7B
print_info: model params     = 1.72 B
print_info: general.name     = Qwen3-1.7B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 11 ','
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151654 '<|vision_pad|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors:   CPU_Mapped model buffer size =  1002.15 MiB
.............................................................................
llama_context: constructing llama_context
llama_context: n_seq_max     = 4
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 512
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (512) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     2.32 MiB
llama_kv_cache_unified:        CPU KV buffer size =   224.00 MiB
llama_kv_cache_unified: size =  224.00 MiB (   512 cells,  28 layers,  4/4 seqs), K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:        CPU compute buffer size =   300.75 MiB
llama_context: graph nodes  = 1098
llama_context: graph splits = 1
common_init_from_params: added <|endoftext|> logit bias = -inf
common_init_from_params: added <|im_end|> logit bias = -inf
common_init_from_params: added <|fim_pad|> logit bias = -inf
common_init_from_params: added <|repo_name|> logit bias = -inf
common_init_from_params: added <|file_sep|> logit bias = -inf
common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)

system_info: n_threads = 64 (n_threads_batch = 64) / 64 | CPU : LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 2806.44 ms
perplexity: calculating perplexity over 584 chunks, n_ctx=512, batch_size=2048, n_seq=4
perplexity: 69.51 seconds per pass - ETA 2 hours 49.15 minutes
[1]13.0330,[2]17.6112,[3]17.0308,[4]17.5311,[5]17.8891,[6]17.7881,[7]18.0920,[8]19.0321,[9]20.2554,[10]20.7166,[11]20.7429,[12]20.7666,[13]21.3954,[14]21.0294,[15]20.7432,[16]21.2228,[17]19.9747,[18]20.1360,[19]19.8492,[20]19.8447,[21]19.4171,[22]19.4071,[23]18.6544,[24]17.6980,[25]17.2556,[26]16.7237,[27]16.1959,[28]16.1200,[29]16.2415,[30]16.1936,[31]15.9648,[32]15.9561,[33]15.6862,[34]15.8892,[35]15.9852,[36]16.2221,[37]16.4848,[38]16.5868,[39]16.4857,[40]16.5803,[41]16.5418,[42]16.4538,[43]16.6807,[44]16.7697,[45]16.7520,[46]16.7226,[47]17.0569,[48]17.2007,[49]17.1175,[50]17.2198,[51]17.2840,[52]17.3108,[53]17.3890,[54]17.4300,[55]17.4164,[56]17.5549,[57]17.4566,[58]17.4995,[59]17.5869,[60]17.6634,[61]17.6843,[62]17.7421,[63]17.8792,[64]18.0242,[65]18.2262,[66]18.3528,[67]18.5055,[68]18.4073,[69]18.3983,[70]18.3460,[71]18.3728,[72]18.4891,[73]18.5359,[74]18.5287,[75]18.3977,[76]18.3448,[77]18.3642,[78]18.3675,[79]18.2077,[80]18.1968,[81]18.1067,[82]18.1850,[83]18.1552,[84]18.1262,[85]18.2100,[86]18.3674,[87]18.4793,[88]18.4357,[89]18.4136,[90]18.3688,[91]18.4461,[92]18.3875,[93]18.5112,[94]18.5452,[95]18.4848,[96]18.4700,[97]18.4402,[98]18.4408,[99]18.4271,[100]18.5699,[101]18.6342,[102]18.5947,[103]18.6549,[104]18.5615,[105]18.6009,[106]18.5587,[107]18.6156,[108]18.6949,[109]18.7447,[110]18.8359,[111]19.0348,[112]19.0780,[113]18.9400,[114]18.9970,[115]19.0431,[116]18.9915,[117]18.9674,[118]18.9216,[119]18.7968,[120]18.8451,[121]18.7891,[122]18.8285,[123]18.8006,[124]18.7150,[125]18.6793,[126]18.6969,[127]18.6514,[128]18.6068,[129]18.5729,[130]18.5237,[131]18.4183,[132]18.3256,[133]18.2683,[134]18.2697,[135]18.2450,[136]18.2104,[137]18.1216,[138]18.0512,[139]18.0674,[140]18.0617,[141]18.0631,[142]18.0763,[143]18.1107,[144]18.1863,[145]18.1477,[146]18.0610,[147]17.9782,[148]17.9102,[149]17.8854,[150]17.7851,[151]17.7979,[152]17.7690,[153]17.7580,[154]17.6587,[155]17.7032,[156]17.6223,[157]17.5766,[158]17.5136,[159]17.4627,[160]17.3939,[161]17.3554,[162]17.3509,[163]17.3494,[164]17.3504,[165]17.3067,[166]17.2945,[167]17.2468,[168]17.2961,[169]17.2845,[170]17.3655,[171]17.4471,[172]17.5235,[173]17.6074,[174]17.6543,[175]17.7295,[176]17.8060,[177]17.9129,[178]17.9902,[179]18.0483,[180]18.0552,[181]18.1716,[182]18.2297,[183]18.2884,[184]18.3535,[185]18.4028,[186]18.4318,[187]18.4431,[188]18.4409,[189]18.4662,[190]18.5139,[191]18.5184,[192]18.5530,[193]18.5647,[194]18.6037,[195]18.6246,[196]18.6314,[197]18.6372,[198]18.5806,[199]18.5551,[200]18.5431,[201]18.6024,[202]18.6306,[203]18.6601,[204]18.6808,[205]18.6752,[206]18.6661,[207]18.7192,[208]18.6698,[209]18.6671,[210]18.6757,[211]18.6756,[212]18.6771,[213]18.7265,[214]18.7057,[215]18.6633,[216]18.6543,[217]18.6516,[218]18.6469,[219]18.5792,[220]18.5483,[221]18.5038,[222]18.4697,[223]18.4623,[224]18.4836,[225]18.4291,[226]18.4231,[227]18.3971,[228]18.3346,[229]18.2748,[230]18.2281,[231]18.1812,[232]18.1411,[233]18.1343,[234]18.1443,[235]18.1309,[236]18.1045,[237]18.0569,[238]17.9932,[239]17.9507,[240]17.9439,[241]17.9240,[242]17.9262,[243]17.9502,[244]17.9838,[245]17.9901,[246]18.0522,[247]18.0451,[248]18.0711,[249]18.0911,[250]18.0879,[251]18.1219,[252]18.1383,[253]18.1880,[254]18.2057,[255]18.1980,[256]18.2399,[257]18.2484,[258]18.2148,[259]18.1635,[260]18.1314,[261]18.0899,[262]18.0683,[263]18.0782,[264]18.0777,[265]18.1033,[266]18.1375,[267]18.1428,[268]18.1305,[269]18.1352,[270]18.1644,[271]18.1279,[272]18.1352,[273]18.1337,[274]18.1397,[275]18.1346,[276]18.0898,[277]18.0752,[278]18.0928,[279]18.0889,[280]18.0756,[281]18.0759,[282]18.1204,[283]18.0505,[284]17.9753,[285]17.9858,[286]17.9281,[287]17.8675,[288]17.8573,[289]17.8618,[290]17.9177,[291]17.9151,[292]17.9071,[293]17.9044,[294]17.9442,[295]18.0082,[296]18.0694,[297]18.1018,[298]18.0927,[299]18.0595,[300]18.0611,[301]18.0544,[302]18.0596,[303]18.0454,[304]18.0710,[305]18.0544,[306]18.0415,[307]18.0563,[308]18.0498,[309]18.0455,[310]18.0727,[311]18.0731,[312]18.0488,[313]18.0387,[314]18.0495,[315]18.0146,[316]18.0538,[317]18.1027,[318]18.1078,[319]18.0833,[320]18.0906,[321]18.0551,[322]18.0843,[323]18.1125,[324]18.1428,[325]18.1681,[326]18.1930,[327]18.1671,[328]18.1652,[329]18.1224,[330]18.1248,[331]18.0972,[332]18.0947,[333]18.0967,[334]18.0756,[335]18.0382,[336]18.0348,[337]18.0469,[338]18.0625,[339]18.0589,[340]18.0683,[341]18.0373,[342]18.0438,[343]18.0304,[344]18.0403,[345]18.0378,[346]18.0325,[347]18.0054,[348]18.0183,[349]18.0148,[350]18.0147,[351]18.0287,[352]18.0284,[353]18.0182,[354]17.9804,[355]18.0063,[356]18.0373,[357]18.0537,[358]18.0384,[359]18.0336,[360]18.0514,[361]18.0577,[362]18.0664,[363]18.0620,[364]18.1847,[365]18.2154,[366]18.2765,[367]18.3169,[368]18.3747,[369]18.4054,[370]18.4322,[371]18.4675,[372]18.5079,[373]18.5250,[374]18.5505,[375]18.5947,[376]18.6395,[377]18.6580,[378]18.6962,[379]18.7202,[380]18.7484,[381]18.7745,[382]18.7932,[383]18.8061,[384]18.8438,[385]18.8998,[386]18.9514,[387]18.9590,[388]18.9434,[389]18.9671,[390]19.0072,[391]19.0362,[392]19.0241,[393]19.0182,[394]19.0005,[395]19.0174,[396]19.0266,[397]19.0275,[398]19.0464,[399]19.0488,[400]19.0813,[401]19.0908,[402]19.0984,[403]19.0732,[404]19.0566,[405]19.0415,[406]19.0368,[407]19.0426,[408]19.0550,[409]19.0498,[410]19.0512,[411]19.0770,[412]19.0902,[413]19.0953,[414]19.0847,[415]19.0664,[416]19.0593,[417]19.0697,[418]19.0795,[419]19.0760,[420]19.0606,[421]19.0624,[422]19.0316,[423]19.0385,[424]19.0438,[425]19.0480,[426]19.0609,[427]19.0983,[428]19.1311,[429]19.1440,[430]19.1274,[431]19.1082,[432]19.1181,[433]19.1151,[434]19.1005,[435]19.1205,[436]19.0777,[437]19.0732,[438]19.0710,[439]19.0527,[440]19.0730,[441]19.0798,[442]19.0660,[443]19.0519,[444]19.0663,[445]19.0419,[446]19.0762,[447]19.0825,[448]19.0620,[449]19.0481,[450]19.0513,[451]19.0415,[452]19.0348,[453]19.0203,[454]19.0022,[455]19.0086,[456]19.0034,[457]19.0156,[458]19.0384,[459]19.0423,[460]19.0458,[461]19.0417,[462]19.0450,[463]19.0661,[464]19.0621,[465]19.0689,[466]19.0712,[467]19.0846,[468]19.1058,[469]19.1170,[470]19.1252,[471]19.1083,[472]19.1223,[473]19.0890,[474]19.0860,[475]19.0917,[476]19.1131,[477]19.0980,[478]19.0673,[479]19.0735,[480]19.0913,[481]19.1250,[482]19.0925,[483]19.1119,[484]19.1271,[485]19.1274,[486]19.1164,[487]19.1226,[488]19.0895,[489]19.0603,[490]19.0546,[491]19.0339,[492]19.0302,[493]18.9914,[494]18.9779,[495]18.9459,[496]18.9467,[497]18.9721,[498]18.9797,[499]18.9603,[500]18.9552,[501]18.9454,[502]18.9465,[503]18.9779,[504]18.9938,[505]18.9869,[506]18.9828,[507]18.9634,[508]18.9674,[509]18.9454,[510]18.9542,[511]18.9684,[512]18.9659,[513]18.9792,[514]18.9798,[515]19.0104,[516]19.0063,[517]19.0084,[518]19.0148,[519]19.0077,[520]18.9958,[521]18.9930,[522]18.9719,[523]18.9730,[524]18.9652,[525]18.9731,[526]18.9786,[527]18.9702,[528]18.9645,[529]18.9627,[530]18.9504,[531]18.9489,[532]18.9406,[533]18.9462,[534]18.9402,[535]18.9487,[536]18.9443,[537]18.9642,[538]18.9769,[539]18.9787,[540]19.0103,[541]19.0268,[542]18.9986,[543]19.0016,[544]19.0323,[545]19.0239,[546]19.0128,[547]18.9981,[548]18.9822,[549]18.9852,[550]18.9487,[551]18.9195,[552]18.8899,[553]18.8066,[554]18.7926,[555]18.8104,[556]18.8105,[557]18.7998,[558]18.7851,[559]18.7889,[560]18.7995,[561]18.8012,[562]18.8301,[563]18.8479,[564]18.8370,[565]18.8517,[566]18.8565,[567]18.8334,[568]18.8286,[569]18.8078,[570]18.8172,[571]18.8171,[572]18.8455,[573]18.8555,[574]18.8617,[575]18.8627,[576]18.8855,[577]18.8815,[578]18.8974,[579]18.9185,[580]18.9635,[581]18.9691,[582]18.9933,[583]18.9715,[584]18.9771,
Final estimate: PPL = 18.9771 +/- 0.18091

llama_perf_context_print:        load time =    1749.86 ms
llama_perf_context_print: prompt eval time = 9997592.04 ms / 299008 tokens (   33.44 ms per token,    29.91 tokens per second)
llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_perf_context_print:       total time = 10057686.06 ms / 299009 tokens
llama_perf_context_print:    graphs reused =          0
状态: 完成
-------------------------------------------
执行时间: 2025-12-21 13:45:51
测试配置: [变体: base] [模型: Qwen3-1.7B-Q4_0.gguf.aria2] [线程: 64] [第 1 次运行]
命令: ./llama-perplexity -m /root/wmw/unsloth_riscv/Qwen3-1.7B-GGUF/Qwen3-1.7B-Q4_0.gguf.aria2 -t 64 -f /root/wmw/hzeng_vw/riscv_kernels/e2e/rvv10/../wiki.test.raw
build: 0 (unknown) with riscv64-unknown-linux-gnu-gcc () 15.1.0 for riscv64-unknown-linux-gnu
gguf_init_from_file_impl: invalid magic characters: '????', expected 'GGUF'
llama_model_load: error loading model: llama_model_loader: failed to load model from /root/wmw/unsloth_riscv/Qwen3-1.7B-GGUF/Qwen3-1.7B-Q4_0.gguf.aria2
llama_model_load_from_file_impl: failed to load model
common_init_from_params: failed to load model '/root/wmw/unsloth_riscv/Qwen3-1.7B-GGUF/Qwen3-1.7B-Q4_0.gguf.aria2'
main: unable to load model
状态: 失败 (请查看日志详情)
-------------------------------------------
执行时间: 2025-12-21 13:45:51
测试配置: [变体: base] [模型: Qwen3-1.7B-Q8_0.gguf] [线程: 64] [第 1 次运行]
命令: ./llama-perplexity -m /root/wmw/unsloth_riscv/Qwen3-1.7B-GGUF/Qwen3-1.7B-Q8_0.gguf -t 64 -f /root/wmw/hzeng_vw/riscv_kernels/e2e/rvv10/../wiki.test.raw
build: 0 (unknown) with riscv64-unknown-linux-gnu-gcc () 15.1.0 for riscv64-unknown-linux-gnu
llama_model_loader: loaded meta data with 32 key-value pairs and 310 tensors from /root/wmw/unsloth_riscv/Qwen3-1.7B-GGUF/Qwen3-1.7B-Q8_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3-1.7B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3-1.7B
llama_model_loader: - kv   4:                       general.quantized_by str              = Unsloth
llama_model_loader: - kv   5:                         general.size_label str              = 1.7B
llama_model_loader: - kv   6:                           general.repo_url str              = https://huggingface.co/unsloth
llama_model_loader: - kv   7:                          qwen3.block_count u32              = 28
llama_model_loader: - kv   8:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   9:                     qwen3.embedding_length u32              = 2048
llama_model_loader: - kv  10:                  qwen3.feed_forward_length u32              = 6144
llama_model_loader: - kv  11:                 qwen3.attention.head_count u32              = 16
llama_model_loader: - kv  12:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  13:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  14:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  15:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  16:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  22:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 151654
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 7
llama_model_loader: - kv  28:                      quantize.imatrix.file str              = Qwen3-1.7B-GGUF/imatrix_unsloth.dat
llama_model_loader: - kv  29:                   quantize.imatrix.dataset str              = unsloth_calibration_Qwen3-1.7B.txt
llama_model_loader: - kv  30:             quantize.imatrix.entries_count i32              = 196
llama_model_loader: - kv  31:              quantize.imatrix.chunks_count i32              = 685
llama_model_loader: - type  f32:  113 tensors
llama_model_loader: - type q8_0:  197 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q8_0
print_info: file size   = 1.70 GiB (8.50 BPW) 
load: printing all EOG tokens:
load:   - 151643 ('<|endoftext|>')
load:   - 151645 ('<|im_end|>')
load:   - 151662 ('<|fim_pad|>')
load:   - 151663 ('<|repo_name|>')
load:   - 151664 ('<|file_sep|>')
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 2048
print_info: n_layer          = 28
print_info: n_head           = 16
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 2
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 6144
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: model type       = 1.7B
print_info: model params     = 1.72 B
print_info: general.name     = Qwen3-1.7B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 11 ','
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151654 '<|vision_pad|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors:   CPU_Mapped model buffer size =  1743.77 MiB
...................................................................................
llama_context: constructing llama_context
llama_context: n_seq_max     = 4
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 512
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (512) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     2.32 MiB
llama_kv_cache_unified:        CPU KV buffer size =   224.00 MiB
llama_kv_cache_unified: size =  224.00 MiB (   512 cells,  28 layers,  4/4 seqs), K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:        CPU compute buffer size =   300.75 MiB
llama_context: graph nodes  = 1098
llama_context: graph splits = 1
common_init_from_params: added <|endoftext|> logit bias = -inf
common_init_from_params: added <|im_end|> logit bias = -inf
common_init_from_params: added <|fim_pad|> logit bias = -inf
common_init_from_params: added <|repo_name|> logit bias = -inf
common_init_from_params: added <|file_sep|> logit bias = -inf
common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)

system_info: n_threads = 64 (n_threads_batch = 64) / 64 | CPU : LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 2874.52 ms
perplexity: calculating perplexity over 584 chunks, n_ctx=512, batch_size=2048, n_seq=4
perplexity: 86.25 seconds per pass - ETA 3 hours 29.87 minutes
[1]11.7780,[2]15.3230,[3]15.1188,[4]15.6493,[5]15.5926,[6]15.6866,[7]16.0785,[8]16.7759,[9]17.8863,[10]18.4172,[11]18.5169,[12]18.5908,[13]19.2280,[14]18.8388,[15]18.7701,[16]19.1443,[17]18.0559,[18]18.1981,[19]18.0057,[20]18.0111,[21]17.7395,[22]17.7861,[23]17.1154,[24]16.2070,[25]15.8134,[26]15.3757,[27]14.9202,[28]14.7555,[29]14.8208,[30]14.7613,[31]14.5624,[32]14.5486,[33]14.2982,[34]14.5685,[35]14.6610,[36]14.9011,[37]15.1325,[38]15.2285,[39]15.1827,[40]15.2878,[41]15.2461,[42]15.1561,[43]15.2822,[44]15.3320,[45]15.3165,[46]15.2658,[47]15.5714,[48]15.6913,[49]15.6044,[50]15.6981,[51]15.7702,[52]15.8111,[53]15.8979,[54]15.9426,[55]15.9447,[56]16.0682,[57]15.9823,[58]16.0364,[59]16.1127,[60]16.1907,[61]16.2292,[62]16.2771,[63]16.3599,[64]16.4848,[65]16.6709,[66]16.8002,[67]16.9432,[68]16.8533,[69]16.8312,[70]16.7948,[71]16.8126,[72]16.9135,[73]16.9495,[74]16.9493,[75]16.8332,[76]16.7967,[77]16.8406,[78]16.8391,[79]16.7085,[80]16.7195,[81]16.6521,[82]16.7291,[83]16.6897,[84]16.6491,[85]16.7024,[86]16.8645,[87]16.9671,[88]16.9391,[89]16.9191,[90]16.8816,[91]16.9469,[92]16.9025,[93]17.0221,[94]17.0622,[95]17.0119,[96]16.9944,[97]16.9578,[98]16.9608,[99]16.9268,[100]17.0525,[101]17.1118,[102]17.0727,[103]17.1138,[104]17.0392,[105]17.0693,[106]17.0374,[107]17.0754,[108]17.1424,[109]17.1954,[110]17.2808,[111]17.4776,[112]17.5057,[113]17.3702,[114]17.4256,[115]17.4639,[116]17.3982,[117]17.3749,[118]17.3354,[119]17.2192,[120]17.2694,[121]17.2250,[122]17.2440,[123]17.2131,[124]17.1215,[125]17.1001,[126]17.0977,[127]17.0629,[128]17.0229,[129]16.9806,[130]16.9432,[131]16.8434,[132]16.7574,[133]16.7074,[134]16.7159,[135]16.6928,[136]16.6534,[137]16.5739,[138]16.5071,[139]16.5203,[140]16.4987,[141]16.4827,[142]16.4950,[143]16.5194,[144]16.5905,[145]16.5511,[146]16.4656,[147]16.3801,[148]16.3068,[149]16.2913,[150]16.2085,[151]16.1941,[152]16.1799,[153]16.1697,[154]16.0931,[155]16.1389,[156]16.0759,[157]16.0354,[158]15.9848,[159]15.9449,[160]15.8797,[161]15.8478,[162]15.8466,[163]15.8517,[164]15.8628,[165]15.8350,[166]15.8183,[167]15.7777,[168]15.8152,[169]15.7947,[170]15.8687,[171]15.9352,[172]16.0059,[173]16.0843,[174]16.1153,[175]16.1908,[176]16.2553,[177]16.3452,[178]16.4139,[179]16.4690,[180]16.4732,[181]16.5322,[182]16.5862,[183]16.6246,[184]16.6860,[185]16.7290,[186]16.7512,[187]16.7597,[188]16.7643,[189]16.7852,[190]16.8295,[191]16.8350,[192]16.8619,[193]16.8690,[194]16.9056,[195]16.9258,[196]16.9369,[197]16.9440,[198]16.8908,[199]16.8694,[200]16.8530,[201]16.8924,[202]16.9190,[203]16.9526,[204]16.9725,[205]16.9737,[206]16.9652,[207]17.0151,[208]16.9785,[209]16.9791,[210]16.9757,[211]16.9723,[212]16.9705,[213]16.9868,[214]16.9555,[215]16.9168,[216]16.9112,[217]16.9051,[218]16.8988,[219]16.8379,[220]16.8125,[221]16.7734,[222]16.7368,[223]16.7263,[224]16.7389,[225]16.6962,[226]16.6857,[227]16.6593,[228]16.5981,[229]16.5397,[230]16.4939,[231]16.4530,[232]16.4141,[233]16.4115,[234]16.4252,[235]16.4123,[236]16.3798,[237]16.3428,[238]16.2894,[239]16.2556,[240]16.2577,[241]16.2423,[242]16.2510,[243]16.2712,[244]16.2946,[245]16.3007,[246]16.3540,[247]16.3468,[248]16.3619,[249]16.3826,[250]16.3804,[251]16.4094,[252]16.4215,[253]16.4641,[254]16.4868,[255]16.4814,[256]16.5175,[257]16.5192,[258]16.4948,[259]16.4484,[260]16.4199,[261]16.3797,[262]16.3626,[263]16.3711,[264]16.3759,[265]16.4061,[266]16.4363,[267]16.4428,[268]16.4340,[269]16.4428,[270]16.4651,[271]16.4315,[272]16.4443,[273]16.4433,[274]16.4475,[275]16.4505,[276]16.4195,[277]16.4067,[278]16.4170,[279]16.4148,[280]16.4008,[281]16.4048,[282]16.4478,[283]16.3924,[284]16.3288,[285]16.3415,[286]16.2930,[287]16.2416,[288]16.2368,[289]16.2438,[290]16.2959,[291]16.2997,[292]16.2930,[293]16.2950,[294]16.3295,[295]16.3837,[296]16.4528,[297]16.4886,[298]16.4808,[299]16.4520,[300]16.4510,[301]16.4446,[302]16.4499,[303]16.4367,[304]16.4600,[305]16.4500,[306]16.4388,[307]16.4519,[308]16.4407,[309]16.4334,[310]16.4545,[311]16.4547,[312]16.4331,[313]16.4233,[314]16.4313,[315]16.4020,[316]16.4339,[317]16.4820,[318]16.4841,[319]16.4638,[320]16.4679,[321]16.4362,[322]16.4587,[323]16.4855,[324]16.5129,[325]16.5360,[326]16.5493,[327]16.5254,[328]16.5246,[329]16.4821,[330]16.4750,[331]16.4505,[332]16.4490,[333]16.4473,[334]16.4268,[335]16.3923,[336]16.3880,[337]16.3989,[338]16.4118,[339]16.4064,[340]16.3966,[341]16.3718,[342]16.3698,[343]16.3574,[344]16.3697,[345]16.3709,[346]16.3684,[347]16.3446,[348]16.3579,[349]16.3541,[350]16.3516,[351]16.3644,[352]16.3676,[353]16.3623,[354]16.3327,[355]16.3571,[356]16.3741,[357]16.3863,[358]16.3676,[359]16.3621,[360]16.3755,[361]16.3851,[362]16.3911,[363]16.3834,[364]16.4573,[365]16.4873,[366]16.5406,[367]16.5781,[368]16.6288,[369]16.6612,[370]16.6856,[371]16.7209,[372]16.7559,[373]16.7736,[374]16.7968,[375]16.8355,[376]16.8687,[377]16.8838,[378]16.9048,[379]16.9239,[380]16.9492,[381]16.9726,[382]16.9942,[383]17.0096,[384]17.0407,[385]17.0881,[386]17.1330,[387]17.1364,[388]17.1258,[389]17.1519,[390]17.1924,[391]17.2216,[392]17.2119,[393]17.2050,[394]17.1914,[395]17.2036,[396]17.2144,[397]17.2151,[398]17.2258,[399]17.2266,[400]17.2588,[401]17.2679,[402]17.2747,[403]17.2518,[404]17.2411,[405]17.2237,[406]17.2190,[407]17.2237,[408]17.2338,[409]17.2314,[410]17.2341,[411]17.2581,[412]17.2701,[413]17.2726,[414]17.2641,[415]17.2472,[416]17.2397,[417]17.2465,[418]17.2517,[419]17.2542,[420]17.2416,[421]17.2425,[422]17.2124,[423]17.2238,[424]17.2281,[425]17.2330,[426]17.2440,[427]17.2727,[428]17.3064,[429]17.3157,[430]17.2990,[431]17.2825,[432]17.2943,[433]17.2904,[434]17.2747,[435]17.2910,[436]17.2531,[437]17.2468,[438]17.2442,[439]17.2261,[440]17.2433,[441]17.2493,[442]17.2344,[443]17.2238,[444]17.2329,[445]17.2059,[446]17.2283,[447]17.2289,[448]17.2107,[449]17.1988,[450]17.2010,[451]17.1902,[452]17.1803,[453]17.1637,[454]17.1478,[455]17.1533,[456]17.1451,[457]17.1574,[458]17.1812,[459]17.1825,[460]17.1867,[461]17.1857,[462]17.1900,[463]17.2075,[464]17.2077,[465]17.2131,[466]17.2165,[467]17.2288,[468]17.2458,[469]17.2556,[470]17.2627,[471]17.2467,[472]17.2613,[473]17.2335,[474]17.2315,[475]17.2383,[476]17.2557,[477]17.2424,[478]17.2182,[479]17.2238,[480]17.2424,[481]17.2624,[482]17.2326,[483]17.2508,[484]17.2673,[485]17.2698,[486]17.2625,[487]17.2697,[488]17.2418,[489]17.2195,[490]17.2155,[491]17.1974,[492]17.1900,[493]17.1585,[494]17.1456,[495]17.1196,[496]17.1179,[497]17.1375,[498]17.1451,[499]17.1276,[500]17.1250,[501]17.1169,[502]17.1212,[503]17.1493,[504]17.1653,[505]17.1601,[506]17.1590,[507]17.1411,[508]17.1442,[509]17.1232,[510]17.1291,[511]17.1404,[512]17.1366,[513]17.1472,[514]17.1460,[515]17.1526,[516]17.1504,[517]17.1507,[518]17.1484,[519]17.1412,[520]17.1320,[521]17.1312,[522]17.1142,[523]17.1154,[524]17.1096,[525]17.1165,[526]17.1221,[527]17.1171,[528]17.1144,[529]17.1102,[530]17.0965,[531]17.0946,[532]17.0853,[533]17.0903,[534]17.0857,[535]17.0928,[536]17.0909,[537]17.1091,[538]17.1212,[539]17.1242,[540]17.1534,[541]17.1587,[542]17.1341,[543]17.1386,[544]17.1599,[545]17.1528,[546]17.1443,[547]17.1281,[548]17.1124,[549]17.1154,[550]17.0839,[551]17.0579,[552]17.0326,[553]16.9598,[554]16.9493,[555]16.9634,[556]16.9636,[557]16.9542,[558]16.9412,[559]16.9452,[560]16.9567,[561]16.9595,[562]16.9852,[563]17.0008,[564]16.9918,[565]17.0068,[566]17.0095,[567]16.9913,[568]16.9872,[569]16.9702,[570]16.9731,[571]16.9744,[572]16.9957,[573]17.0047,[574]17.0082,[575]17.0098,[576]17.0315,[577]17.0304,[578]17.0434,[579]17.0631,[580]17.0895,[581]17.0946,[582]17.1175,[583]17.0975,[584]17.1029,
Final estimate: PPL = 17.1029 +/- 0.15983

llama_perf_context_print:        load time =    1936.72 ms
llama_perf_context_print: prompt eval time = 12449116.65 ms / 299008 tokens (   41.63 ms per token,    24.02 tokens per second)
llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_perf_context_print:       total time = 12501298.03 ms / 299009 tokens
llama_perf_context_print:    graphs reused =          0
状态: 完成
正在处理变体目录: vw
-------------------------------------------
执行时间: 2025-12-21 17:14:14
测试配置: [变体: vw] [模型: Qwen3-0.6B-Q4_0.gguf] [线程: 64] [第 1 次运行]
命令: ./llama-perplexity -m /root/wmw/unsloth_riscv/Qwen3-0.6B-GGUF/Qwen3-0.6B-Q4_0.gguf -t 64 -f /root/wmw/hzeng_vw/riscv_kernels/e2e/rvv10/../wiki.test.raw
build: 0 (unknown) with riscv64-unknown-linux-gnu-gcc () 15.1.0 for riscv64-unknown-linux-gnu
llama_model_loader: loaded meta data with 32 key-value pairs and 310 tensors from /root/wmw/unsloth_riscv/Qwen3-0.6B-GGUF/Qwen3-0.6B-Q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3-0.6B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3-0.6B
llama_model_loader: - kv   4:                       general.quantized_by str              = Unsloth
llama_model_loader: - kv   5:                         general.size_label str              = 0.6B
llama_model_loader: - kv   6:                           general.repo_url str              = https://huggingface.co/unsloth
llama_model_loader: - kv   7:                          qwen3.block_count u32              = 28
llama_model_loader: - kv   8:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   9:                     qwen3.embedding_length u32              = 1024
llama_model_loader: - kv  10:                  qwen3.feed_forward_length u32              = 3072
llama_model_loader: - kv  11:                 qwen3.attention.head_count u32              = 16
llama_model_loader: - kv  12:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  13:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  14:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  15:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  16:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  22:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 151654
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 2
llama_model_loader: - kv  28:                      quantize.imatrix.file str              = Qwen3-0.6B-GGUF/imatrix_unsloth.dat
llama_model_loader: - kv  29:                   quantize.imatrix.dataset str              = unsloth_calibration_Qwen3-0.6B.txt
llama_model_loader: - kv  30:             quantize.imatrix.entries_count u32              = 196
llama_model_loader: - kv  31:              quantize.imatrix.chunks_count u32              = 688
llama_model_loader: - type  f32:  113 tensors
llama_model_loader: - type q4_0:  193 tensors
llama_model_loader: - type q4_1:    3 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 358.78 MiB (5.05 BPW) 
load: printing all EOG tokens:
load:   - 151643 ('<|endoftext|>')
load:   - 151645 ('<|im_end|>')
load:   - 151662 ('<|fim_pad|>')
load:   - 151663 ('<|repo_name|>')
load:   - 151664 ('<|file_sep|>')
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 1024
print_info: n_layer          = 28
print_info: n_head           = 16
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 2
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 3072
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: model type       = 0.6B
print_info: model params     = 596.05 M
print_info: general.name     = Qwen3-0.6B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 11 ','
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151654 '<|vision_pad|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors:   CPU_Mapped model buffer size =   358.78 MiB
....................................................................
llama_context: constructing llama_context
llama_context: n_seq_max     = 4
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 512
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (512) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     2.32 MiB
llama_kv_cache_unified:        CPU KV buffer size =   224.00 MiB
llama_kv_cache_unified: size =  224.00 MiB (   512 cells,  28 layers,  4/4 seqs), K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:        CPU compute buffer size =   298.75 MiB
llama_context: graph nodes  = 1098
llama_context: graph splits = 1
common_init_from_params: added <|endoftext|> logit bias = -inf
common_init_from_params: added <|im_end|> logit bias = -inf
common_init_from_params: added <|fim_pad|> logit bias = -inf
common_init_from_params: added <|repo_name|> logit bias = -inf
common_init_from_params: added <|file_sep|> logit bias = -inf
common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)

system_info: n_threads = 64 (n_threads_batch = 64) / 64 | CPU : RISCV_V = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 2814.77 ms
perplexity: calculating perplexity over 584 chunks, n_ctx=512, batch_size=2048, n_seq=4
perplexity: 25.67 seconds per pass - ETA 1 hours 2.47 minutes
[1]14.0218,[2]19.2364,[3]19.5762,[4]20.3182,[5]20.4968,[6]21.0042,[7]21.2561,[8]23.0500,[9]24.5698,[10]25.5217,[11]26.0085,[12]26.2836,[13]27.0781,[14]26.7060,[15]26.4014,[16]27.2241,[17]25.1718,[18]25.6323,[19]25.2444,[20]25.4993,[21]24.8521,[22]24.8979,[23]23.7824,[24]22.5496,[25]22.0365,[26]21.5552,[27]20.8458,[28]20.4221,[29]20.6269,[30]20.6675,[31]20.3942,[32]20.4890,[33]20.0989,[34]20.3212,[35]20.5257,[36]20.8799,[37]21.3325,[38]21.4887,[39]21.2779,[40]21.3727,[41]21.3063,[42]21.1878,[43]21.4159,[44]21.4891,[45]21.5916,[46]21.5476,[47]22.0299,[48]22.2886,[49]22.1542,[50]22.3643,[51]22.4083,[52]22.4292,[53]22.4897,[54]22.6646,[55]22.6547,[56]22.7843,[57]22.7017,[58]22.7854,[59]22.9367,[60]23.0655,[61]23.0509,[62]23.1401,[63]23.3257,[64]23.5102,[65]23.7879,[66]24.0092,[67]24.2392,[68]24.1356,[69]24.1410,[70]24.1759,[71]24.1976,[72]24.3643,[73]24.4226,[74]24.4281,[75]24.2831,[76]24.1690,[77]24.1574,[78]24.1098,[79]23.9198,[80]23.8739,[81]23.7309,[82]23.8307,[83]23.7891,[84]23.7152,[85]23.7407,[86]23.9514,[87]24.1379,[88]24.0681,[89]24.0071,[90]23.9494,[91]24.0461,[92]23.9655,[93]24.1034,[94]24.2199,[95]24.1478,[96]24.2096,[97]24.1715,[98]24.1828,[99]24.1118,[100]24.2627,[101]24.3498,[102]24.2984,[103]24.3639,[104]24.2531,[105]24.2920,[106]24.1681,[107]24.1936,[108]24.3236,[109]24.4042,[110]24.5191,[111]24.8036,[112]24.8728,[113]24.7239,[114]24.8343,[115]24.8730,[116]24.8022,[117]24.7967,[118]24.7547,[119]24.6326,[120]24.6850,[121]24.6401,[122]24.6825,[123]24.6280,[124]24.5010,[125]24.4429,[126]24.4227,[127]24.3742,[128]24.3073,[129]24.2887,[130]24.2021,[131]24.0541,[132]23.9698,[133]23.9021,[134]23.9147,[135]23.9017,[136]23.8660,[137]23.7427,[138]23.6190,[139]23.6479,[140]23.5755,[141]23.5589,[142]23.5479,[143]23.6015,[144]23.6563,[145]23.6025,[146]23.4443,[147]23.3133,[148]23.2095,[149]23.1424,[150]23.0023,[151]22.9635,[152]22.9530,[153]22.9163,[154]22.7910,[155]22.8244,[156]22.7593,[157]22.7067,[158]22.6063,[159]22.5597,[160]22.5124,[161]22.4945,[162]22.4962,[163]22.4957,[164]22.4960,[165]22.4662,[166]22.4877,[167]22.3987,[168]22.4471,[169]22.4132,[170]22.5572,[171]22.6429,[172]22.7550,[173]22.8841,[174]22.9239,[175]23.0223,[176]23.1258,[177]23.2323,[178]23.3213,[179]23.3801,[180]23.4121,[181]23.5068,[182]23.5945,[183]23.6744,[184]23.7727,[185]23.8377,[186]23.8507,[187]23.8464,[188]23.8587,[189]23.8797,[190]23.9457,[191]23.9573,[192]23.9800,[193]23.9754,[194]24.0393,[195]24.0674,[196]24.0981,[197]24.1005,[198]24.0194,[199]23.9705,[200]23.9577,[201]23.9990,[202]24.0340,[203]24.0765,[204]24.0910,[205]24.0847,[206]24.0481,[207]24.0995,[208]24.0138,[209]24.0034,[210]24.0032,[211]23.9952,[212]23.9935,[213]24.0033,[214]23.9464,[215]23.8771,[216]23.8865,[217]23.8906,[218]23.8506,[219]23.7701,[220]23.7137,[221]23.6614,[222]23.6184,[223]23.6156,[224]23.6261,[225]23.5646,[226]23.5717,[227]23.5417,[228]23.4692,[229]23.3776,[230]23.3142,[231]23.2622,[232]23.2200,[233]23.2314,[234]23.2448,[235]23.2205,[236]23.1596,[237]23.1210,[238]23.0358,[239]23.0001,[240]23.0221,[241]22.9965,[242]23.0202,[243]23.0368,[244]23.0751,[245]23.0815,[246]23.1434,[247]23.1405,[248]23.1659,[249]23.1905,[250]23.1926,[251]23.2640,[252]23.2674,[253]23.3255,[254]23.3635,[255]23.3741,[256]23.4356,[257]23.4446,[258]23.3893,[259]23.3466,[260]23.3159,[261]23.2752,[262]23.2560,[263]23.2409,[264]23.2430,[265]23.2958,[266]23.3232,[267]23.3289,[268]23.2987,[269]23.2934,[270]23.3061,[271]23.2647,[272]23.2731,[273]23.2725,[274]23.2619,[275]23.2599,[276]23.2082,[277]23.1779,[278]23.2088,[279]23.2054,[280]23.1921,[281]23.1834,[282]23.2579,[283]23.1645,[284]23.0590,[285]23.0698,[286]22.9994,[287]22.9194,[288]22.9339,[289]22.9342,[290]23.0119,[291]23.0113,[292]23.0036,[293]23.0057,[294]23.0566,[295]23.1158,[296]23.1707,[297]23.2210,[298]23.2196,[299]23.1764,[300]23.1805,[301]23.1577,[302]23.1620,[303]23.1461,[304]23.1716,[305]23.1518,[306]23.1340,[307]23.1436,[308]23.1227,[309]23.1139,[310]23.1523,[311]23.1601,[312]23.1154,[313]23.1091,[314]23.1392,[315]23.0989,[316]23.1527,[317]23.2129,[318]23.2054,[319]23.1784,[320]23.1936,[321]23.1374,[322]23.1552,[323]23.1953,[324]23.2246,[325]23.2651,[326]23.2734,[327]23.2313,[328]23.2271,[329]23.1608,[330]23.1343,[331]23.0912,[332]23.0791,[333]23.0755,[334]23.0429,[335]22.9929,[336]22.9708,[337]22.9931,[338]23.0182,[339]22.9958,[340]22.9575,[341]22.9261,[342]22.9222,[343]22.9156,[344]22.9383,[345]22.9563,[346]22.9574,[347]22.9407,[348]22.9513,[349]22.9398,[350]22.9432,[351]22.9665,[352]22.9733,[353]22.9728,[354]22.9167,[355]22.9605,[356]22.9752,[357]22.9835,[358]22.9533,[359]22.9498,[360]22.9454,[361]22.9516,[362]22.9783,[363]22.9710,[364]23.0137,[365]23.0411,[366]23.1135,[367]23.1679,[368]23.2410,[369]23.2996,[370]23.3364,[371]23.3900,[372]23.4454,[373]23.4664,[374]23.4947,[375]23.5546,[376]23.5957,[377]23.6228,[378]23.6760,[379]23.7183,[380]23.7668,[381]23.7916,[382]23.8259,[383]23.8560,[384]23.8993,[385]23.9654,[386]24.0059,[387]23.9987,[388]23.9759,[389]24.0169,[390]24.0714,[391]24.1164,[392]24.1124,[393]24.1082,[394]24.0872,[395]24.0930,[396]24.1133,[397]24.1213,[398]24.1271,[399]24.1310,[400]24.1698,[401]24.1655,[402]24.1692,[403]24.1449,[404]24.1463,[405]24.1238,[406]24.1168,[407]24.1241,[408]24.1380,[409]24.1395,[410]24.1384,[411]24.1788,[412]24.1905,[413]24.2040,[414]24.2023,[415]24.1992,[416]24.1940,[417]24.2038,[418]24.2167,[419]24.2081,[420]24.2040,[421]24.2122,[422]24.1741,[423]24.1866,[424]24.1967,[425]24.2067,[426]24.2306,[427]24.2650,[428]24.3111,[429]24.3177,[430]24.2955,[431]24.2613,[432]24.2611,[433]24.2544,[434]24.2360,[435]24.2515,[436]24.1826,[437]24.1764,[438]24.1801,[439]24.1519,[440]24.1852,[441]24.1962,[442]24.1825,[443]24.1736,[444]24.1882,[445]24.1493,[446]24.1588,[447]24.1546,[448]24.1457,[449]24.1408,[450]24.1625,[451]24.1551,[452]24.1510,[453]24.1309,[454]24.1110,[455]24.1208,[456]24.1184,[457]24.1293,[458]24.1706,[459]24.1781,[460]24.1751,[461]24.1785,[462]24.1881,[463]24.2153,[464]24.2198,[465]24.2292,[466]24.2296,[467]24.2440,[468]24.2683,[469]24.2890,[470]24.3152,[471]24.3022,[472]24.3239,[473]24.2805,[474]24.2747,[475]24.2862,[476]24.3012,[477]24.2872,[478]24.2450,[479]24.2383,[480]24.2580,[481]24.2737,[482]24.2361,[483]24.2466,[484]24.2625,[485]24.2649,[486]24.2513,[487]24.2689,[488]24.2305,[489]24.2022,[490]24.1963,[491]24.1652,[492]24.1556,[493]24.1125,[494]24.1007,[495]24.0627,[496]24.0598,[497]24.0872,[498]24.1062,[499]24.0867,[500]24.0971,[501]24.0877,[502]24.0965,[503]24.1414,[504]24.1618,[505]24.1629,[506]24.1433,[507]24.1177,[508]24.1269,[509]24.1016,[510]24.1042,[511]24.1154,[512]24.1036,[513]24.1172,[514]24.1094,[515]24.1048,[516]24.1059,[517]24.1014,[518]24.0895,[519]24.0768,[520]24.0586,[521]24.0577,[522]24.0395,[523]24.0432,[524]24.0329,[525]24.0393,[526]24.0469,[527]24.0334,[528]24.0242,[529]24.0070,[530]23.9942,[531]23.9966,[532]23.9902,[533]24.0038,[534]23.9981,[535]23.9967,[536]23.9817,[537]24.0036,[538]24.0215,[539]24.0199,[540]24.0582,[541]24.0625,[542]24.0309,[543]24.0296,[544]24.0463,[545]24.0364,[546]24.0241,[547]24.0087,[548]23.9792,[549]23.9902,[550]23.9377,[551]23.8902,[552]23.8550,[553]23.7415,[554]23.7275,[555]23.7355,[556]23.7320,[557]23.7194,[558]23.7081,[559]23.7127,[560]23.7272,[561]23.7326,[562]23.7664,[563]23.7918,[564]23.7850,[565]23.8037,[566]23.8134,[567]23.7915,[568]23.7941,[569]23.7765,[570]23.7954,[571]23.7966,[572]23.8248,[573]23.8341,[574]23.8410,[575]23.8391,[576]23.8637,[577]23.8621,[578]23.8728,[579]23.9072,[580]23.9429,[581]23.9524,[582]23.9890,[583]23.9690,[584]23.9808,
Final estimate: PPL = 23.9808 +/- 0.20743

llama_perf_context_print:        load time =    1532.23 ms
llama_perf_context_print: prompt eval time = 3567683.69 ms / 299008 tokens (   11.93 ms per token,    83.81 tokens per second)
llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_perf_context_print:       total time = 3619344.21 ms / 299009 tokens
llama_perf_context_print:    graphs reused =          0
状态: 完成
-------------------------------------------
执行时间: 2025-12-21 18:14:36
测试配置: [变体: vw] [模型: Qwen3-0.6B-Q8_0.gguf] [线程: 64] [第 1 次运行]
命令: ./llama-perplexity -m /root/wmw/unsloth_riscv/Qwen3-0.6B-GGUF/Qwen3-0.6B-Q8_0.gguf -t 64 -f /root/wmw/hzeng_vw/riscv_kernels/e2e/rvv10/../wiki.test.raw
build: 0 (unknown) with riscv64-unknown-linux-gnu-gcc () 15.1.0 for riscv64-unknown-linux-gnu
llama_model_loader: loaded meta data with 32 key-value pairs and 310 tensors from /root/wmw/unsloth_riscv/Qwen3-0.6B-GGUF/Qwen3-0.6B-Q8_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3-0.6B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3-0.6B
llama_model_loader: - kv   4:                       general.quantized_by str              = Unsloth
llama_model_loader: - kv   5:                         general.size_label str              = 0.6B
llama_model_loader: - kv   6:                           general.repo_url str              = https://huggingface.co/unsloth
llama_model_loader: - kv   7:                          qwen3.block_count u32              = 28
llama_model_loader: - kv   8:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   9:                     qwen3.embedding_length u32              = 1024
llama_model_loader: - kv  10:                  qwen3.feed_forward_length u32              = 3072
llama_model_loader: - kv  11:                 qwen3.attention.head_count u32              = 16
llama_model_loader: - kv  12:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  13:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  14:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  15:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  16:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  22:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 151654
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 7
llama_model_loader: - kv  28:                      quantize.imatrix.file str              = Qwen3-0.6B-GGUF/imatrix_unsloth.dat
llama_model_loader: - kv  29:                   quantize.imatrix.dataset str              = unsloth_calibration_Qwen3-0.6B.txt
llama_model_loader: - kv  30:             quantize.imatrix.entries_count u32              = 196
llama_model_loader: - kv  31:              quantize.imatrix.chunks_count u32              = 688
llama_model_loader: - type  f32:  113 tensors
llama_model_loader: - type q8_0:  197 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q8_0
print_info: file size   = 604.15 MiB (8.50 BPW) 
load: printing all EOG tokens:
load:   - 151643 ('<|endoftext|>')
load:   - 151645 ('<|im_end|>')
load:   - 151662 ('<|fim_pad|>')
load:   - 151663 ('<|repo_name|>')
load:   - 151664 ('<|file_sep|>')
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 1024
print_info: n_layer          = 28
print_info: n_head           = 16
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 2
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 3072
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: model type       = 0.6B
print_info: model params     = 596.05 M
print_info: general.name     = Qwen3-0.6B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 11 ','
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151654 '<|vision_pad|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors:   CPU_Mapped model buffer size =   604.15 MiB
...........................................................................
llama_context: constructing llama_context
llama_context: n_seq_max     = 4
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 512
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (512) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     2.32 MiB
llama_kv_cache_unified:        CPU KV buffer size =   224.00 MiB
llama_kv_cache_unified: size =  224.00 MiB (   512 cells,  28 layers,  4/4 seqs), K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:        CPU compute buffer size =   298.75 MiB
llama_context: graph nodes  = 1098
llama_context: graph splits = 1
common_init_from_params: added <|endoftext|> logit bias = -inf
common_init_from_params: added <|im_end|> logit bias = -inf
common_init_from_params: added <|fim_pad|> logit bias = -inf
common_init_from_params: added <|repo_name|> logit bias = -inf
common_init_from_params: added <|file_sep|> logit bias = -inf
common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)

system_info: n_threads = 64 (n_threads_batch = 64) / 64 | CPU : RISCV_V = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 2815.95 ms
perplexity: calculating perplexity over 584 chunks, n_ctx=512, batch_size=2048, n_seq=4
perplexity: 25.43 seconds per pass - ETA 1 hours 1.87 minutes
[1]12.7985,[2]18.1217,[3]18.2742,[4]19.2390,[5]19.5312,[6]19.9603,[7]20.1241,[8]21.6986,[9]22.9975,[10]23.7583,[11]24.1909,[12]24.4451,[13]25.2513,[14]24.8614,[15]24.5424,[16]25.1449,[17]23.3015,[18]23.7694,[19]23.4599,[20]23.6478,[21]23.0898,[22]23.0567,[23]22.0545,[24]20.8396,[25]20.3861,[26]19.8978,[27]19.2056,[28]18.7961,[29]18.9760,[30]18.9941,[31]18.7543,[32]18.8264,[33]18.4695,[34]18.6492,[35]18.7952,[36]19.1142,[37]19.5061,[38]19.6519,[39]19.4689,[40]19.5818,[41]19.5586,[42]19.4193,[43]19.6136,[44]19.6667,[45]19.7465,[46]19.6938,[47]20.1134,[48]20.3400,[49]20.2010,[50]20.3986,[51]20.4537,[52]20.4849,[53]20.5591,[54]20.7102,[55]20.6742,[56]20.8056,[57]20.7327,[58]20.8011,[59]20.9385,[60]21.0527,[61]21.0372,[62]21.1024,[63]21.2638,[64]21.4272,[65]21.6729,[66]21.8597,[67]22.0708,[68]21.9847,[69]21.9950,[70]22.0177,[71]22.0414,[72]22.1923,[73]22.2391,[74]22.2456,[75]22.1146,[76]22.0196,[77]22.0369,[78]21.9857,[79]21.8166,[80]21.7800,[81]21.6512,[82]21.7461,[83]21.6987,[84]21.6119,[85]21.6182,[86]21.8175,[87]21.9795,[88]21.9244,[89]21.8561,[90]21.7913,[91]21.8874,[92]21.8146,[93]21.9496,[94]22.0617,[95]21.9961,[96]22.0297,[97]21.9848,[98]22.0014,[99]21.9564,[100]22.1106,[101]22.1997,[102]22.1678,[103]22.2354,[104]22.1421,[105]22.1781,[106]22.0805,[107]22.1088,[108]22.2305,[109]22.3079,[110]22.4278,[111]22.6990,[112]22.7461,[113]22.6042,[114]22.6984,[115]22.7452,[116]22.6762,[117]22.6603,[118]22.6082,[119]22.4768,[120]22.5229,[121]22.4804,[122]22.5342,[123]22.4828,[124]22.3680,[125]22.3241,[126]22.3074,[127]22.2762,[128]22.2062,[129]22.1771,[130]22.0901,[131]21.9581,[132]21.8685,[133]21.8040,[134]21.8136,[135]21.8006,[136]21.7698,[137]21.6698,[138]21.5629,[139]21.5862,[140]21.5297,[141]21.5319,[142]21.5263,[143]21.5781,[144]21.6284,[145]21.5774,[146]21.4360,[147]21.3172,[148]21.2200,[149]21.1566,[150]21.0244,[151]20.9818,[152]20.9749,[153]20.9405,[154]20.8369,[155]20.8784,[156]20.8182,[157]20.7716,[158]20.6985,[159]20.6484,[160]20.6122,[161]20.5901,[162]20.5903,[163]20.5886,[164]20.5778,[165]20.5516,[166]20.5677,[167]20.4920,[168]20.5289,[169]20.4954,[170]20.6266,[171]20.7040,[172]20.8062,[173]20.9228,[174]20.9529,[175]21.0448,[176]21.1378,[177]21.2380,[178]21.3132,[179]21.3767,[180]21.3983,[181]21.4884,[182]21.5715,[183]21.6494,[184]21.7444,[185]21.8014,[186]21.8135,[187]21.8134,[188]21.8175,[189]21.8307,[190]21.8946,[191]21.8995,[192]21.9104,[193]21.8953,[194]21.9488,[195]21.9787,[196]22.0068,[197]22.0126,[198]21.9365,[199]21.8937,[200]21.8839,[201]21.9266,[202]21.9583,[203]21.9949,[204]22.0116,[205]22.0079,[206]21.9760,[207]22.0285,[208]21.9500,[209]21.9390,[210]21.9370,[211]21.9310,[212]21.9313,[213]21.9374,[214]21.8925,[215]21.8259,[216]21.8256,[217]21.8235,[218]21.7913,[219]21.7228,[220]21.6737,[221]21.6319,[222]21.5858,[223]21.5861,[224]21.5935,[225]21.5315,[226]21.5381,[227]21.5134,[228]21.4449,[229]21.3658,[230]21.2996,[231]21.2520,[232]21.2162,[233]21.2178,[234]21.2245,[235]21.2089,[236]21.1564,[237]21.1228,[238]21.0473,[239]21.0140,[240]21.0375,[241]21.0148,[242]21.0370,[243]21.0539,[244]21.0897,[245]21.0882,[246]21.1500,[247]21.1442,[248]21.1570,[249]21.1885,[250]21.1894,[251]21.2493,[252]21.2493,[253]21.3065,[254]21.3389,[255]21.3468,[256]21.3960,[257]21.4024,[258]21.3527,[259]21.3062,[260]21.2730,[261]21.2397,[262]21.2200,[263]21.2044,[264]21.2072,[265]21.2548,[266]21.2726,[267]21.2808,[268]21.2555,[269]21.2537,[270]21.2637,[271]21.2280,[272]21.2395,[273]21.2343,[274]21.2293,[275]21.2241,[276]21.1688,[277]21.1411,[278]21.1602,[279]21.1559,[280]21.1454,[281]21.1355,[282]21.1973,[283]21.1031,[284]21.0107,[285]21.0233,[286]20.9639,[287]20.8921,[288]20.9021,[289]20.9070,[290]20.9811,[291]20.9816,[292]20.9777,[293]20.9840,[294]21.0289,[295]21.0792,[296]21.1311,[297]21.1769,[298]21.1690,[299]21.1263,[300]21.1227,[301]21.1004,[302]21.1071,[303]21.0965,[304]21.1206,[305]21.1016,[306]21.0877,[307]21.0976,[308]21.0802,[309]21.0674,[310]21.1003,[311]21.1023,[312]21.0603,[313]21.0520,[314]21.0687,[315]21.0297,[316]21.0829,[317]21.1401,[318]21.1360,[319]21.1131,[320]21.1301,[321]21.0853,[322]21.1060,[323]21.1394,[324]21.1712,[325]21.2056,[326]21.2206,[327]21.1831,[328]21.1770,[329]21.1174,[330]21.0959,[331]21.0534,[332]21.0418,[333]21.0374,[334]21.0049,[335]20.9619,[336]20.9464,[337]20.9613,[338]20.9892,[339]20.9664,[340]20.9312,[341]20.9028,[342]20.8994,[343]20.8941,[344]20.9179,[345]20.9301,[346]20.9338,[347]20.9249,[348]20.9313,[349]20.9187,[350]20.9185,[351]20.9424,[352]20.9482,[353]20.9473,[354]20.8969,[355]20.9376,[356]20.9537,[357]20.9617,[358]20.9380,[359]20.9351,[360]20.9304,[361]20.9369,[362]20.9597,[363]20.9553,[364]20.9919,[365]21.0234,[366]21.0898,[367]21.1393,[368]21.2057,[369]21.2600,[370]21.2993,[371]21.3490,[372]21.3984,[373]21.4205,[374]21.4479,[375]21.5038,[376]21.5426,[377]21.5638,[378]21.6042,[379]21.6410,[380]21.6829,[381]21.7102,[382]21.7422,[383]21.7702,[384]21.8167,[385]21.8761,[386]21.9139,[387]21.9132,[388]21.8924,[389]21.9301,[390]21.9790,[391]22.0190,[392]22.0187,[393]22.0161,[394]21.9930,[395]21.9972,[396]22.0163,[397]22.0219,[398]22.0279,[399]22.0292,[400]22.0691,[401]22.0649,[402]22.0702,[403]22.0467,[404]22.0425,[405]22.0225,[406]22.0195,[407]22.0231,[408]22.0334,[409]22.0351,[410]22.0356,[411]22.0719,[412]22.0822,[413]22.0917,[414]22.0851,[415]22.0817,[416]22.0711,[417]22.0814,[418]22.0914,[419]22.0844,[420]22.0757,[421]22.0843,[422]22.0538,[423]22.0663,[424]22.0764,[425]22.0852,[426]22.1032,[427]22.1354,[428]22.1794,[429]22.1822,[430]22.1597,[431]22.1313,[432]22.1319,[433]22.1249,[434]22.1112,[435]22.1266,[436]22.0649,[437]22.0574,[438]22.0604,[439]22.0342,[440]22.0606,[441]22.0687,[442]22.0566,[443]22.0471,[444]22.0584,[445]22.0248,[446]22.0322,[447]22.0269,[448]22.0155,[449]22.0053,[450]22.0213,[451]22.0134,[452]22.0069,[453]21.9869,[454]21.9689,[455]21.9731,[456]21.9675,[457]21.9822,[458]22.0170,[459]22.0247,[460]22.0204,[461]22.0257,[462]22.0348,[463]22.0597,[464]22.0615,[465]22.0685,[466]22.0702,[467]22.0823,[468]22.1051,[469]22.1210,[470]22.1432,[471]22.1337,[472]22.1546,[473]22.1140,[474]22.1097,[475]22.1207,[476]22.1365,[477]22.1245,[478]22.0861,[479]22.0826,[480]22.1057,[481]22.1234,[482]22.0866,[483]22.0974,[484]22.1167,[485]22.1176,[486]22.1051,[487]22.1208,[488]22.0864,[489]22.0602,[490]22.0563,[491]22.0303,[492]22.0222,[493]21.9841,[494]21.9736,[495]21.9382,[496]21.9359,[497]21.9565,[498]21.9746,[499]21.9535,[500]21.9598,[501]21.9499,[502]21.9565,[503]21.9966,[504]22.0163,[505]22.0192,[506]22.0043,[507]21.9831,[508]21.9898,[509]21.9652,[510]21.9666,[511]21.9764,[512]21.9651,[513]21.9803,[514]21.9765,[515]21.9731,[516]21.9723,[517]21.9684,[518]21.9536,[519]21.9414,[520]21.9260,[521]21.9217,[522]21.9012,[523]21.9024,[524]21.8946,[525]21.9021,[526]21.9121,[527]21.8999,[528]21.8907,[529]21.8785,[530]21.8673,[531]21.8707,[532]21.8660,[533]21.8768,[534]21.8756,[535]21.8757,[536]21.8632,[537]21.8824,[538]21.8986,[539]21.8980,[540]21.9318,[541]21.9361,[542]21.9083,[543]21.9087,[544]21.9243,[545]21.9140,[546]21.9044,[547]21.8894,[548]21.8633,[549]21.8733,[550]21.8275,[551]21.7842,[552]21.7520,[553]21.6503,[554]21.6364,[555]21.6461,[556]21.6432,[557]21.6326,[558]21.6184,[559]21.6206,[560]21.6346,[561]21.6385,[562]21.6673,[563]21.6902,[564]21.6826,[565]21.7011,[566]21.7087,[567]21.6877,[568]21.6876,[569]21.6707,[570]21.6894,[571]21.6922,[572]21.7188,[573]21.7269,[574]21.7336,[575]21.7299,[576]21.7527,[577]21.7515,[578]21.7582,[579]21.7914,[580]21.8263,[581]21.8368,[582]21.8714,[583]21.8514,[584]21.8626,
Final estimate: PPL = 21.8626 +/- 0.19096

llama_perf_context_print:        load time =    1627.18 ms
llama_perf_context_print: prompt eval time = 3502102.72 ms / 299008 tokens (   11.71 ms per token,    85.38 tokens per second)
llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_perf_context_print:       total time = 3554217.80 ms / 299009 tokens
llama_perf_context_print:    graphs reused =          0
状态: 完成
-------------------------------------------
执行时间: 2025-12-21 19:13:52
测试配置: [变体: vw] [模型: Qwen3-1.7B-Q4_0.gguf] [线程: 64] [第 1 次运行]
命令: ./llama-perplexity -m /root/wmw/unsloth_riscv/Qwen3-1.7B-GGUF/Qwen3-1.7B-Q4_0.gguf -t 64 -f /root/wmw/hzeng_vw/riscv_kernels/e2e/rvv10/../wiki.test.raw
build: 0 (unknown) with riscv64-unknown-linux-gnu-gcc () 15.1.0 for riscv64-unknown-linux-gnu
llama_model_loader: loaded meta data with 32 key-value pairs and 310 tensors from /root/wmw/unsloth_riscv/Qwen3-1.7B-GGUF/Qwen3-1.7B-Q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3-1.7B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3-1.7B
llama_model_loader: - kv   4:                       general.quantized_by str              = Unsloth
llama_model_loader: - kv   5:                         general.size_label str              = 1.7B
llama_model_loader: - kv   6:                           general.repo_url str              = https://huggingface.co/unsloth
llama_model_loader: - kv   7:                          qwen3.block_count u32              = 28
llama_model_loader: - kv   8:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   9:                     qwen3.embedding_length u32              = 2048
llama_model_loader: - kv  10:                  qwen3.feed_forward_length u32              = 6144
llama_model_loader: - kv  11:                 qwen3.attention.head_count u32              = 16
llama_model_loader: - kv  12:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  13:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  14:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  15:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  16:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  22:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 151654
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 2
llama_model_loader: - kv  28:                      quantize.imatrix.file str              = Qwen3-1.7B-GGUF/imatrix_unsloth.dat
llama_model_loader: - kv  29:                   quantize.imatrix.dataset str              = unsloth_calibration_Qwen3-1.7B.txt
llama_model_loader: - kv  30:             quantize.imatrix.entries_count i32              = 196
llama_model_loader: - kv  31:              quantize.imatrix.chunks_count i32              = 685
llama_model_loader: - type  f32:  113 tensors
llama_model_loader: - type q4_0:  193 tensors
llama_model_loader: - type q4_1:    3 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 1002.15 MiB (4.89 BPW) 
load: printing all EOG tokens:
load:   - 151643 ('<|endoftext|>')
load:   - 151645 ('<|im_end|>')
load:   - 151662 ('<|fim_pad|>')
load:   - 151663 ('<|repo_name|>')
load:   - 151664 ('<|file_sep|>')
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 2048
print_info: n_layer          = 28
print_info: n_head           = 16
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 2
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 6144
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: model type       = 1.7B
print_info: model params     = 1.72 B
print_info: general.name     = Qwen3-1.7B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 11 ','
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151654 '<|vision_pad|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors:   CPU_Mapped model buffer size =  1002.15 MiB
.............................................................................
llama_context: constructing llama_context
llama_context: n_seq_max     = 4
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 512
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (512) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     2.32 MiB
llama_kv_cache_unified:        CPU KV buffer size =   224.00 MiB
llama_kv_cache_unified: size =  224.00 MiB (   512 cells,  28 layers,  4/4 seqs), K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:        CPU compute buffer size =   300.75 MiB
llama_context: graph nodes  = 1098
llama_context: graph splits = 1
common_init_from_params: added <|endoftext|> logit bias = -inf
common_init_from_params: added <|im_end|> logit bias = -inf
common_init_from_params: added <|fim_pad|> logit bias = -inf
common_init_from_params: added <|repo_name|> logit bias = -inf
common_init_from_params: added <|file_sep|> logit bias = -inf
common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)

system_info: n_threads = 64 (n_threads_batch = 64) / 64 | CPU : RISCV_V = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 2843.75 ms
perplexity: calculating perplexity over 584 chunks, n_ctx=512, batch_size=2048, n_seq=4
perplexity: 37.91 seconds per pass - ETA 1 hours 32.23 minutes
[1]13.1125,[2]17.7016,[3]17.1508,[4]17.6157,[5]17.9109,[6]17.8064,[7]18.1248,[8]19.0667,[9]20.3287,[10]20.7906,[11]20.8169,[12]20.8363,[13]21.4677,[14]21.1013,[15]20.8276,[16]21.2992,[17]20.0437,[18]20.2121,[19]19.9229,[20]19.9071,[21]19.4808,[22]19.4722,[23]18.7059,[24]17.7327,[25]17.3093,[26]16.7702,[27]16.2423,[28]16.1617,[29]16.2781,[30]16.2264,[31]16.0008,[32]15.9903,[33]15.7290,[34]15.9298,[35]16.0202,[36]16.2600,[37]16.5231,[38]16.6273,[39]16.5235,[40]16.6142,[41]16.5795,[42]16.4884,[43]16.7171,[44]16.7981,[45]16.7798,[46]16.7523,[47]17.0821,[48]17.2254,[49]17.1405,[50]17.2418,[51]17.2993,[52]17.3271,[53]17.4063,[54]17.4451,[55]17.4287,[56]17.5681,[57]17.4691,[58]17.5124,[59]17.6040,[60]17.6802,[61]17.7009,[62]17.7535,[63]17.8883,[64]18.0348,[65]18.2364,[66]18.3626,[67]18.5154,[68]18.4168,[69]18.4060,[70]18.3541,[71]18.3832,[72]18.4989,[73]18.5476,[74]18.5387,[75]18.4090,[76]18.3547,[77]18.3737,[78]18.3790,[79]18.2184,[80]18.2071,[81]18.1170,[82]18.1940,[83]18.1633,[84]18.1316,[85]18.2136,[86]18.3716,[87]18.4832,[88]18.4408,[89]18.4229,[90]18.3754,[91]18.4508,[92]18.3916,[93]18.5194,[94]18.5534,[95]18.4915,[96]18.4763,[97]18.4461,[98]18.4468,[99]18.4315,[100]18.5721,[101]18.6354,[102]18.5955,[103]18.6602,[104]18.5676,[105]18.6069,[106]18.5635,[107]18.6190,[108]18.6979,[109]18.7483,[110]18.8389,[111]19.0379,[112]19.0818,[113]18.9429,[114]19.0006,[115]19.0476,[116]18.9937,[117]18.9697,[118]18.9228,[119]18.7961,[120]18.8450,[121]18.7890,[122]18.8252,[123]18.7952,[124]18.7100,[125]18.6738,[126]18.6921,[127]18.6471,[128]18.6025,[129]18.5687,[130]18.5198,[131]18.4136,[132]18.3219,[133]18.2643,[134]18.2660,[135]18.2424,[136]18.2079,[137]18.1190,[138]18.0468,[139]18.0631,[140]18.0558,[141]18.0563,[142]18.0678,[143]18.1012,[144]18.1757,[145]18.1386,[146]18.0513,[147]17.9673,[148]17.9026,[149]17.8778,[150]17.7795,[151]17.7919,[152]17.7652,[153]17.7539,[154]17.6533,[155]17.6970,[156]17.6164,[157]17.5706,[158]17.5075,[159]17.4563,[160]17.3879,[161]17.3472,[162]17.3421,[163]17.3413,[164]17.3424,[165]17.2982,[166]17.2851,[167]17.2376,[168]17.2882,[169]17.2763,[170]17.3577,[171]17.4384,[172]17.5162,[173]17.5993,[174]17.6462,[175]17.7231,[176]17.8005,[177]17.9077,[178]17.9843,[179]18.0432,[180]18.0504,[181]18.1667,[182]18.2241,[183]18.2834,[184]18.3486,[185]18.3978,[186]18.4262,[187]18.4378,[188]18.4352,[189]18.4605,[190]18.5082,[191]18.5133,[192]18.5462,[193]18.5592,[194]18.5984,[195]18.6199,[196]18.6267,[197]18.6334,[198]18.5754,[199]18.5514,[200]18.5416,[201]18.6009,[202]18.6294,[203]18.6575,[204]18.6787,[205]18.6727,[206]18.6653,[207]18.7171,[208]18.6672,[209]18.6657,[210]18.6756,[211]18.6759,[212]18.6773,[213]18.7236,[214]18.7015,[215]18.6593,[216]18.6508,[217]18.6484,[218]18.6435,[219]18.5759,[220]18.5491,[221]18.5046,[222]18.4689,[223]18.4616,[224]18.4842,[225]18.4302,[226]18.4237,[227]18.3965,[228]18.3335,[229]18.2738,[230]18.2267,[231]18.1809,[232]18.1407,[233]18.1343,[234]18.1440,[235]18.1313,[236]18.1054,[237]18.0582,[238]17.9941,[239]17.9504,[240]17.9440,[241]17.9249,[242]17.9276,[243]17.9519,[244]17.9864,[245]17.9927,[246]18.0538,[247]18.0466,[248]18.0724,[249]18.0921,[250]18.0884,[251]18.1221,[252]18.1378,[253]18.1878,[254]18.2065,[255]18.1981,[256]18.2398,[257]18.2477,[258]18.2141,[259]18.1628,[260]18.1302,[261]18.0884,[262]18.0671,[263]18.0771,[264]18.0764,[265]18.1026,[266]18.1364,[267]18.1421,[268]18.1299,[269]18.1351,[270]18.1660,[271]18.1298,[272]18.1373,[273]18.1356,[274]18.1409,[275]18.1368,[276]18.0921,[277]18.0775,[278]18.0954,[279]18.0922,[280]18.0784,[281]18.0789,[282]18.1219,[283]18.0522,[284]17.9771,[285]17.9878,[286]17.9304,[287]17.8699,[288]17.8607,[289]17.8649,[290]17.9210,[291]17.9174,[292]17.9100,[293]17.9075,[294]17.9471,[295]18.0106,[296]18.0732,[297]18.1062,[298]18.0957,[299]18.0626,[300]18.0642,[301]18.0581,[302]18.0629,[303]18.0492,[304]18.0743,[305]18.0583,[306]18.0456,[307]18.0600,[308]18.0537,[309]18.0497,[310]18.0762,[311]18.0763,[312]18.0522,[313]18.0423,[314]18.0525,[315]18.0173,[316]18.0557,[317]18.1048,[318]18.1098,[319]18.0855,[320]18.0927,[321]18.0585,[322]18.0879,[323]18.1162,[324]18.1468,[325]18.1724,[326]18.1982,[327]18.1726,[328]18.1713,[329]18.1281,[330]18.1312,[331]18.1032,[332]18.1001,[333]18.1024,[334]18.0810,[335]18.0436,[336]18.0404,[337]18.0522,[338]18.0670,[339]18.0632,[340]18.0726,[341]18.0417,[342]18.0484,[343]18.0348,[344]18.0445,[345]18.0419,[346]18.0365,[347]18.0093,[348]18.0228,[349]18.0194,[350]18.0185,[351]18.0323,[352]18.0318,[353]18.0214,[354]17.9846,[355]18.0107,[356]18.0410,[357]18.0573,[358]18.0413,[359]18.0367,[360]18.0540,[361]18.0600,[362]18.0693,[363]18.0658,[364]18.1880,[365]18.2189,[366]18.2796,[367]18.3205,[368]18.3788,[369]18.4094,[370]18.4356,[371]18.4707,[372]18.5107,[373]18.5278,[374]18.5535,[375]18.5978,[376]18.6424,[377]18.6605,[378]18.6966,[379]18.7212,[380]18.7497,[381]18.7753,[382]18.7941,[383]18.8067,[384]18.8438,[385]18.8998,[386]18.9522,[387]18.9601,[388]18.9449,[389]18.9688,[390]19.0088,[391]19.0381,[392]19.0264,[393]19.0205,[394]19.0022,[395]19.0192,[396]19.0287,[397]19.0299,[398]19.0485,[399]19.0513,[400]19.0840,[401]19.0933,[402]19.1001,[403]19.0749,[404]19.0589,[405]19.0443,[406]19.0400,[407]19.0455,[408]19.0579,[409]19.0522,[410]19.0539,[411]19.0794,[412]19.0939,[413]19.0985,[414]19.0879,[415]19.0692,[416]19.0622,[417]19.0725,[418]19.0817,[419]19.0781,[420]19.0624,[421]19.0640,[422]19.0330,[423]19.0404,[424]19.0455,[425]19.0494,[426]19.0627,[427]19.1001,[428]19.1329,[429]19.1455,[430]19.1291,[431]19.1097,[432]19.1195,[433]19.1170,[434]19.1023,[435]19.1214,[436]19.0788,[437]19.0746,[438]19.0724,[439]19.0541,[440]19.0738,[441]19.0795,[442]19.0655,[443]19.0514,[444]19.0654,[445]19.0410,[446]19.0761,[447]19.0827,[448]19.0621,[449]19.0490,[450]19.0519,[451]19.0423,[452]19.0359,[453]19.0214,[454]19.0032,[455]19.0091,[456]19.0040,[457]19.0165,[458]19.0388,[459]19.0431,[460]19.0472,[461]19.0430,[462]19.0458,[463]19.0666,[464]19.0636,[465]19.0705,[466]19.0730,[467]19.0862,[468]19.1077,[469]19.1190,[470]19.1275,[471]19.1106,[472]19.1252,[473]19.0918,[474]19.0887,[475]19.0946,[476]19.1153,[477]19.1003,[478]19.0696,[479]19.0759,[480]19.0939,[481]19.1273,[482]19.0946,[483]19.1138,[484]19.1291,[485]19.1290,[486]19.1182,[487]19.1241,[488]19.0911,[489]19.0624,[490]19.0566,[491]19.0357,[492]19.0316,[493]18.9925,[494]18.9789,[495]18.9468,[496]18.9475,[497]18.9733,[498]18.9810,[499]18.9613,[500]18.9561,[501]18.9462,[502]18.9473,[503]18.9785,[504]18.9943,[505]18.9875,[506]18.9832,[507]18.9637,[508]18.9677,[509]18.9455,[510]18.9548,[511]18.9687,[512]18.9666,[513]18.9798,[514]18.9801,[515]19.0066,[516]19.0021,[517]19.0046,[518]19.0106,[519]19.0033,[520]18.9918,[521]18.9890,[522]18.9677,[523]18.9688,[524]18.9612,[525]18.9691,[526]18.9747,[527]18.9661,[528]18.9603,[529]18.9588,[530]18.9467,[531]18.9452,[532]18.9366,[533]18.9425,[534]18.9366,[535]18.9451,[536]18.9413,[537]18.9615,[538]18.9743,[539]18.9762,[540]19.0079,[541]19.0246,[542]18.9965,[543]19.0002,[544]19.0307,[545]19.0226,[546]19.0117,[547]18.9967,[548]18.9815,[549]18.9846,[550]18.9485,[551]18.9192,[552]18.8894,[553]18.8060,[554]18.7918,[555]18.8100,[556]18.8105,[557]18.7999,[558]18.7852,[559]18.7892,[560]18.8000,[561]18.8017,[562]18.8309,[563]18.8490,[564]18.8378,[565]18.8528,[566]18.8575,[567]18.8340,[568]18.8291,[569]18.8084,[570]18.8174,[571]18.8178,[572]18.8465,[573]18.8563,[574]18.8619,[575]18.8632,[576]18.8861,[577]18.8820,[578]18.8968,[579]18.9181,[580]18.9624,[581]18.9675,[582]18.9920,[583]18.9698,[584]18.9758,
Final estimate: PPL = 18.9758 +/- 0.18087

llama_perf_context_print:        load time =    1696.31 ms
llama_perf_context_print: prompt eval time = 5366281.73 ms / 299008 tokens (   17.95 ms per token,    55.72 tokens per second)
llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_perf_context_print:       total time = 5418559.01 ms / 299009 tokens
llama_perf_context_print:    graphs reused =          0
状态: 完成
-------------------------------------------
执行时间: 2025-12-21 20:44:13
测试配置: [变体: vw] [模型: Qwen3-1.7B-Q4_0.gguf.aria2] [线程: 64] [第 1 次运行]
命令: ./llama-perplexity -m /root/wmw/unsloth_riscv/Qwen3-1.7B-GGUF/Qwen3-1.7B-Q4_0.gguf.aria2 -t 64 -f /root/wmw/hzeng_vw/riscv_kernels/e2e/rvv10/../wiki.test.raw
build: 0 (unknown) with riscv64-unknown-linux-gnu-gcc () 15.1.0 for riscv64-unknown-linux-gnu
gguf_init_from_file_impl: invalid magic characters: '????', expected 'GGUF'
llama_model_load: error loading model: llama_model_loader: failed to load model from /root/wmw/unsloth_riscv/Qwen3-1.7B-GGUF/Qwen3-1.7B-Q4_0.gguf.aria2
llama_model_load_from_file_impl: failed to load model
common_init_from_params: failed to load model '/root/wmw/unsloth_riscv/Qwen3-1.7B-GGUF/Qwen3-1.7B-Q4_0.gguf.aria2'
main: unable to load model
状态: 失败 (请查看日志详情)
-------------------------------------------
执行时间: 2025-12-21 20:44:13
测试配置: [变体: vw] [模型: Qwen3-1.7B-Q8_0.gguf] [线程: 64] [第 1 次运行]
命令: ./llama-perplexity -m /root/wmw/unsloth_riscv/Qwen3-1.7B-GGUF/Qwen3-1.7B-Q8_0.gguf -t 64 -f /root/wmw/hzeng_vw/riscv_kernels/e2e/rvv10/../wiki.test.raw
build: 0 (unknown) with riscv64-unknown-linux-gnu-gcc () 15.1.0 for riscv64-unknown-linux-gnu
llama_model_loader: loaded meta data with 32 key-value pairs and 310 tensors from /root/wmw/unsloth_riscv/Qwen3-1.7B-GGUF/Qwen3-1.7B-Q8_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3-1.7B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3-1.7B
llama_model_loader: - kv   4:                       general.quantized_by str              = Unsloth
llama_model_loader: - kv   5:                         general.size_label str              = 1.7B
llama_model_loader: - kv   6:                           general.repo_url str              = https://huggingface.co/unsloth
llama_model_loader: - kv   7:                          qwen3.block_count u32              = 28
llama_model_loader: - kv   8:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   9:                     qwen3.embedding_length u32              = 2048
llama_model_loader: - kv  10:                  qwen3.feed_forward_length u32              = 6144
llama_model_loader: - kv  11:                 qwen3.attention.head_count u32              = 16
llama_model_loader: - kv  12:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  13:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  14:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  15:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  16:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  22:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 151654
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 7
llama_model_loader: - kv  28:                      quantize.imatrix.file str              = Qwen3-1.7B-GGUF/imatrix_unsloth.dat
llama_model_loader: - kv  29:                   quantize.imatrix.dataset str              = unsloth_calibration_Qwen3-1.7B.txt
llama_model_loader: - kv  30:             quantize.imatrix.entries_count i32              = 196
llama_model_loader: - kv  31:              quantize.imatrix.chunks_count i32              = 685
llama_model_loader: - type  f32:  113 tensors
llama_model_loader: - type q8_0:  197 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q8_0
print_info: file size   = 1.70 GiB (8.50 BPW) 
load: printing all EOG tokens:
load:   - 151643 ('<|endoftext|>')
load:   - 151645 ('<|im_end|>')
load:   - 151662 ('<|fim_pad|>')
load:   - 151663 ('<|repo_name|>')
load:   - 151664 ('<|file_sep|>')
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 2048
print_info: n_layer          = 28
print_info: n_head           = 16
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 2
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 6144
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: model type       = 1.7B
print_info: model params     = 1.72 B
print_info: general.name     = Qwen3-1.7B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 11 ','
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151654 '<|vision_pad|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors:   CPU_Mapped model buffer size =  1743.77 MiB
...................................................................................
llama_context: constructing llama_context
llama_context: n_seq_max     = 4
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 512
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (512) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     2.32 MiB
llama_kv_cache_unified:        CPU KV buffer size =   224.00 MiB
llama_kv_cache_unified: size =  224.00 MiB (   512 cells,  28 layers,  4/4 seqs), K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:        CPU compute buffer size =   300.75 MiB
llama_context: graph nodes  = 1098
llama_context: graph splits = 1
common_init_from_params: added <|endoftext|> logit bias = -inf
common_init_from_params: added <|im_end|> logit bias = -inf
common_init_from_params: added <|fim_pad|> logit bias = -inf
common_init_from_params: added <|repo_name|> logit bias = -inf
common_init_from_params: added <|file_sep|> logit bias = -inf
common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)

system_info: n_threads = 64 (n_threads_batch = 64) / 64 | CPU : RISCV_V = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 3101.44 ms
perplexity: calculating perplexity over 584 chunks, n_ctx=512, batch_size=2048, n_seq=4
perplexity: 35.18 seconds per pass - ETA 1 hours 25.60 minutes
[1]11.8368,[2]15.3671,[3]15.1026,[4]15.6064,[5]15.5295,[6]15.6455,[7]16.0590,[8]16.7701,[9]17.8751,[10]18.4006,[11]18.5094,[12]18.5864,[13]19.2209,[14]18.8204,[15]18.7518,[16]19.1184,[17]18.0270,[18]18.1704,[19]17.9949,[20]17.9972,[21]17.7160,[22]17.7518,[23]17.0802,[24]16.1796,[25]15.7896,[26]15.3564,[27]14.8967,[28]14.7395,[29]14.8021,[30]14.7385,[31]14.5429,[32]14.5270,[33]14.2741,[34]14.5500,[35]14.6424,[36]14.8855,[37]15.1174,[38]15.2105,[39]15.1698,[40]15.2735,[41]15.2375,[42]15.1458,[43]15.2709,[44]15.3182,[45]15.3048,[46]15.2543,[47]15.5633,[48]15.6840,[49]15.5986,[50]15.6926,[51]15.7639,[52]15.8063,[53]15.8919,[54]15.9334,[55]15.9355,[56]16.0592,[57]15.9753,[58]16.0306,[59]16.1059,[60]16.1856,[61]16.2227,[62]16.2715,[63]16.3512,[64]16.4756,[65]16.6622,[66]16.7926,[67]16.9340,[68]16.8439,[69]16.8229,[70]16.7900,[71]16.8086,[72]16.9072,[73]16.9443,[74]16.9440,[75]16.8276,[76]16.7918,[77]16.8364,[78]16.8332,[79]16.7018,[80]16.7089,[81]16.6394,[82]16.7158,[83]16.6776,[84]16.6368,[85]16.6922,[86]16.8539,[87]16.9562,[88]16.9272,[89]16.9070,[90]16.8701,[91]16.9367,[92]16.8936,[93]17.0113,[94]17.0516,[95]17.0003,[96]16.9826,[97]16.9455,[98]16.9494,[99]16.9156,[100]17.0421,[101]17.1016,[102]17.0626,[103]17.1015,[104]17.0263,[105]17.0561,[106]17.0244,[107]17.0623,[108]17.1290,[109]17.1835,[110]17.2669,[111]17.4643,[112]17.4914,[113]17.3568,[114]17.4116,[115]17.4511,[116]17.3862,[117]17.3638,[118]17.3251,[119]17.2102,[120]17.2600,[121]17.2159,[122]17.2344,[123]17.2038,[124]17.1129,[125]17.0932,[126]17.0898,[127]17.0542,[128]17.0149,[129]16.9738,[130]16.9368,[131]16.8374,[132]16.7515,[133]16.7018,[134]16.7115,[135]16.6879,[136]16.6482,[137]16.5692,[138]16.5032,[139]16.5162,[140]16.4953,[141]16.4802,[142]16.4919,[143]16.5171,[144]16.5893,[145]16.5497,[146]16.4644,[147]16.3794,[148]16.3054,[149]16.2900,[150]16.2076,[151]16.1933,[152]16.1793,[153]16.1698,[154]16.0923,[155]16.1392,[156]16.0754,[157]16.0347,[158]15.9843,[159]15.9439,[160]15.8795,[161]15.8476,[162]15.8468,[163]15.8511,[164]15.8619,[165]15.8342,[166]15.8175,[167]15.7768,[168]15.8138,[169]15.7933,[170]15.8677,[171]15.9350,[172]16.0058,[173]16.0839,[174]16.1148,[175]16.1898,[176]16.2538,[177]16.3445,[178]16.4129,[179]16.4691,[180]16.4737,[181]16.5337,[182]16.5867,[183]16.6268,[184]16.6881,[185]16.7320,[186]16.7540,[187]16.7625,[188]16.7673,[189]16.7875,[190]16.8322,[191]16.8361,[192]16.8630,[193]16.8698,[194]16.9048,[195]16.9246,[196]16.9364,[197]16.9440,[198]16.8909,[199]16.8703,[200]16.8541,[201]16.8938,[202]16.9204,[203]16.9536,[204]16.9732,[205]16.9741,[206]16.9658,[207]17.0153,[208]16.9785,[209]16.9787,[210]16.9748,[211]16.9704,[212]16.9689,[213]16.9855,[214]16.9532,[215]16.9139,[216]16.9079,[217]16.9023,[218]16.8957,[219]16.8348,[220]16.8090,[221]16.7694,[222]16.7326,[223]16.7227,[224]16.7357,[225]16.6933,[226]16.6836,[227]16.6572,[228]16.5957,[229]16.5373,[230]16.4928,[231]16.4519,[232]16.4130,[233]16.4107,[234]16.4245,[235]16.4114,[236]16.3793,[237]16.3425,[238]16.2887,[239]16.2551,[240]16.2572,[241]16.2411,[242]16.2495,[243]16.2700,[244]16.2942,[245]16.3006,[246]16.3528,[247]16.3456,[248]16.3597,[249]16.3793,[250]16.3770,[251]16.4058,[252]16.4175,[253]16.4598,[254]16.4824,[255]16.4774,[256]16.5137,[257]16.5153,[258]16.4903,[259]16.4449,[260]16.4159,[261]16.3754,[262]16.3588,[263]16.3674,[264]16.3724,[265]16.4024,[266]16.4330,[267]16.4392,[268]16.4311,[269]16.4394,[270]16.4638,[271]16.4304,[272]16.4423,[273]16.4409,[274]16.4448,[275]16.4478,[276]16.4168,[277]16.4042,[278]16.4145,[279]16.4120,[280]16.3985,[281]16.4026,[282]16.4465,[283]16.3916,[284]16.3280,[285]16.3407,[286]16.2926,[287]16.2412,[288]16.2363,[289]16.2431,[290]16.2955,[291]16.2989,[292]16.2924,[293]16.2939,[294]16.3279,[295]16.3826,[296]16.4512,[297]16.4869,[298]16.4790,[299]16.4504,[300]16.4497,[301]16.4434,[302]16.4490,[303]16.4359,[304]16.4589,[305]16.4485,[306]16.4371,[307]16.4506,[308]16.4392,[309]16.4319,[310]16.4529,[311]16.4532,[312]16.4315,[313]16.4221,[314]16.4296,[315]16.4004,[316]16.4321,[317]16.4799,[318]16.4816,[319]16.4612,[320]16.4656,[321]16.4341,[322]16.4569,[323]16.4834,[324]16.5108,[325]16.5338,[326]16.5467,[327]16.5226,[328]16.5218,[329]16.4792,[330]16.4723,[331]16.4484,[332]16.4469,[333]16.4452,[334]16.4245,[335]16.3903,[336]16.3857,[337]16.3966,[338]16.4098,[339]16.4048,[340]16.3950,[341]16.3703,[342]16.3682,[343]16.3558,[344]16.3683,[345]16.3698,[346]16.3675,[347]16.3436,[348]16.3570,[349]16.3525,[350]16.3495,[351]16.3624,[352]16.3658,[353]16.3607,[354]16.3311,[355]16.3554,[356]16.3722,[357]16.3845,[358]16.3658,[359]16.3612,[360]16.3750,[361]16.3847,[362]16.3904,[363]16.3825,[364]16.4568,[365]16.4864,[366]16.5397,[367]16.5771,[368]16.6277,[369]16.6599,[370]16.6843,[371]16.7191,[372]16.7544,[373]16.7716,[374]16.7944,[375]16.8334,[376]16.8664,[377]16.8815,[378]16.9022,[379]16.9216,[380]16.9463,[381]16.9697,[382]16.9906,[383]17.0059,[384]17.0374,[385]17.0845,[386]17.1297,[387]17.1336,[388]17.1227,[389]17.1488,[390]17.1894,[391]17.2187,[392]17.2090,[393]17.2023,[394]17.1887,[395]17.2011,[396]17.2118,[397]17.2127,[398]17.2235,[399]17.2240,[400]17.2564,[401]17.2656,[402]17.2722,[403]17.2495,[404]17.2386,[405]17.2211,[406]17.2167,[407]17.2211,[408]17.2311,[409]17.2289,[410]17.2316,[411]17.2557,[412]17.2677,[413]17.2702,[414]17.2619,[415]17.2448,[416]17.2378,[417]17.2447,[418]17.2499,[419]17.2525,[420]17.2403,[421]17.2409,[422]17.2111,[423]17.2221,[424]17.2264,[425]17.2314,[426]17.2425,[427]17.2709,[428]17.3046,[429]17.3141,[430]17.2979,[431]17.2813,[432]17.2933,[433]17.2893,[434]17.2738,[435]17.2901,[436]17.2520,[437]17.2454,[438]17.2430,[439]17.2251,[440]17.2426,[441]17.2489,[442]17.2340,[443]17.2233,[444]17.2326,[445]17.2058,[446]17.2282,[447]17.2289,[448]17.2109,[449]17.1986,[450]17.2007,[451]17.1903,[452]17.1804,[453]17.1638,[454]17.1477,[455]17.1530,[456]17.1449,[457]17.1573,[458]17.1813,[459]17.1825,[460]17.1866,[461]17.1855,[462]17.1895,[463]17.2068,[464]17.2074,[465]17.2130,[466]17.2159,[467]17.2283,[468]17.2452,[469]17.2551,[470]17.2624,[471]17.2464,[472]17.2609,[473]17.2331,[474]17.2314,[475]17.2382,[476]17.2560,[477]17.2426,[478]17.2186,[479]17.2243,[480]17.2429,[481]17.2633,[482]17.2335,[483]17.2519,[484]17.2684,[485]17.2710,[486]17.2640,[487]17.2713,[488]17.2435,[489]17.2213,[490]17.2176,[491]17.1994,[492]17.1918,[493]17.1603,[494]17.1475,[495]17.1217,[496]17.1199,[497]17.1397,[498]17.1476,[499]17.1301,[500]17.1270,[501]17.1189,[502]17.1234,[503]17.1516,[504]17.1677,[505]17.1627,[506]17.1616,[507]17.1437,[508]17.1468,[509]17.1261,[510]17.1320,[511]17.1434,[512]17.1399,[513]17.1503,[514]17.1493,[515]17.1555,[516]17.1534,[517]17.1537,[518]17.1513,[519]17.1444,[520]17.1351,[521]17.1342,[522]17.1169,[523]17.1179,[524]17.1120,[525]17.1188,[526]17.1242,[527]17.1192,[528]17.1168,[529]17.1124,[530]17.0987,[531]17.0969,[532]17.0878,[533]17.0930,[534]17.0884,[535]17.0958,[536]17.0944,[537]17.1128,[538]17.1251,[539]17.1283,[540]17.1575,[541]17.1632,[542]17.1385,[543]17.1431,[544]17.1640,[545]17.1569,[546]17.1484,[547]17.1322,[548]17.1169,[549]17.1201,[550]17.0883,[551]17.0623,[552]17.0370,[553]16.9641,[554]16.9537,[555]16.9675,[556]16.9678,[557]16.9583,[558]16.9453,[559]16.9493,[560]16.9608,[561]16.9634,[562]16.9891,[563]17.0049,[564]16.9961,[565]17.0109,[566]17.0135,[567]16.9955,[568]16.9915,[569]16.9747,[570]16.9778,[571]16.9790,[572]17.0001,[573]17.0093,[574]17.0129,[575]17.0141,[576]17.0358,[577]17.0345,[578]17.0474,[579]17.0669,[580]17.0934,[581]17.0984,[582]17.1211,[583]17.1007,[584]17.1064,
Final estimate: PPL = 17.1064 +/- 0.15987

llama_perf_context_print:        load time =    2179.72 ms
llama_perf_context_print: prompt eval time = 4954395.64 ms / 299008 tokens (   16.57 ms per token,    60.35 tokens per second)
llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_perf_context_print:       total time = 5014731.66 ms / 299009 tokens
llama_perf_context_print:    graphs reused =          0
状态: 完成
正在处理变体目录: 4
-------------------------------------------
执行时间: 2025-12-21 22:07:50
测试配置: [变体: 4] [模型: Qwen3-0.6B-Q4_0.gguf] [线程: 64] [第 1 次运行]
命令: ./llama-perplexity -m /root/wmw/unsloth_riscv/Qwen3-0.6B-GGUF/Qwen3-0.6B-Q4_0.gguf -t 64 -f /root/wmw/hzeng_vw/riscv_kernels/e2e/rvv10/../wiki.test.raw
build: 0 (unknown) with riscv64-unknown-linux-gnu-gcc () 15.1.0 for riscv64-unknown-linux-gnu
llama_model_loader: loaded meta data with 32 key-value pairs and 310 tensors from /root/wmw/unsloth_riscv/Qwen3-0.6B-GGUF/Qwen3-0.6B-Q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3-0.6B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3-0.6B
llama_model_loader: - kv   4:                       general.quantized_by str              = Unsloth
llama_model_loader: - kv   5:                         general.size_label str              = 0.6B
llama_model_loader: - kv   6:                           general.repo_url str              = https://huggingface.co/unsloth
llama_model_loader: - kv   7:                          qwen3.block_count u32              = 28
llama_model_loader: - kv   8:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   9:                     qwen3.embedding_length u32              = 1024
llama_model_loader: - kv  10:                  qwen3.feed_forward_length u32              = 3072
llama_model_loader: - kv  11:                 qwen3.attention.head_count u32              = 16
llama_model_loader: - kv  12:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  13:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  14:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  15:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  16:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  22:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 151654
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 2
llama_model_loader: - kv  28:                      quantize.imatrix.file str              = Qwen3-0.6B-GGUF/imatrix_unsloth.dat
llama_model_loader: - kv  29:                   quantize.imatrix.dataset str              = unsloth_calibration_Qwen3-0.6B.txt
llama_model_loader: - kv  30:             quantize.imatrix.entries_count u32              = 196
llama_model_loader: - kv  31:              quantize.imatrix.chunks_count u32              = 688
llama_model_loader: - type  f32:  113 tensors
llama_model_loader: - type q4_0:  193 tensors
llama_model_loader: - type q4_1:    3 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 358.78 MiB (5.05 BPW) 
load: printing all EOG tokens:
load:   - 151643 ('<|endoftext|>')
load:   - 151645 ('<|im_end|>')
load:   - 151662 ('<|fim_pad|>')
load:   - 151663 ('<|repo_name|>')
load:   - 151664 ('<|file_sep|>')
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 1024
print_info: n_layer          = 28
print_info: n_head           = 16
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 2
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 3072
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: model type       = 0.6B
print_info: model params     = 596.05 M
print_info: general.name     = Qwen3-0.6B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 11 ','
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151654 '<|vision_pad|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors:   CPU_Mapped model buffer size =   358.78 MiB
....................................................................
llama_context: constructing llama_context
llama_context: n_seq_max     = 4
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 512
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (512) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     2.32 MiB
llama_kv_cache_unified:        CPU KV buffer size =   224.00 MiB
llama_kv_cache_unified: size =  224.00 MiB (   512 cells,  28 layers,  4/4 seqs), K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:        CPU compute buffer size =   298.75 MiB
llama_context: graph nodes  = 1098
llama_context: graph splits = 1
common_init_from_params: added <|endoftext|> logit bias = -inf
common_init_from_params: added <|im_end|> logit bias = -inf
common_init_from_params: added <|fim_pad|> logit bias = -inf
common_init_from_params: added <|repo_name|> logit bias = -inf
common_init_from_params: added <|file_sep|> logit bias = -inf
common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)

system_info: n_threads = 64 (n_threads_batch = 64) / 64 | CPU : RISCV_V = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 2854.24 ms
perplexity: calculating perplexity over 584 chunks, n_ctx=512, batch_size=2048, n_seq=4
perplexity: 25.33 seconds per pass - ETA 1 hours 1.63 minutes
[1]14.0800,[2]19.2844,[3]19.5544,[4]20.3107,[5]20.4576,[6]20.9998,[7]21.2697,[8]23.0400,[9]24.5671,[10]25.5547,[11]26.0234,[12]26.2751,[13]27.0734,[14]26.7024,[15]26.3964,[16]27.2144,[17]25.1741,[18]25.6498,[19]25.2676,[20]25.5269,[21]24.8917,[22]24.9623,[23]23.8507,[24]22.6143,[25]22.0927,[26]21.6181,[27]20.9119,[28]20.4830,[29]20.6856,[30]20.7177,[31]20.4425,[32]20.5470,[33]20.1592,[34]20.3870,[35]20.5950,[36]20.9518,[37]21.4138,[38]21.5605,[39]21.3522,[40]21.4426,[41]21.3863,[42]21.2674,[43]21.4963,[44]21.5745,[45]21.6784,[46]21.6417,[47]22.1396,[48]22.4072,[49]22.2738,[50]22.4899,[51]22.5309,[52]22.5511,[53]22.6062,[54]22.7842,[55]22.7790,[56]22.9126,[57]22.8263,[58]22.9171,[59]23.0673,[60]23.1992,[61]23.1879,[62]23.2796,[63]23.4669,[64]23.6635,[65]23.9470,[66]24.1673,[67]24.4083,[68]24.3075,[69]24.3193,[70]24.3566,[71]24.3751,[72]24.5435,[73]24.6079,[74]24.6127,[75]24.4627,[76]24.3537,[77]24.3418,[78]24.2910,[79]24.0986,[80]24.0525,[81]23.9071,[82]24.0046,[83]23.9616,[84]23.8887,[85]23.9173,[86]24.1279,[87]24.3138,[88]24.2439,[89]24.1808,[90]24.1199,[91]24.2180,[92]24.1376,[93]24.2787,[94]24.3970,[95]24.3268,[96]24.3832,[97]24.3466,[98]24.3545,[99]24.2840,[100]24.4355,[101]24.5224,[102]24.4757,[103]24.5412,[104]24.4286,[105]24.4679,[106]24.3407,[107]24.3672,[108]24.4958,[109]24.5755,[110]24.6890,[111]24.9766,[112]25.0444,[113]24.8933,[114]25.0032,[115]25.0416,[116]24.9695,[117]24.9657,[118]24.9251,[119]24.8023,[120]24.8570,[121]24.8166,[122]24.8563,[123]24.8029,[124]24.6736,[125]24.6176,[126]24.5968,[127]24.5483,[128]24.4818,[129]24.4655,[130]24.3817,[131]24.2310,[132]24.1491,[133]24.0795,[134]24.0926,[135]24.0792,[136]24.0431,[137]23.9187,[138]23.7927,[139]23.8207,[140]23.7463,[141]23.7295,[142]23.7178,[143]23.7702,[144]23.8259,[145]23.7706,[146]23.6113,[147]23.4804,[148]23.3759,[149]23.3132,[150]23.1716,[151]23.1328,[152]23.1224,[153]23.0867,[154]22.9605,[155]22.9944,[156]22.9276,[157]22.8761,[158]22.7771,[159]22.7311,[160]22.6840,[161]22.6652,[162]22.6658,[163]22.6621,[164]22.6624,[165]22.6299,[166]22.6559,[167]22.5646,[168]22.6126,[169]22.5760,[170]22.7193,[171]22.8031,[172]22.9167,[173]23.0458,[174]23.0853,[175]23.1856,[176]23.2902,[177]23.3988,[178]23.4878,[179]23.5460,[180]23.5767,[181]23.6723,[182]23.7608,[183]23.8428,[184]23.9423,[185]24.0073,[186]24.0205,[187]24.0192,[188]24.0301,[189]24.0514,[190]24.1157,[191]24.1271,[192]24.1511,[193]24.1472,[194]24.2097,[195]24.2391,[196]24.2710,[197]24.2731,[198]24.1904,[199]24.1416,[200]24.1273,[201]24.1691,[202]24.2023,[203]24.2430,[204]24.2563,[205]24.2485,[206]24.2098,[207]24.2615,[208]24.1747,[209]24.1633,[210]24.1628,[211]24.1542,[212]24.1528,[213]24.1629,[214]24.1049,[215]24.0346,[216]24.0430,[217]24.0467,[218]24.0064,[219]23.9253,[220]23.8685,[221]23.8152,[222]23.7717,[223]23.7701,[224]23.7812,[225]23.7190,[226]23.7259,[227]23.6958,[228]23.6233,[229]23.5310,[230]23.4681,[231]23.4165,[232]23.3746,[233]23.3858,[234]23.4002,[235]23.3768,[236]23.3171,[237]23.2795,[238]23.1949,[239]23.1594,[240]23.1812,[241]23.1568,[242]23.1803,[243]23.1964,[244]23.2356,[245]23.2422,[246]23.3049,[247]23.3031,[248]23.3287,[249]23.3520,[250]23.3535,[251]23.4235,[252]23.4267,[253]23.4868,[254]23.5271,[255]23.5376,[256]23.6001,[257]23.6089,[258]23.5529,[259]23.5101,[260]23.4812,[261]23.4401,[262]23.4222,[263]23.4074,[264]23.4096,[265]23.4629,[266]23.4903,[267]23.4958,[268]23.4662,[269]23.4604,[270]23.4746,[271]23.4339,[272]23.4422,[273]23.4417,[274]23.4304,[275]23.4271,[276]23.3756,[277]23.3432,[278]23.3717,[279]23.3682,[280]23.3533,[281]23.3451,[282]23.4202,[283]23.3263,[284]23.2203,[285]23.2309,[286]23.1592,[287]23.0782,[288]23.0927,[289]23.0928,[290]23.1717,[291]23.1711,[292]23.1640,[293]23.1655,[294]23.2170,[295]23.2754,[296]23.3284,[297]23.3799,[298]23.3780,[299]23.3357,[300]23.3395,[301]23.3175,[302]23.3217,[303]23.3051,[304]23.3302,[305]23.3101,[306]23.2922,[307]23.3023,[308]23.2812,[309]23.2719,[310]23.3108,[311]23.3181,[312]23.2740,[313]23.2690,[314]23.3012,[315]23.2605,[316]23.3141,[317]23.3729,[318]23.3665,[319]23.3389,[320]23.3535,[321]23.2965,[322]23.3138,[323]23.3540,[324]23.3832,[325]23.4237,[326]23.4319,[327]23.3886,[328]23.3850,[329]23.3185,[330]23.2911,[331]23.2474,[332]23.2350,[333]23.2309,[334]23.1980,[335]23.1478,[336]23.1264,[337]23.1492,[338]23.1752,[339]23.1528,[340]23.1135,[341]23.0813,[342]23.0780,[343]23.0728,[344]23.0955,[345]23.1131,[346]23.1143,[347]23.0972,[348]23.1062,[349]23.0960,[350]23.0995,[351]23.1233,[352]23.1300,[353]23.1292,[354]23.0733,[355]23.1166,[356]23.1304,[357]23.1390,[358]23.1086,[359]23.1057,[360]23.1011,[361]23.1075,[362]23.1354,[363]23.1285,[364]23.1732,[365]23.2004,[366]23.2729,[367]23.3281,[368]23.4017,[369]23.4605,[370]23.4975,[371]23.5512,[372]23.6068,[373]23.6277,[374]23.6565,[375]23.7167,[376]23.7577,[377]23.7844,[378]23.8369,[379]23.8787,[380]23.9284,[381]23.9532,[382]23.9884,[383]24.0189,[384]24.0628,[385]24.1294,[386]24.1705,[387]24.1630,[388]24.1401,[389]24.1815,[390]24.2369,[391]24.2822,[392]24.2777,[393]24.2746,[394]24.2535,[395]24.2595,[396]24.2810,[397]24.2889,[398]24.2952,[399]24.2991,[400]24.3386,[401]24.3346,[402]24.3380,[403]24.3141,[404]24.3152,[405]24.2924,[406]24.2848,[407]24.2919,[408]24.3064,[409]24.3077,[410]24.3063,[411]24.3474,[412]24.3594,[413]24.3736,[414]24.3719,[415]24.3676,[416]24.3623,[417]24.3728,[418]24.3851,[419]24.3777,[420]24.3731,[421]24.3814,[422]24.3429,[423]24.3551,[424]24.3645,[425]24.3749,[426]24.3995,[427]24.4336,[428]24.4809,[429]24.4879,[430]24.4661,[431]24.4317,[432]24.4319,[433]24.4259,[434]24.4074,[435]24.4227,[436]24.3530,[437]24.3470,[438]24.3516,[439]24.3230,[440]24.3566,[441]24.3678,[442]24.3540,[443]24.3445,[444]24.3584,[445]24.3194,[446]24.3292,[447]24.3260,[448]24.3173,[449]24.3113,[450]24.3324,[451]24.3247,[452]24.3203,[453]24.2999,[454]24.2793,[455]24.2888,[456]24.2855,[457]24.2961,[458]24.3368,[459]24.3439,[460]24.3409,[461]24.3436,[462]24.3548,[463]24.3822,[464]24.3866,[465]24.3969,[466]24.3975,[467]24.4124,[468]24.4379,[469]24.4584,[470]24.4842,[471]24.4719,[472]24.4935,[473]24.4496,[474]24.4431,[475]24.4540,[476]24.4680,[477]24.4536,[478]24.4103,[479]24.4047,[480]24.4251,[481]24.4408,[482]24.4025,[483]24.4133,[484]24.4295,[485]24.4324,[486]24.4184,[487]24.4361,[488]24.3971,[489]24.3686,[490]24.3630,[491]24.3320,[492]24.3219,[493]24.2782,[494]24.2665,[495]24.2282,[496]24.2255,[497]24.2527,[498]24.2723,[499]24.2528,[500]24.2628,[501]24.2536,[502]24.2625,[503]24.3086,[504]24.3292,[505]24.3292,[506]24.3094,[507]24.2833,[508]24.2923,[509]24.2665,[510]24.2691,[511]24.2799,[512]24.2677,[513]24.2808,[514]24.2734,[515]24.2686,[516]24.2703,[517]24.2660,[518]24.2544,[519]24.2419,[520]24.2235,[521]24.2221,[522]24.2041,[523]24.2078,[524]24.1975,[525]24.2047,[526]24.2128,[527]24.1991,[528]24.1890,[529]24.1712,[530]24.1587,[531]24.1617,[532]24.1545,[533]24.1681,[534]24.1625,[535]24.1607,[536]24.1454,[537]24.1669,[538]24.1854,[539]24.1840,[540]24.2227,[541]24.2271,[542]24.1956,[543]24.1945,[544]24.2115,[545]24.2013,[546]24.1889,[547]24.1726,[548]24.1426,[549]24.1533,[550]24.1004,[551]24.0530,[552]24.0178,[553]23.9032,[554]23.8890,[555]23.8970,[556]23.8933,[557]23.8804,[558]23.8689,[559]23.8734,[560]23.8881,[561]23.8935,[562]23.9271,[563]23.9524,[564]23.9457,[565]23.9644,[566]23.9746,[567]23.9531,[568]23.9555,[569]23.9376,[570]23.9564,[571]23.9574,[572]23.9857,[573]23.9948,[574]24.0015,[575]23.9993,[576]24.0237,[577]24.0216,[578]24.0333,[579]24.0676,[580]24.1037,[581]24.1123,[582]24.1488,[583]24.1288,[584]24.1405,
Final estimate: PPL = 24.1405 +/- 0.20864

llama_perf_context_print:        load time =    1591.21 ms
llama_perf_context_print: prompt eval time = 3514944.40 ms / 299008 tokens (   11.76 ms per token,    85.07 tokens per second)
llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_perf_context_print:       total time = 3575033.46 ms / 299009 tokens
llama_perf_context_print:    graphs reused =          0
状态: 完成
-------------------------------------------
执行时间: 2025-12-21 23:07:27
测试配置: [变体: 4] [模型: Qwen3-0.6B-Q8_0.gguf] [线程: 64] [第 1 次运行]
命令: ./llama-perplexity -m /root/wmw/unsloth_riscv/Qwen3-0.6B-GGUF/Qwen3-0.6B-Q8_0.gguf -t 64 -f /root/wmw/hzeng_vw/riscv_kernels/e2e/rvv10/../wiki.test.raw
build: 0 (unknown) with riscv64-unknown-linux-gnu-gcc () 15.1.0 for riscv64-unknown-linux-gnu
llama_model_loader: loaded meta data with 32 key-value pairs and 310 tensors from /root/wmw/unsloth_riscv/Qwen3-0.6B-GGUF/Qwen3-0.6B-Q8_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3-0.6B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3-0.6B
llama_model_loader: - kv   4:                       general.quantized_by str              = Unsloth
llama_model_loader: - kv   5:                         general.size_label str              = 0.6B
llama_model_loader: - kv   6:                           general.repo_url str              = https://huggingface.co/unsloth
llama_model_loader: - kv   7:                          qwen3.block_count u32              = 28
llama_model_loader: - kv   8:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   9:                     qwen3.embedding_length u32              = 1024
llama_model_loader: - kv  10:                  qwen3.feed_forward_length u32              = 3072
llama_model_loader: - kv  11:                 qwen3.attention.head_count u32              = 16
llama_model_loader: - kv  12:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  13:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  14:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  15:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  16:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  22:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 151654
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 7
llama_model_loader: - kv  28:                      quantize.imatrix.file str              = Qwen3-0.6B-GGUF/imatrix_unsloth.dat
llama_model_loader: - kv  29:                   quantize.imatrix.dataset str              = unsloth_calibration_Qwen3-0.6B.txt
llama_model_loader: - kv  30:             quantize.imatrix.entries_count u32              = 196
llama_model_loader: - kv  31:              quantize.imatrix.chunks_count u32              = 688
llama_model_loader: - type  f32:  113 tensors
llama_model_loader: - type q8_0:  197 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q8_0
print_info: file size   = 604.15 MiB (8.50 BPW) 
load: printing all EOG tokens:
load:   - 151643 ('<|endoftext|>')
load:   - 151645 ('<|im_end|>')
load:   - 151662 ('<|fim_pad|>')
load:   - 151663 ('<|repo_name|>')
load:   - 151664 ('<|file_sep|>')
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 1024
print_info: n_layer          = 28
print_info: n_head           = 16
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 2
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 3072
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: model type       = 0.6B
print_info: model params     = 596.05 M
print_info: general.name     = Qwen3-0.6B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 11 ','
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151654 '<|vision_pad|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors:   CPU_Mapped model buffer size =   604.15 MiB
...........................................................................
llama_context: constructing llama_context
llama_context: n_seq_max     = 4
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 512
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (512) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     2.32 MiB
llama_kv_cache_unified:        CPU KV buffer size =   224.00 MiB
llama_kv_cache_unified: size =  224.00 MiB (   512 cells,  28 layers,  4/4 seqs), K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:        CPU compute buffer size =   298.75 MiB
llama_context: graph nodes  = 1098
llama_context: graph splits = 1
common_init_from_params: added <|endoftext|> logit bias = -inf
common_init_from_params: added <|im_end|> logit bias = -inf
common_init_from_params: added <|fim_pad|> logit bias = -inf
common_init_from_params: added <|repo_name|> logit bias = -inf
common_init_from_params: added <|file_sep|> logit bias = -inf
common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)

system_info: n_threads = 64 (n_threads_batch = 64) / 64 | CPU : RISCV_V = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 2891.9 ms
perplexity: calculating perplexity over 584 chunks, n_ctx=512, batch_size=2048, n_seq=4
perplexity: 24.99 seconds per pass - ETA 1 hours 0.80 minutes
[1]12.7218,[2]18.1444,[3]18.2077,[4]19.2079,[5]19.5436,[6]19.9973,[7]20.1821,[8]21.7043,[9]22.9801,[10]23.7626,[11]24.1758,[12]24.4121,[13]25.2263,[14]24.8251,[15]24.5261,[16]25.1405,[17]23.3032,[18]23.7863,[19]23.4752,[20]23.6572,[21]23.1021,[22]23.0789,[23]22.0705,[24]20.8533,[25]20.3940,[26]19.9129,[27]19.2291,[28]18.8129,[29]18.9986,[30]19.0134,[31]18.7715,[32]18.8456,[33]18.4917,[34]18.6750,[35]18.8333,[36]19.1501,[37]19.5518,[38]19.6925,[39]19.5036,[40]19.6187,[41]19.5964,[42]19.4557,[43]19.6451,[44]19.6970,[45]19.7818,[46]19.7389,[47]20.1608,[48]20.4007,[49]20.2608,[50]20.4624,[51]20.5095,[52]20.5432,[53]20.6179,[54]20.7694,[55]20.7347,[56]20.8617,[57]20.7854,[58]20.8561,[59]20.9944,[60]21.1158,[61]21.1048,[62]21.1688,[63]21.3300,[64]21.5021,[65]21.7539,[66]21.9431,[67]22.1562,[68]22.0724,[69]22.0862,[70]22.1172,[71]22.1389,[72]22.2928,[73]22.3431,[74]22.3471,[75]22.2139,[76]22.1214,[77]22.1398,[78]22.0902,[79]21.9197,[80]21.8839,[81]21.7541,[82]21.8507,[83]21.8049,[84]21.7212,[85]21.7294,[86]21.9304,[87]22.0946,[88]22.0394,[89]21.9675,[90]21.8996,[91]21.9968,[92]21.9257,[93]22.0600,[94]22.1725,[95]22.1062,[96]22.1388,[97]22.0943,[98]22.1091,[99]22.0636,[100]22.2165,[101]22.3066,[102]22.2732,[103]22.3379,[104]22.2427,[105]22.2789,[106]22.1780,[107]22.2088,[108]22.3313,[109]22.4093,[110]22.5291,[111]22.8008,[112]22.8452,[113]22.7011,[114]22.7976,[115]22.8462,[116]22.7774,[117]22.7633,[118]22.7138,[119]22.5813,[120]22.6302,[121]22.5927,[122]22.6472,[123]22.5972,[124]22.4815,[125]22.4383,[126]22.4229,[127]22.3923,[128]22.3188,[129]22.2914,[130]22.2080,[131]22.0739,[132]21.9860,[133]21.9197,[134]21.9301,[135]21.9186,[136]21.8864,[137]21.7876,[138]21.6774,[139]21.7004,[140]21.6411,[141]21.6423,[142]21.6336,[143]21.6832,[144]21.7350,[145]21.6811,[146]21.5371,[147]21.4179,[148]21.3209,[149]21.2576,[150]21.1238,[151]21.0814,[152]21.0752,[153]21.0400,[154]20.9374,[155]20.9789,[156]20.9191,[157]20.8730,[158]20.8010,[159]20.7510,[160]20.7156,[161]20.6933,[162]20.6919,[163]20.6888,[164]20.6776,[165]20.6457,[166]20.6640,[167]20.5865,[168]20.6244,[169]20.5905,[170]20.7223,[171]20.8003,[172]20.9036,[173]21.0220,[174]21.0494,[175]21.1419,[176]21.2358,[177]21.3374,[178]21.4125,[179]21.4766,[180]21.4987,[181]21.5897,[182]21.6743,[183]21.7517,[184]21.8452,[185]21.9027,[186]21.9152,[187]21.9161,[188]21.9197,[189]21.9325,[190]21.9950,[191]21.9994,[192]22.0111,[193]21.9959,[194]22.0498,[195]22.0791,[196]22.1070,[197]22.1126,[198]22.0359,[199]21.9933,[200]21.9822,[201]22.0239,[202]22.0552,[203]22.0936,[204]22.1102,[205]22.1070,[206]22.0753,[207]22.1288,[208]22.0502,[209]22.0384,[210]22.0359,[211]22.0294,[212]22.0293,[213]22.0352,[214]21.9896,[215]21.9233,[216]21.9237,[217]21.9211,[218]21.8888,[219]21.8206,[220]21.7721,[221]21.7291,[222]21.6832,[223]21.6840,[224]21.6924,[225]21.6291,[226]21.6344,[227]21.6103,[228]21.5412,[229]21.4626,[230]21.3965,[231]21.3488,[232]21.3126,[233]21.3157,[234]21.3245,[235]21.3092,[236]21.2571,[237]21.2251,[238]21.1498,[239]21.1158,[240]21.1384,[241]21.1164,[242]21.1388,[243]21.1551,[244]21.1914,[245]21.1898,[246]21.2528,[247]21.2464,[248]21.2597,[249]21.2918,[250]21.2921,[251]21.3524,[252]21.3532,[253]21.4114,[254]21.4443,[255]21.4527,[256]21.5011,[257]21.5064,[258]21.4563,[259]21.4095,[260]21.3767,[261]21.3424,[262]21.3237,[263]21.3084,[264]21.3121,[265]21.3597,[266]21.3778,[267]21.3867,[268]21.3616,[269]21.3590,[270]21.3706,[271]21.3349,[272]21.3461,[273]21.3413,[274]21.3375,[275]21.3317,[276]21.2760,[277]21.2487,[278]21.2665,[279]21.2616,[280]21.2509,[281]21.2411,[282]21.3022,[283]21.2067,[284]21.1140,[285]21.1266,[286]21.0675,[287]20.9947,[288]21.0046,[289]21.0094,[290]21.0840,[291]21.0854,[292]21.0816,[293]21.0876,[294]21.1332,[295]21.1834,[296]21.2348,[297]21.2813,[298]21.2731,[299]21.2310,[300]21.2274,[301]21.2055,[302]21.2115,[303]21.2002,[304]21.2248,[305]21.2049,[306]21.1904,[307]21.2004,[308]21.1824,[309]21.1701,[310]21.2034,[311]21.2048,[312]21.1631,[313]21.1551,[314]21.1727,[315]21.1327,[316]21.1858,[317]21.2425,[318]21.2384,[319]21.2156,[320]21.2326,[321]21.1876,[322]21.2074,[323]21.2403,[324]21.2719,[325]21.3061,[326]21.3208,[327]21.2833,[328]21.2773,[329]21.2178,[330]21.1958,[331]21.1529,[332]21.1411,[333]21.1369,[334]21.1032,[335]21.0601,[336]21.0439,[337]21.0588,[338]21.0863,[339]21.0642,[340]21.0282,[341]20.9997,[342]20.9972,[343]20.9920,[344]21.0157,[345]21.0277,[346]21.0308,[347]21.0211,[348]21.0273,[349]21.0146,[350]21.0137,[351]21.0374,[352]21.0433,[353]21.0422,[354]20.9922,[355]21.0332,[356]21.0492,[357]21.0573,[358]21.0332,[359]21.0300,[360]21.0249,[361]21.0306,[362]21.0538,[363]21.0496,[364]21.0874,[365]21.1187,[366]21.1855,[367]21.2364,[368]21.3032,[369]21.3587,[370]21.3972,[371]21.4472,[372]21.4966,[373]21.5189,[374]21.5460,[375]21.6030,[376]21.6413,[377]21.6625,[378]21.7020,[379]21.7385,[380]21.7808,[381]21.8087,[382]21.8401,[383]21.8687,[384]21.9156,[385]21.9761,[386]22.0142,[387]22.0138,[388]21.9930,[389]22.0315,[390]22.0804,[391]22.1212,[392]22.1211,[393]22.1193,[394]22.0964,[395]22.1005,[396]22.1194,[397]22.1251,[398]22.1315,[399]22.1332,[400]22.1738,[401]22.1700,[402]22.1750,[403]22.1513,[404]22.1472,[405]22.1263,[406]22.1241,[407]22.1273,[408]22.1381,[409]22.1401,[410]22.1409,[411]22.1779,[412]22.1880,[413]22.1974,[414]22.1907,[415]22.1877,[416]22.1766,[417]22.1865,[418]22.1968,[419]22.1904,[420]22.1815,[421]22.1904,[422]22.1600,[423]22.1716,[424]22.1810,[425]22.1895,[426]22.2079,[427]22.2402,[428]22.2850,[429]22.2877,[430]22.2656,[431]22.2366,[432]22.2372,[433]22.2298,[434]22.2154,[435]22.2309,[436]22.1685,[437]22.1611,[438]22.1643,[439]22.1375,[440]22.1642,[441]22.1727,[442]22.1608,[443]22.1509,[444]22.1624,[445]22.1287,[446]22.1361,[447]22.1311,[448]22.1194,[449]22.1087,[450]22.1250,[451]22.1166,[452]22.1099,[453]22.0895,[454]22.0717,[455]22.0762,[456]22.0704,[457]22.0846,[458]22.1195,[459]22.1263,[460]22.1220,[461]22.1274,[462]22.1368,[463]22.1623,[464]22.1644,[465]22.1722,[466]22.1737,[467]22.1859,[468]22.2093,[469]22.2253,[470]22.2470,[471]22.2382,[472]22.2597,[473]22.2184,[474]22.2138,[475]22.2243,[476]22.2392,[477]22.2269,[478]22.1880,[479]22.1843,[480]22.2077,[481]22.2251,[482]22.1877,[483]22.1983,[484]22.2181,[485]22.2193,[486]22.2069,[487]22.2225,[488]22.1880,[489]22.1618,[490]22.1587,[491]22.1319,[492]22.1234,[493]22.0854,[494]22.0753,[495]22.0393,[496]22.0372,[497]22.0583,[498]22.0769,[499]22.0557,[500]22.0616,[501]22.0515,[502]22.0583,[503]22.0997,[504]22.1196,[505]22.1220,[506]22.1069,[507]22.0853,[508]22.0915,[509]22.0674,[510]22.0689,[511]22.0787,[512]22.0671,[513]22.0818,[514]22.0786,[515]22.0753,[516]22.0754,[517]22.0715,[518]22.0569,[519]22.0447,[520]22.0294,[521]22.0251,[522]22.0053,[523]22.0059,[524]21.9984,[525]22.0062,[526]22.0155,[527]22.0031,[528]21.9935,[529]21.9811,[530]21.9698,[531]21.9731,[532]21.9684,[533]21.9804,[534]21.9795,[535]21.9798,[536]21.9671,[537]21.9864,[538]22.0029,[539]22.0021,[540]22.0364,[541]22.0407,[542]22.0125,[543]22.0131,[544]22.0289,[545]22.0185,[546]22.0092,[547]21.9937,[548]21.9670,[549]21.9769,[550]21.9308,[551]21.8877,[552]21.8558,[553]21.7534,[554]21.7396,[555]21.7492,[556]21.7461,[557]21.7358,[558]21.7215,[559]21.7234,[560]21.7371,[561]21.7410,[562]21.7698,[563]21.7927,[564]21.7849,[565]21.8041,[566]21.8120,[567]21.7911,[568]21.7909,[569]21.7737,[570]21.7922,[571]21.7949,[572]21.8214,[573]21.8298,[574]21.8363,[575]21.8324,[576]21.8549,[577]21.8532,[578]21.8604,[579]21.8933,[580]21.9284,[581]21.9386,[582]21.9731,[583]21.9531,[584]21.9645,
Final estimate: PPL = 21.9645 +/- 0.19168

llama_perf_context_print:        load time =    1650.53 ms
llama_perf_context_print: prompt eval time = 3437721.52 ms / 299008 tokens (   11.50 ms per token,    86.98 tokens per second)
llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_perf_context_print:       total time = 3498118.44 ms / 299009 tokens
llama_perf_context_print:    graphs reused =          0
状态: 完成
-------------------------------------------
执行时间: 2025-12-22 00:05:48
测试配置: [变体: 4] [模型: Qwen3-1.7B-Q4_0.gguf] [线程: 64] [第 1 次运行]
命令: ./llama-perplexity -m /root/wmw/unsloth_riscv/Qwen3-1.7B-GGUF/Qwen3-1.7B-Q4_0.gguf -t 64 -f /root/wmw/hzeng_vw/riscv_kernels/e2e/rvv10/../wiki.test.raw
build: 0 (unknown) with riscv64-unknown-linux-gnu-gcc () 15.1.0 for riscv64-unknown-linux-gnu
llama_model_loader: loaded meta data with 32 key-value pairs and 310 tensors from /root/wmw/unsloth_riscv/Qwen3-1.7B-GGUF/Qwen3-1.7B-Q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3-1.7B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3-1.7B
llama_model_loader: - kv   4:                       general.quantized_by str              = Unsloth
llama_model_loader: - kv   5:                         general.size_label str              = 1.7B
llama_model_loader: - kv   6:                           general.repo_url str              = https://huggingface.co/unsloth
llama_model_loader: - kv   7:                          qwen3.block_count u32              = 28
llama_model_loader: - kv   8:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   9:                     qwen3.embedding_length u32              = 2048
llama_model_loader: - kv  10:                  qwen3.feed_forward_length u32              = 6144
llama_model_loader: - kv  11:                 qwen3.attention.head_count u32              = 16
llama_model_loader: - kv  12:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  13:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  14:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  15:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  16:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  22:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 151654
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 2
llama_model_loader: - kv  28:                      quantize.imatrix.file str              = Qwen3-1.7B-GGUF/imatrix_unsloth.dat
llama_model_loader: - kv  29:                   quantize.imatrix.dataset str              = unsloth_calibration_Qwen3-1.7B.txt
llama_model_loader: - kv  30:             quantize.imatrix.entries_count i32              = 196
llama_model_loader: - kv  31:              quantize.imatrix.chunks_count i32              = 685
llama_model_loader: - type  f32:  113 tensors
llama_model_loader: - type q4_0:  193 tensors
llama_model_loader: - type q4_1:    3 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 1002.15 MiB (4.89 BPW) 
load: printing all EOG tokens:
load:   - 151643 ('<|endoftext|>')
load:   - 151645 ('<|im_end|>')
load:   - 151662 ('<|fim_pad|>')
load:   - 151663 ('<|repo_name|>')
load:   - 151664 ('<|file_sep|>')
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 2048
print_info: n_layer          = 28
print_info: n_head           = 16
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 2
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 6144
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: model type       = 1.7B
print_info: model params     = 1.72 B
print_info: general.name     = Qwen3-1.7B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 11 ','
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151654 '<|vision_pad|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors:   CPU_Mapped model buffer size =  1002.15 MiB
.............................................................................
llama_context: constructing llama_context
llama_context: n_seq_max     = 4
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 512
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (512) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     2.32 MiB
llama_kv_cache_unified:        CPU KV buffer size =   224.00 MiB
llama_kv_cache_unified: size =  224.00 MiB (   512 cells,  28 layers,  4/4 seqs), K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:        CPU compute buffer size =   300.75 MiB
llama_context: graph nodes  = 1098
llama_context: graph splits = 1
common_init_from_params: added <|endoftext|> logit bias = -inf
common_init_from_params: added <|im_end|> logit bias = -inf
common_init_from_params: added <|fim_pad|> logit bias = -inf
common_init_from_params: added <|repo_name|> logit bias = -inf
common_init_from_params: added <|file_sep|> logit bias = -inf
common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)

system_info: n_threads = 64 (n_threads_batch = 64) / 64 | CPU : RISCV_V = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 2930.93 ms
perplexity: calculating perplexity over 584 chunks, n_ctx=512, batch_size=2048, n_seq=4
perplexity: 36.17 seconds per pass - ETA 1 hours 28.00 minutes
[1]13.1943,[2]17.7785,[3]17.1292,[4]17.6263,[5]17.9983,[6]17.9040,[7]18.2787,[8]19.2154,[9]20.4891,[10]20.9468,[11]20.9956,[12]21.0120,[13]21.6065,[14]21.2601,[15]20.9779,[16]21.4802,[17]20.2135,[18]20.4463,[19]20.1820,[20]20.1719,[21]19.7557,[22]19.7496,[23]18.9847,[24]18.0065,[25]17.5789,[26]17.0362,[27]16.4915,[28]16.4185,[29]16.5370,[30]16.4891,[31]16.2476,[32]16.2267,[33]15.9556,[34]16.1744,[35]16.2718,[36]16.5077,[37]16.7740,[38]16.8740,[39]16.7616,[40]16.8479,[41]16.8089,[42]16.7199,[43]16.9507,[44]17.0279,[45]17.0120,[46]16.9661,[47]17.2976,[48]17.4322,[49]17.3473,[50]17.4466,[51]17.5060,[52]17.5328,[53]17.6038,[54]17.6450,[55]17.6239,[56]17.7695,[57]17.6663,[58]17.7090,[59]17.7996,[60]17.8750,[61]17.8948,[62]17.9500,[63]18.0836,[64]18.2279,[65]18.4378,[66]18.5657,[67]18.7207,[68]18.6188,[69]18.6095,[70]18.5544,[71]18.5815,[72]18.7000,[73]18.7495,[74]18.7418,[75]18.6130,[76]18.5673,[77]18.5847,[78]18.5923,[79]18.4285,[80]18.4182,[81]18.3264,[82]18.4058,[83]18.3779,[84]18.3482,[85]18.4277,[86]18.5861,[87]18.6905,[88]18.6470,[89]18.6274,[90]18.5783,[91]18.6544,[92]18.5963,[93]18.7311,[94]18.7666,[95]18.7060,[96]18.6888,[97]18.6575,[98]18.6590,[99]18.6466,[100]18.7915,[101]18.8572,[102]18.8184,[103]18.8834,[104]18.7885,[105]18.8257,[106]18.7800,[107]18.8337,[108]18.9154,[109]18.9663,[110]19.0567,[111]19.2561,[112]19.2993,[113]19.1551,[114]19.2114,[115]19.2570,[116]19.2038,[117]19.1794,[118]19.1298,[119]19.0015,[120]19.0504,[121]18.9946,[122]19.0350,[123]19.0045,[124]18.9201,[125]18.8825,[126]18.8998,[127]18.8536,[128]18.8079,[129]18.7743,[130]18.7232,[131]18.6164,[132]18.5225,[133]18.4648,[134]18.4633,[135]18.4418,[136]18.4058,[137]18.3187,[138]18.2458,[139]18.2614,[140]18.2569,[141]18.2593,[142]18.2738,[143]18.3108,[144]18.3876,[145]18.3486,[146]18.2633,[147]18.1806,[148]18.1179,[149]18.0926,[150]17.9941,[151]18.0143,[152]17.9877,[153]17.9741,[154]17.8712,[155]17.9164,[156]17.8347,[157]17.7929,[158]17.7281,[159]17.6774,[160]17.6091,[161]17.5675,[162]17.5636,[163]17.5656,[164]17.5676,[165]17.5236,[166]17.5094,[167]17.4612,[168]17.5171,[169]17.5053,[170]17.5866,[171]17.6721,[172]17.7511,[173]17.8359,[174]17.8838,[175]17.9603,[176]18.0399,[177]18.1472,[178]18.2269,[179]18.2862,[180]18.2943,[181]18.4096,[182]18.4644,[183]18.5325,[184]18.5971,[185]18.6486,[186]18.6777,[187]18.6902,[188]18.6865,[189]18.7091,[190]18.7562,[191]18.7614,[192]18.7974,[193]18.8065,[194]18.8442,[195]18.8669,[196]18.8732,[197]18.8789,[198]18.8205,[199]18.7940,[200]18.7822,[201]18.8476,[202]18.8763,[203]18.9073,[204]18.9287,[205]18.9219,[206]18.9161,[207]18.9710,[208]18.9205,[209]18.9186,[210]18.9276,[211]18.9272,[212]18.9281,[213]18.9780,[214]18.9590,[215]18.9135,[216]18.9046,[217]18.9011,[218]18.8965,[219]18.8262,[220]18.7984,[221]18.7537,[222]18.7187,[223]18.7106,[224]18.7342,[225]18.6803,[226]18.6735,[227]18.6449,[228]18.5800,[229]18.5196,[230]18.4712,[231]18.4243,[232]18.3830,[233]18.3771,[234]18.3873,[235]18.3747,[236]18.3489,[237]18.3035,[238]18.2378,[239]18.1937,[240]18.1866,[241]18.1672,[242]18.1697,[243]18.1944,[244]18.2311,[245]18.2374,[246]18.2987,[247]18.2912,[248]18.3180,[249]18.3391,[250]18.3351,[251]18.3678,[252]18.3833,[253]18.4335,[254]18.4522,[255]18.4440,[256]18.4870,[257]18.4956,[258]18.4627,[259]18.4097,[260]18.3781,[261]18.3361,[262]18.3170,[263]18.3269,[264]18.3273,[265]18.3534,[266]18.3899,[267]18.3953,[268]18.3855,[269]18.3911,[270]18.4242,[271]18.3873,[272]18.3974,[273]18.3967,[274]18.4004,[275]18.3962,[276]18.3507,[277]18.3359,[278]18.3548,[279]18.3528,[280]18.3388,[281]18.3398,[282]18.3856,[283]18.3138,[284]18.2374,[285]18.2473,[286]18.1890,[287]18.1272,[288]18.1170,[289]18.1224,[290]18.1780,[291]18.1746,[292]18.1657,[293]18.1632,[294]18.2022,[295]18.2674,[296]18.3318,[297]18.3642,[298]18.3546,[299]18.3209,[300]18.3235,[301]18.3171,[302]18.3228,[303]18.3084,[304]18.3333,[305]18.3166,[306]18.3040,[307]18.3172,[308]18.3103,[309]18.3055,[310]18.3326,[311]18.3328,[312]18.3079,[313]18.2974,[314]18.3083,[315]18.2725,[316]18.3124,[317]18.3621,[318]18.3679,[319]18.3431,[320]18.3505,[321]18.3151,[322]18.3457,[323]18.3747,[324]18.4053,[325]18.4312,[326]18.4620,[327]18.4347,[328]18.4334,[329]18.3898,[330]18.3951,[331]18.3682,[332]18.3668,[333]18.3687,[334]18.3471,[335]18.3089,[336]18.3073,[337]18.3201,[338]18.3351,[339]18.3332,[340]18.3492,[341]18.3190,[342]18.3276,[343]18.3124,[344]18.3221,[345]18.3193,[346]18.3144,[347]18.2872,[348]18.2991,[349]18.2956,[350]18.2949,[351]18.3098,[352]18.3100,[353]18.3002,[354]18.2633,[355]18.2899,[356]18.3219,[357]18.3424,[358]18.3263,[359]18.3213,[360]18.3416,[361]18.3468,[362]18.3565,[363]18.3542,[364]18.4970,[365]18.5273,[366]18.5894,[367]18.6311,[368]18.6900,[369]18.7209,[370]18.7475,[371]18.7835,[372]18.8242,[373]18.8406,[374]18.8676,[375]18.9127,[376]18.9595,[377]18.9799,[378]19.0220,[379]19.0464,[380]19.0749,[381]19.1008,[382]19.1201,[383]19.1326,[384]19.1705,[385]19.2281,[386]19.2809,[387]19.2892,[388]19.2733,[389]19.2967,[390]19.3373,[391]19.3668,[392]19.3547,[393]19.3485,[394]19.3304,[395]19.3479,[396]19.3577,[397]19.3587,[398]19.3778,[399]19.3799,[400]19.4130,[401]19.4238,[402]19.4303,[403]19.4042,[404]19.3881,[405]19.3725,[406]19.3666,[407]19.3714,[408]19.3833,[409]19.3783,[410]19.3807,[411]19.4069,[412]19.4224,[413]19.4260,[414]19.4151,[415]19.3962,[416]19.3902,[417]19.4011,[418]19.4100,[419]19.4066,[420]19.3902,[421]19.3917,[422]19.3600,[423]19.3658,[424]19.3725,[425]19.3758,[426]19.3898,[427]19.4288,[428]19.4622,[429]19.4749,[430]19.4582,[431]19.4381,[432]19.4474,[433]19.4447,[434]19.4299,[435]19.4494,[436]19.4058,[437]19.4008,[438]19.3979,[439]19.3795,[440]19.4000,[441]19.4048,[442]19.3911,[443]19.3767,[444]19.3912,[445]19.3675,[446]19.4116,[447]19.4188,[448]19.3985,[449]19.3860,[450]19.3886,[451]19.3790,[452]19.3722,[453]19.3568,[454]19.3377,[455]19.3447,[456]19.3393,[457]19.3515,[458]19.3742,[459]19.3782,[460]19.3807,[461]19.3757,[462]19.3779,[463]19.3990,[464]19.3949,[465]19.4006,[466]19.4030,[467]19.4160,[468]19.4378,[469]19.4489,[470]19.4572,[471]19.4407,[472]19.4554,[473]19.4220,[474]19.4184,[475]19.4241,[476]19.4447,[477]19.4295,[478]19.3980,[479]19.4053,[480]19.4229,[481]19.4582,[482]19.4259,[483]19.4454,[484]19.4614,[485]19.4610,[486]19.4502,[487]19.4565,[488]19.4228,[489]19.3933,[490]19.3874,[491]19.3664,[492]19.3626,[493]19.3224,[494]19.3075,[495]19.2745,[496]19.2741,[497]19.3011,[498]19.3091,[499]19.2892,[500]19.2839,[501]19.2737,[502]19.2747,[503]19.3060,[504]19.3227,[505]19.3154,[506]19.3102,[507]19.2900,[508]19.2937,[509]19.2709,[510]19.2803,[511]19.2955,[512]19.2934,[513]19.3070,[514]19.3064,[515]19.3402,[516]19.3367,[517]19.3388,[518]19.3458,[519]19.3390,[520]19.3272,[521]19.3239,[522]19.3029,[523]19.3039,[524]19.2962,[525]19.3047,[526]19.3097,[527]19.3007,[528]19.2946,[529]19.2929,[530]19.2798,[531]19.2782,[532]19.2696,[533]19.2746,[534]19.2687,[535]19.2773,[536]19.2720,[537]19.2918,[538]19.3052,[539]19.3070,[540]19.3395,[541]19.3558,[542]19.3272,[543]19.3324,[544]19.3623,[545]19.3540,[546]19.3428,[547]19.3274,[548]19.3129,[549]19.3155,[550]19.2785,[551]19.2479,[552]19.2174,[553]19.1324,[554]19.1177,[555]19.1371,[556]19.1376,[557]19.1264,[558]19.1109,[559]19.1146,[560]19.1260,[561]19.1275,[562]19.1576,[563]19.1751,[564]19.1632,[565]19.1784,[566]19.1826,[567]19.1587,[568]19.1537,[569]19.1325,[570]19.1420,[571]19.1419,[572]19.1713,[573]19.1807,[574]19.1870,[575]19.1876,[576]19.2115,[577]19.2074,[578]19.2239,[579]19.2461,[580]19.2950,[581]19.3001,[582]19.3249,[583]19.3020,[584]19.3072,
Final estimate: PPL = 19.3072 +/- 0.18513

llama_perf_context_print:        load time =    1702.68 ms
llama_perf_context_print: prompt eval time = 5103868.04 ms / 299008 tokens (   17.07 ms per token,    58.58 tokens per second)
llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_perf_context_print:       total time = 5156072.57 ms / 299009 tokens
llama_perf_context_print:    graphs reused =          0
状态: 完成
-------------------------------------------
执行时间: 2025-12-22 01:31:46
测试配置: [变体: 4] [模型: Qwen3-1.7B-Q4_0.gguf.aria2] [线程: 64] [第 1 次运行]
命令: ./llama-perplexity -m /root/wmw/unsloth_riscv/Qwen3-1.7B-GGUF/Qwen3-1.7B-Q4_0.gguf.aria2 -t 64 -f /root/wmw/hzeng_vw/riscv_kernels/e2e/rvv10/../wiki.test.raw
build: 0 (unknown) with riscv64-unknown-linux-gnu-gcc () 15.1.0 for riscv64-unknown-linux-gnu
gguf_init_from_file_impl: invalid magic characters: '????', expected 'GGUF'
llama_model_load: error loading model: llama_model_loader: failed to load model from /root/wmw/unsloth_riscv/Qwen3-1.7B-GGUF/Qwen3-1.7B-Q4_0.gguf.aria2
llama_model_load_from_file_impl: failed to load model
common_init_from_params: failed to load model '/root/wmw/unsloth_riscv/Qwen3-1.7B-GGUF/Qwen3-1.7B-Q4_0.gguf.aria2'
main: unable to load model
状态: 失败 (请查看日志详情)
-------------------------------------------
执行时间: 2025-12-22 01:31:46
测试配置: [变体: 4] [模型: Qwen3-1.7B-Q8_0.gguf] [线程: 64] [第 1 次运行]
命令: ./llama-perplexity -m /root/wmw/unsloth_riscv/Qwen3-1.7B-GGUF/Qwen3-1.7B-Q8_0.gguf -t 64 -f /root/wmw/hzeng_vw/riscv_kernels/e2e/rvv10/../wiki.test.raw
build: 0 (unknown) with riscv64-unknown-linux-gnu-gcc () 15.1.0 for riscv64-unknown-linux-gnu
llama_model_loader: loaded meta data with 32 key-value pairs and 310 tensors from /root/wmw/unsloth_riscv/Qwen3-1.7B-GGUF/Qwen3-1.7B-Q8_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3-1.7B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3-1.7B
llama_model_loader: - kv   4:                       general.quantized_by str              = Unsloth
llama_model_loader: - kv   5:                         general.size_label str              = 1.7B
llama_model_loader: - kv   6:                           general.repo_url str              = https://huggingface.co/unsloth
llama_model_loader: - kv   7:                          qwen3.block_count u32              = 28
llama_model_loader: - kv   8:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   9:                     qwen3.embedding_length u32              = 2048
llama_model_loader: - kv  10:                  qwen3.feed_forward_length u32              = 6144
llama_model_loader: - kv  11:                 qwen3.attention.head_count u32              = 16
llama_model_loader: - kv  12:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  13:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  14:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  15:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  16:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  22:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 151654
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 7
llama_model_loader: - kv  28:                      quantize.imatrix.file str              = Qwen3-1.7B-GGUF/imatrix_unsloth.dat
llama_model_loader: - kv  29:                   quantize.imatrix.dataset str              = unsloth_calibration_Qwen3-1.7B.txt
llama_model_loader: - kv  30:             quantize.imatrix.entries_count i32              = 196
llama_model_loader: - kv  31:              quantize.imatrix.chunks_count i32              = 685
llama_model_loader: - type  f32:  113 tensors
llama_model_loader: - type q8_0:  197 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q8_0
print_info: file size   = 1.70 GiB (8.50 BPW) 
load: printing all EOG tokens:
load:   - 151643 ('<|endoftext|>')
load:   - 151645 ('<|im_end|>')
load:   - 151662 ('<|fim_pad|>')
load:   - 151663 ('<|repo_name|>')
load:   - 151664 ('<|file_sep|>')
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 2048
print_info: n_layer          = 28
print_info: n_head           = 16
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 2
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 6144
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: model type       = 1.7B
print_info: model params     = 1.72 B
print_info: general.name     = Qwen3-1.7B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 11 ','
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151654 '<|vision_pad|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors:   CPU_Mapped model buffer size =  1743.77 MiB
...................................................................................
llama_context: constructing llama_context
llama_context: n_seq_max     = 4
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 512
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (512) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     2.32 MiB
llama_kv_cache_unified:        CPU KV buffer size =   224.00 MiB
llama_kv_cache_unified: size =  224.00 MiB (   512 cells,  28 layers,  4/4 seqs), K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:        CPU compute buffer size =   300.75 MiB
llama_context: graph nodes  = 1098
llama_context: graph splits = 1
common_init_from_params: added <|endoftext|> logit bias = -inf
common_init_from_params: added <|im_end|> logit bias = -inf
common_init_from_params: added <|fim_pad|> logit bias = -inf
common_init_from_params: added <|repo_name|> logit bias = -inf
common_init_from_params: added <|file_sep|> logit bias = -inf
common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)

system_info: n_threads = 64 (n_threads_batch = 64) / 64 | CPU : RISCV_V = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 3204 ms
perplexity: calculating perplexity over 584 chunks, n_ctx=512, batch_size=2048, n_seq=4
perplexity: 36.54 seconds per pass - ETA 1 hours 28.90 minutes
[1]11.4300,[2]15.1327,[3]14.8006,[4]15.4354,[5]15.4249,[6]15.5606,[7]15.9786,[8]16.6940,[9]17.8151,[10]18.3606,[11]18.4929,[12]18.5778,[13]19.2155,[14]18.8063,[15]18.7245,[16]19.1096,[17]18.0380,[18]18.2215,[19]18.0615,[20]18.0681,[21]17.7998,[22]17.8383,[23]17.1712,[24]16.2608,[25]15.8719,[26]15.4447,[27]14.9863,[28]14.8290,[29]14.8865,[30]14.8325,[31]14.6325,[32]14.6189,[33]14.3625,[34]14.6623,[35]14.7609,[36]14.9975,[37]15.2270,[38]15.3233,[39]15.2859,[40]15.3908,[41]15.3581,[42]15.2717,[43]15.4074,[44]15.4618,[45]15.4464,[46]15.3920,[47]15.6972,[48]15.8175,[49]15.7314,[50]15.8216,[51]15.8929,[52]15.9329,[53]16.0170,[54]16.0576,[55]16.0570,[56]16.1825,[57]16.0928,[58]16.1493,[59]16.2282,[60]16.3110,[61]16.3507,[62]16.4003,[63]16.4805,[64]16.6034,[65]16.7950,[66]16.9232,[67]17.0663,[68]16.9766,[69]16.9580,[70]16.9225,[71]16.9411,[72]17.0394,[73]17.0798,[74]17.0801,[75]16.9612,[76]16.9308,[77]16.9695,[78]16.9646,[79]16.8308,[80]16.8353,[81]16.7610,[82]16.8395,[83]16.8022,[84]16.7599,[85]16.8150,[86]16.9767,[87]17.0770,[88]17.0476,[89]17.0261,[90]16.9853,[91]17.0513,[92]17.0083,[93]17.1305,[94]17.1734,[95]17.1222,[96]17.1024,[97]17.0654,[98]17.0713,[99]17.0400,[100]17.1664,[101]17.2276,[102]17.1905,[103]17.2305,[104]17.1536,[105]17.1839,[106]17.1507,[107]17.1875,[108]17.2561,[109]17.3092,[110]17.3926,[111]17.5914,[112]17.6175,[113]17.4810,[114]17.5370,[115]17.5777,[116]17.5137,[117]17.4921,[118]17.4532,[119]17.3347,[120]17.3858,[121]17.3428,[122]17.3596,[123]17.3324,[124]17.2391,[125]17.2197,[126]17.2196,[127]17.1841,[128]17.1475,[129]17.1062,[130]17.0696,[131]16.9700,[132]16.8833,[133]16.8343,[134]16.8438,[135]16.8217,[136]16.7809,[137]16.7026,[138]16.6370,[139]16.6492,[140]16.6269,[141]16.6120,[142]16.6225,[143]16.6482,[144]16.7220,[145]16.6800,[146]16.5952,[147]16.5074,[148]16.4328,[149]16.4156,[150]16.3315,[151]16.3213,[152]16.3073,[153]16.2970,[154]16.2187,[155]16.2674,[156]16.2037,[157]16.1636,[158]16.1122,[159]16.0727,[160]16.0075,[161]15.9776,[162]15.9765,[163]15.9826,[164]15.9927,[165]15.9645,[166]15.9474,[167]15.9053,[168]15.9429,[169]15.9215,[170]15.9971,[171]16.0658,[172]16.1364,[173]16.2152,[174]16.2486,[175]16.3244,[176]16.3895,[177]16.4793,[178]16.5498,[179]16.6060,[180]16.6106,[181]16.6693,[182]16.7225,[183]16.7667,[184]16.8272,[185]16.8705,[186]16.8916,[187]16.9007,[188]16.9046,[189]16.9247,[190]16.9694,[191]16.9744,[192]17.0018,[193]17.0071,[194]17.0433,[195]17.0628,[196]17.0736,[197]17.0786,[198]17.0261,[199]17.0045,[200]16.9897,[201]17.0312,[202]17.0583,[203]17.0930,[204]17.1131,[205]17.1135,[206]17.1082,[207]17.1591,[208]17.1236,[209]17.1245,[210]17.1192,[211]17.1146,[212]17.1133,[213]17.1298,[214]17.0986,[215]17.0582,[216]17.0523,[217]17.0463,[218]17.0410,[219]16.9786,[220]16.9539,[221]16.9143,[222]16.8772,[223]16.8671,[224]16.8806,[225]16.8377,[226]16.8270,[227]16.8010,[228]16.7383,[229]16.6805,[230]16.6340,[231]16.5915,[232]16.5509,[233]16.5496,[234]16.5629,[235]16.5497,[236]16.5169,[237]16.4814,[238]16.4273,[239]16.3926,[240]16.3942,[241]16.3795,[242]16.3882,[243]16.4079,[244]16.4325,[245]16.4386,[246]16.4913,[247]16.4829,[248]16.4986,[249]16.5199,[250]16.5175,[251]16.5464,[252]16.5590,[253]16.6015,[254]16.6255,[255]16.6208,[256]16.6562,[257]16.6578,[258]16.6337,[259]16.5875,[260]16.5592,[261]16.5175,[262]16.5011,[263]16.5111,[264]16.5173,[265]16.5476,[266]16.5792,[267]16.5860,[268]16.5778,[269]16.5856,[270]16.6090,[271]16.5754,[272]16.5886,[273]16.5886,[274]16.5913,[275]16.5942,[276]16.5627,[277]16.5496,[278]16.5603,[279]16.5595,[280]16.5456,[281]16.5504,[282]16.5962,[283]16.5407,[284]16.4754,[285]16.4886,[286]16.4402,[287]16.3875,[288]16.3824,[289]16.3889,[290]16.4407,[291]16.4444,[292]16.4375,[293]16.4398,[294]16.4724,[295]16.5279,[296]16.5997,[297]16.6348,[298]16.6272,[299]16.5990,[300]16.5989,[301]16.5939,[302]16.5994,[303]16.5859,[304]16.6092,[305]16.5995,[306]16.5882,[307]16.6011,[308]16.5890,[309]16.5814,[310]16.6023,[311]16.6032,[312]16.5811,[313]16.5721,[314]16.5802,[315]16.5518,[316]16.5839,[317]16.6315,[318]16.6351,[319]16.6149,[320]16.6194,[321]16.5890,[322]16.6115,[323]16.6379,[324]16.6656,[325]16.6889,[326]16.7037,[327]16.6793,[328]16.6794,[329]16.6373,[330]16.6315,[331]16.6080,[332]16.6065,[333]16.6047,[334]16.5836,[335]16.5492,[336]16.5450,[337]16.5569,[338]16.5704,[339]16.5665,[340]16.5602,[341]16.5358,[342]16.5343,[343]16.5213,[344]16.5344,[345]16.5346,[346]16.5323,[347]16.5086,[348]16.5219,[349]16.5182,[350]16.5159,[351]16.5290,[352]16.5324,[353]16.5273,[354]16.4984,[355]16.5230,[356]16.5397,[357]16.5529,[358]16.5337,[359]16.5282,[360]16.5428,[361]16.5524,[362]16.5599,[363]16.5524,[364]16.6343,[365]16.6644,[366]16.7188,[367]16.7564,[368]16.8074,[369]16.8398,[370]16.8637,[371]16.8991,[372]16.9344,[373]16.9513,[374]16.9752,[375]17.0147,[376]17.0483,[377]17.0648,[378]17.0876,[379]17.1064,[380]17.1322,[381]17.1559,[382]17.1776,[383]17.1929,[384]17.2252,[385]17.2728,[386]17.3177,[387]17.3212,[388]17.3107,[389]17.3374,[390]17.3787,[391]17.4085,[392]17.3981,[393]17.3912,[394]17.3776,[395]17.3906,[396]17.4016,[397]17.4023,[398]17.4135,[399]17.4144,[400]17.4476,[401]17.4564,[402]17.4627,[403]17.4395,[404]17.4295,[405]17.4125,[406]17.4071,[407]17.4118,[408]17.4219,[409]17.4195,[410]17.4227,[411]17.4470,[412]17.4597,[413]17.4618,[414]17.4539,[415]17.4361,[416]17.4291,[417]17.4359,[418]17.4413,[419]17.4445,[420]17.4318,[421]17.4330,[422]17.4022,[423]17.4132,[424]17.4174,[425]17.4218,[426]17.4333,[427]17.4626,[428]17.4969,[429]17.5059,[430]17.4896,[431]17.4729,[432]17.4854,[433]17.4817,[434]17.4661,[435]17.4819,[436]17.4431,[437]17.4365,[438]17.4334,[439]17.4156,[440]17.4335,[441]17.4384,[442]17.4229,[443]17.4119,[444]17.4210,[445]17.3940,[446]17.4185,[447]17.4182,[448]17.3999,[449]17.3877,[450]17.3896,[451]17.3793,[452]17.3691,[453]17.3525,[454]17.3363,[455]17.3419,[456]17.3330,[457]17.3448,[458]17.3688,[459]17.3702,[460]17.3741,[461]17.3733,[462]17.3776,[463]17.3954,[464]17.3958,[465]17.4003,[466]17.4033,[467]17.4159,[468]17.4325,[469]17.4421,[470]17.4494,[471]17.4336,[472]17.4493,[473]17.4215,[474]17.4194,[475]17.4262,[476]17.4432,[477]17.4304,[478]17.4053,[479]17.4121,[480]17.4306,[481]17.4518,[482]17.4221,[483]17.4403,[484]17.4571,[485]17.4594,[486]17.4521,[487]17.4593,[488]17.4307,[489]17.4083,[490]17.4045,[491]17.3860,[492]17.3784,[493]17.3465,[494]17.3329,[495]17.3060,[496]17.3039,[497]17.3243,[498]17.3324,[499]17.3143,[500]17.3113,[501]17.3028,[502]17.3071,[503]17.3352,[504]17.3513,[505]17.3459,[506]17.3445,[507]17.3262,[508]17.3298,[509]17.3084,[510]17.3146,[511]17.3268,[512]17.3239,[513]17.3348,[514]17.3334,[515]17.3409,[516]17.3387,[517]17.3391,[518]17.3370,[519]17.3303,[520]17.3208,[521]17.3201,[522]17.3031,[523]17.3051,[524]17.2994,[525]17.3064,[526]17.3118,[527]17.3064,[528]17.3039,[529]17.2998,[530]17.2858,[531]17.2835,[532]17.2742,[533]17.2788,[534]17.2741,[535]17.2810,[536]17.2785,[537]17.2970,[538]17.3099,[539]17.3124,[540]17.3423,[541]17.3475,[542]17.3225,[543]17.3269,[544]17.3486,[545]17.3419,[546]17.3333,[547]17.3168,[548]17.3007,[549]17.3039,[550]17.2722,[551]17.2462,[552]17.2206,[553]17.1469,[554]17.1364,[555]17.1506,[556]17.1511,[557]17.1414,[558]17.1276,[559]17.1311,[560]17.1434,[561]17.1462,[562]17.1723,[563]17.1876,[564]17.1784,[565]17.1931,[566]17.1960,[567]17.1775,[568]17.1731,[569]17.1561,[570]17.1588,[571]17.1604,[572]17.1819,[573]17.1912,[574]17.1946,[575]17.1957,[576]17.2172,[577]17.2160,[578]17.2305,[579]17.2504,[580]17.2778,[581]17.2831,[582]17.3061,[583]17.2859,[584]17.2915,
Final estimate: PPL = 17.2915 +/- 0.16206

llama_perf_context_print:        load time =    2004.49 ms
llama_perf_context_print: prompt eval time = 5168208.06 ms / 299008 tokens (   17.28 ms per token,    57.86 tokens per second)
llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_perf_context_print:       total time = 5228924.80 ms / 299009 tokens
llama_perf_context_print:    graphs reused =          0
状态: 完成
===========================================
所有测试已结束。
