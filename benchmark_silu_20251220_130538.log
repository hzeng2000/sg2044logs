开始测试...
日志文件: /root/wmw/hzeng_vw/riscv_kernels/e2e/rvv10/out/phase_silu/benchmark_silu_20251220_130538.log
===========================================
正在处理变体目录: base
-------------------------------------------
执行时间: 2025-12-20 13:05:38
测试配置: [变体: base] [模型: Qwen3-0.6B-Q4_0.gguf] [线程: 64] [第 1 次运行]
命令: ./llama-perplexity -m /root/wmw/unsloth_riscv/Qwen3-0.6B-GGUF/Qwen3-0.6B-Q4_0.gguf -t 64 -f /root/wmw/hzeng_vw/riscv_kernels/e2e/rvv10/../wiki.test.raw
build: 0 (unknown) with riscv64-unknown-linux-gnu-gcc () 15.1.0 for riscv64-unknown-linux-gnu
llama_model_loader: loaded meta data with 32 key-value pairs and 310 tensors from /root/wmw/unsloth_riscv/Qwen3-0.6B-GGUF/Qwen3-0.6B-Q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3-0.6B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3-0.6B
llama_model_loader: - kv   4:                       general.quantized_by str              = Unsloth
llama_model_loader: - kv   5:                         general.size_label str              = 0.6B
llama_model_loader: - kv   6:                           general.repo_url str              = https://huggingface.co/unsloth
llama_model_loader: - kv   7:                          qwen3.block_count u32              = 28
llama_model_loader: - kv   8:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   9:                     qwen3.embedding_length u32              = 1024
llama_model_loader: - kv  10:                  qwen3.feed_forward_length u32              = 3072
llama_model_loader: - kv  11:                 qwen3.attention.head_count u32              = 16
llama_model_loader: - kv  12:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  13:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  14:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  15:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  16:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  22:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 151654
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 2
llama_model_loader: - kv  28:                      quantize.imatrix.file str              = Qwen3-0.6B-GGUF/imatrix_unsloth.dat
llama_model_loader: - kv  29:                   quantize.imatrix.dataset str              = unsloth_calibration_Qwen3-0.6B.txt
llama_model_loader: - kv  30:             quantize.imatrix.entries_count u32              = 196
llama_model_loader: - kv  31:              quantize.imatrix.chunks_count u32              = 688
llama_model_loader: - type  f32:  113 tensors
llama_model_loader: - type q4_0:  193 tensors
llama_model_loader: - type q4_1:    3 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 358.78 MiB (5.05 BPW) 
load: printing all EOG tokens:
load:   - 151643 ('<|endoftext|>')
load:   - 151645 ('<|im_end|>')
load:   - 151662 ('<|fim_pad|>')
load:   - 151663 ('<|repo_name|>')
load:   - 151664 ('<|file_sep|>')
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 1024
print_info: n_layer          = 28
print_info: n_head           = 16
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 2
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 3072
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: model type       = 0.6B
print_info: model params     = 596.05 M
print_info: general.name     = Qwen3-0.6B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 11 ','
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151654 '<|vision_pad|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors:   CPU_Mapped model buffer size =   358.78 MiB
....................................................................
llama_context: constructing llama_context
llama_context: n_seq_max     = 4
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 512
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (512) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     2.32 MiB
llama_kv_cache_unified:        CPU KV buffer size =   224.00 MiB
llama_kv_cache_unified: size =  224.00 MiB (   512 cells,  28 layers,  4/4 seqs), K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:        CPU compute buffer size =   298.75 MiB
llama_context: graph nodes  = 1098
llama_context: graph splits = 1
common_init_from_params: added <|endoftext|> logit bias = -inf
common_init_from_params: added <|im_end|> logit bias = -inf
common_init_from_params: added <|fim_pad|> logit bias = -inf
common_init_from_params: added <|repo_name|> logit bias = -inf
common_init_from_params: added <|file_sep|> logit bias = -inf
common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)

system_info: n_threads = 64 (n_threads_batch = 64) / 64 | CPU : LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 2831.76 ms
perplexity: calculating perplexity over 584 chunks, n_ctx=512, batch_size=2048, n_seq=4
perplexity: 30.91 seconds per pass - ETA 1 hours 15.20 minutes
[1]14.0791,[2]19.2608,[3]19.6351,[4]20.3777,[5]20.5197,[6]21.0242,[7]21.2841,[8]23.0447,[9]24.5809,[10]25.5438,[11]26.0423,[12]26.3236,[13]27.1228,[14]26.7507,[15]26.4454,[16]27.2578,[17]25.1982,[18]25.6607,[19]25.2555,[20]25.5063,[21]24.8639,[22]24.9051,[23]23.7934,[24]22.5578,[25]22.0407,[26]21.5551,[27]20.8476,[28]20.4232,[29]20.6270,[30]20.6640,[31]20.3872,[32]20.4859,[33]20.0951,[34]20.3197,[35]20.5245,[36]20.8829,[37]21.3396,[38]21.4920,[39]21.2851,[40]21.3797,[41]21.3139,[42]21.1910,[43]21.4225,[44]21.4958,[45]21.5972,[46]21.5534,[47]22.0340,[48]22.2870,[49]22.1510,[50]22.3654,[51]22.4106,[52]22.4300,[53]22.4877,[54]22.6652,[55]22.6513,[56]22.7807,[57]22.7003,[58]22.7819,[59]22.9330,[60]23.0611,[61]23.0478,[62]23.1409,[63]23.3266,[64]23.5113,[65]23.7862,[66]24.0034,[67]24.2344,[68]24.1324,[69]24.1404,[70]24.1713,[71]24.1919,[72]24.3575,[73]24.4174,[74]24.4221,[75]24.2770,[76]24.1625,[77]24.1506,[78]24.1013,[79]23.9118,[80]23.8686,[81]23.7271,[82]23.8268,[83]23.7844,[84]23.7111,[85]23.7385,[86]23.9506,[87]24.1354,[88]24.0655,[89]24.0056,[90]23.9463,[91]24.0409,[92]23.9601,[93]24.0966,[94]24.2119,[95]24.1409,[96]24.2023,[97]24.1642,[98]24.1751,[99]24.1035,[100]24.2533,[101]24.3399,[102]24.2921,[103]24.3586,[104]24.2486,[105]24.2866,[106]24.1631,[107]24.1897,[108]24.3205,[109]24.4024,[110]24.5182,[111]24.8033,[112]24.8715,[113]24.7232,[114]24.8331,[115]24.8722,[116]24.8027,[117]24.7968,[118]24.7539,[119]24.6315,[120]24.6834,[121]24.6393,[122]24.6809,[123]24.6279,[124]24.4992,[125]24.4404,[126]24.4197,[127]24.3708,[128]24.3043,[129]24.2849,[130]24.1978,[131]24.0498,[132]23.9661,[133]23.8976,[134]23.9092,[135]23.8962,[136]23.8616,[137]23.7375,[138]23.6139,[139]23.6425,[140]23.5705,[141]23.5534,[142]23.5416,[143]23.5941,[144]23.6491,[145]23.5949,[146]23.4375,[147]23.3073,[148]23.2036,[149]23.1374,[150]22.9971,[151]22.9578,[152]22.9475,[153]22.9100,[154]22.7842,[155]22.8186,[156]22.7541,[157]22.7010,[158]22.6011,[159]22.5554,[160]22.5077,[161]22.4902,[162]22.4930,[163]22.4913,[164]22.4910,[165]22.4613,[166]22.4839,[167]22.3951,[168]22.4435,[169]22.4097,[170]22.5523,[171]22.6369,[172]22.7496,[173]22.8777,[174]22.9185,[175]23.0161,[176]23.1207,[177]23.2276,[178]23.3162,[179]23.3745,[180]23.4064,[181]23.5003,[182]23.5878,[183]23.6689,[184]23.7688,[185]23.8332,[186]23.8454,[187]23.8422,[188]23.8543,[189]23.8760,[190]23.9423,[191]23.9546,[192]23.9783,[193]23.9732,[194]24.0365,[195]24.0638,[196]24.0956,[197]24.0988,[198]24.0178,[199]23.9692,[200]23.9571,[201]23.9989,[202]24.0341,[203]24.0758,[204]24.0900,[205]24.0837,[206]24.0465,[207]24.0983,[208]24.0130,[209]24.0025,[210]24.0030,[211]23.9948,[212]23.9933,[213]24.0021,[214]23.9452,[215]23.8752,[216]23.8843,[217]23.8882,[218]23.8482,[219]23.7671,[220]23.7108,[221]23.6592,[222]23.6167,[223]23.6138,[224]23.6237,[225]23.5619,[226]23.5695,[227]23.5392,[228]23.4666,[229]23.3753,[230]23.3121,[231]23.2596,[232]23.2175,[233]23.2287,[234]23.2430,[235]23.2191,[236]23.1587,[237]23.1203,[238]23.0360,[239]23.0001,[240]23.0220,[241]22.9967,[242]23.0212,[243]23.0378,[244]23.0766,[245]23.0824,[246]23.1444,[247]23.1420,[248]23.1675,[249]23.1913,[250]23.1925,[251]23.2632,[252]23.2666,[253]23.3251,[254]23.3630,[255]23.3730,[256]23.4351,[257]23.4435,[258]23.3881,[259]23.3460,[260]23.3167,[261]23.2765,[262]23.2577,[263]23.2427,[264]23.2445,[265]23.2972,[266]23.3243,[267]23.3297,[268]23.2995,[269]23.2950,[270]23.3078,[271]23.2668,[272]23.2755,[273]23.2748,[274]23.2641,[275]23.2618,[276]23.2103,[277]23.1798,[278]23.2094,[279]23.2053,[280]23.1915,[281]23.1821,[282]23.2569,[283]23.1636,[284]23.0587,[285]23.0695,[286]22.9987,[287]22.9187,[288]22.9331,[289]22.9332,[290]23.0108,[291]23.0095,[292]23.0024,[293]23.0041,[294]23.0547,[295]23.1135,[296]23.1682,[297]23.2189,[298]23.2176,[299]23.1743,[300]23.1782,[301]23.1555,[302]23.1597,[303]23.1447,[304]23.1702,[305]23.1503,[306]23.1326,[307]23.1423,[308]23.1217,[309]23.1128,[310]23.1508,[311]23.1589,[312]23.1143,[313]23.1079,[314]23.1388,[315]23.0984,[316]23.1527,[317]23.2123,[318]23.2053,[319]23.1790,[320]23.1942,[321]23.1381,[322]23.1556,[323]23.1950,[324]23.2243,[325]23.2651,[326]23.2733,[327]23.2313,[328]23.2271,[329]23.1609,[330]23.1343,[331]23.0913,[332]23.0788,[333]23.0749,[334]23.0425,[335]22.9919,[336]22.9698,[337]22.9925,[338]23.0170,[339]22.9947,[340]22.9560,[341]22.9248,[342]22.9212,[343]22.9145,[344]22.9375,[345]22.9555,[346]22.9564,[347]22.9398,[348]22.9496,[349]22.9388,[350]22.9422,[351]22.9653,[352]22.9720,[353]22.9717,[354]22.9160,[355]22.9594,[356]22.9742,[357]22.9822,[358]22.9521,[359]22.9488,[360]22.9439,[361]22.9501,[362]22.9767,[363]22.9695,[364]23.0124,[365]23.0395,[366]23.1123,[367]23.1670,[368]23.2398,[369]23.2990,[370]23.3362,[371]23.3899,[372]23.4451,[373]23.4666,[374]23.4947,[375]23.5546,[376]23.5958,[377]23.6231,[378]23.6757,[379]23.7178,[380]23.7659,[381]23.7905,[382]23.8252,[383]23.8550,[384]23.8982,[385]23.9641,[386]24.0048,[387]23.9980,[388]23.9757,[389]24.0165,[390]24.0714,[391]24.1164,[392]24.1131,[393]24.1082,[394]24.0869,[395]24.0932,[396]24.1138,[397]24.1220,[398]24.1285,[399]24.1325,[400]24.1714,[401]24.1673,[402]24.1711,[403]24.1472,[404]24.1480,[405]24.1257,[406]24.1186,[407]24.1258,[408]24.1396,[409]24.1412,[410]24.1401,[411]24.1807,[412]24.1929,[413]24.2067,[414]24.2052,[415]24.2017,[416]24.1968,[417]24.2071,[418]24.2194,[419]24.2109,[420]24.2065,[421]24.2147,[422]24.1766,[423]24.1893,[424]24.1995,[425]24.2095,[426]24.2333,[427]24.2676,[428]24.3143,[429]24.3210,[430]24.2987,[431]24.2641,[432]24.2639,[433]24.2576,[434]24.2390,[435]24.2546,[436]24.1858,[437]24.1799,[438]24.1835,[439]24.1550,[440]24.1884,[441]24.2000,[442]24.1865,[443]24.1777,[444]24.1924,[445]24.1538,[446]24.1634,[447]24.1594,[448]24.1505,[449]24.1454,[450]24.1663,[451]24.1591,[452]24.1552,[453]24.1349,[454]24.1144,[455]24.1239,[456]24.1215,[457]24.1316,[458]24.1728,[459]24.1797,[460]24.1765,[461]24.1798,[462]24.1898,[463]24.2170,[464]24.2213,[465]24.2313,[466]24.2317,[467]24.2463,[468]24.2708,[469]24.2913,[470]24.3174,[471]24.3049,[472]24.3270,[473]24.2835,[474]24.2775,[475]24.2890,[476]24.3039,[477]24.2895,[478]24.2475,[479]24.2409,[480]24.2605,[481]24.2764,[482]24.2387,[483]24.2493,[484]24.2652,[485]24.2675,[486]24.2535,[487]24.2712,[488]24.2325,[489]24.2039,[490]24.1988,[491]24.1678,[492]24.1583,[493]24.1149,[494]24.1034,[495]24.0649,[496]24.0620,[497]24.0889,[498]24.1082,[499]24.0887,[500]24.0985,[501]24.0895,[502]24.0983,[503]24.1430,[504]24.1634,[505]24.1643,[506]24.1448,[507]24.1192,[508]24.1284,[509]24.1032,[510]24.1060,[511]24.1168,[512]24.1054,[513]24.1184,[514]24.1107,[515]24.1057,[516]24.1071,[517]24.1026,[518]24.0906,[519]24.0777,[520]24.0594,[521]24.0581,[522]24.0395,[523]24.0435,[524]24.0334,[525]24.0404,[526]24.0484,[527]24.0348,[528]24.0250,[529]24.0077,[530]23.9949,[531]23.9974,[532]23.9906,[533]24.0042,[534]23.9986,[535]23.9972,[536]23.9824,[537]24.0039,[538]24.0218,[539]24.0205,[540]24.0587,[541]24.0629,[542]24.0313,[543]24.0304,[544]24.0470,[545]24.0372,[546]24.0248,[547]24.0092,[548]23.9797,[549]23.9911,[550]23.9386,[551]23.8909,[552]23.8557,[553]23.7422,[554]23.7278,[555]23.7363,[556]23.7329,[557]23.7203,[558]23.7086,[559]23.7132,[560]23.7277,[561]23.7331,[562]23.7672,[563]23.7925,[564]23.7858,[565]23.8041,[566]23.8140,[567]23.7926,[568]23.7951,[569]23.7777,[570]23.7963,[571]23.7971,[572]23.8248,[573]23.8334,[574]23.8402,[575]23.8380,[576]23.8626,[577]23.8611,[578]23.8720,[579]23.9069,[580]23.9425,[581]23.9518,[582]23.9883,[583]23.9681,[584]23.9797,
Final estimate: PPL = 23.9797 +/- 0.20740

llama_perf_context_print:        load time =    1557.22 ms
llama_perf_context_print: prompt eval time = 4369116.97 ms / 299008 tokens (   14.61 ms per token,    68.44 tokens per second)
llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_perf_context_print:       total time = 4429080.25 ms / 299009 tokens
llama_perf_context_print:    graphs reused =          0
状态: 完成
-------------------------------------------
执行时间: 2025-12-20 14:19:29
测试配置: [变体: base] [模型: Qwen3-0.6B-Q8_0.gguf] [线程: 64] [第 1 次运行]
命令: ./llama-perplexity -m /root/wmw/unsloth_riscv/Qwen3-0.6B-GGUF/Qwen3-0.6B-Q8_0.gguf -t 64 -f /root/wmw/hzeng_vw/riscv_kernels/e2e/rvv10/../wiki.test.raw
build: 0 (unknown) with riscv64-unknown-linux-gnu-gcc () 15.1.0 for riscv64-unknown-linux-gnu
llama_model_loader: loaded meta data with 32 key-value pairs and 310 tensors from /root/wmw/unsloth_riscv/Qwen3-0.6B-GGUF/Qwen3-0.6B-Q8_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3-0.6B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3-0.6B
llama_model_loader: - kv   4:                       general.quantized_by str              = Unsloth
llama_model_loader: - kv   5:                         general.size_label str              = 0.6B
llama_model_loader: - kv   6:                           general.repo_url str              = https://huggingface.co/unsloth
llama_model_loader: - kv   7:                          qwen3.block_count u32              = 28
llama_model_loader: - kv   8:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   9:                     qwen3.embedding_length u32              = 1024
llama_model_loader: - kv  10:                  qwen3.feed_forward_length u32              = 3072
llama_model_loader: - kv  11:                 qwen3.attention.head_count u32              = 16
llama_model_loader: - kv  12:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  13:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  14:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  15:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  16:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  22:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 151654
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 7
llama_model_loader: - kv  28:                      quantize.imatrix.file str              = Qwen3-0.6B-GGUF/imatrix_unsloth.dat
llama_model_loader: - kv  29:                   quantize.imatrix.dataset str              = unsloth_calibration_Qwen3-0.6B.txt
llama_model_loader: - kv  30:             quantize.imatrix.entries_count u32              = 196
llama_model_loader: - kv  31:              quantize.imatrix.chunks_count u32              = 688
llama_model_loader: - type  f32:  113 tensors
llama_model_loader: - type q8_0:  197 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q8_0
print_info: file size   = 604.15 MiB (8.50 BPW) 
load: printing all EOG tokens:
load:   - 151643 ('<|endoftext|>')
load:   - 151645 ('<|im_end|>')
load:   - 151662 ('<|fim_pad|>')
load:   - 151663 ('<|repo_name|>')
load:   - 151664 ('<|file_sep|>')
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 1024
print_info: n_layer          = 28
print_info: n_head           = 16
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 2
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 3072
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: model type       = 0.6B
print_info: model params     = 596.05 M
print_info: general.name     = Qwen3-0.6B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 11 ','
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151654 '<|vision_pad|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors:   CPU_Mapped model buffer size =   604.15 MiB
...........................................................................
llama_context: constructing llama_context
llama_context: n_seq_max     = 4
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 512
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (512) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     2.32 MiB
llama_kv_cache_unified:        CPU KV buffer size =   224.00 MiB
llama_kv_cache_unified: size =  224.00 MiB (   512 cells,  28 layers,  4/4 seqs), K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:        CPU compute buffer size =   298.75 MiB
llama_context: graph nodes  = 1098
llama_context: graph splits = 1
common_init_from_params: added <|endoftext|> logit bias = -inf
common_init_from_params: added <|im_end|> logit bias = -inf
common_init_from_params: added <|fim_pad|> logit bias = -inf
common_init_from_params: added <|repo_name|> logit bias = -inf
common_init_from_params: added <|file_sep|> logit bias = -inf
common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)

system_info: n_threads = 64 (n_threads_batch = 64) / 64 | CPU : LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 2889.72 ms
perplexity: calculating perplexity over 584 chunks, n_ctx=512, batch_size=2048, n_seq=4
perplexity: 34.20 seconds per pass - ETA 1 hours 23.22 minutes
[1]12.9049,[2]18.2255,[3]18.3677,[4]19.3176,[5]19.5931,[6]20.0124,[7]20.1813,[8]21.7131,[9]22.9996,[10]23.7721,[11]24.1951,[12]24.4437,[13]25.2557,[14]24.8450,[15]24.5444,[16]25.1469,[17]23.3096,[18]23.7820,[19]23.4676,[20]23.6525,[21]23.0978,[22]23.0624,[23]22.0541,[24]20.8408,[25]20.3838,[26]19.8970,[27]19.2036,[28]18.7937,[29]18.9730,[30]18.9905,[31]18.7540,[32]18.8258,[33]18.4690,[34]18.6471,[35]18.7935,[36]19.1124,[37]19.5056,[38]19.6484,[39]19.4641,[40]19.5772,[41]19.5521,[42]19.4127,[43]19.6091,[44]19.6627,[45]19.7421,[46]19.6900,[47]20.1092,[48]20.3356,[49]20.1969,[50]20.3962,[51]20.4521,[52]20.4848,[53]20.5602,[54]20.7116,[55]20.6766,[56]20.8052,[57]20.7310,[58]20.8003,[59]20.9383,[60]21.0561,[61]21.0399,[62]21.1036,[63]21.2643,[64]21.4259,[65]21.6696,[66]21.8553,[67]22.0673,[68]21.9821,[69]21.9916,[70]22.0176,[71]22.0401,[72]22.1909,[73]22.2385,[74]22.2467,[75]22.1151,[76]22.0184,[77]22.0359,[78]21.9851,[79]21.8178,[80]21.7814,[81]21.6514,[82]21.7487,[83]21.7022,[84]21.6169,[85]21.6221,[86]21.8206,[87]21.9839,[88]21.9289,[89]21.8584,[90]21.7919,[91]21.8871,[92]21.8167,[93]21.9514,[94]22.0621,[95]21.9942,[96]22.0289,[97]21.9838,[98]22.0009,[99]21.9558,[100]22.1099,[101]22.1991,[102]22.1676,[103]22.2350,[104]22.1417,[105]22.1772,[106]22.0786,[107]22.1063,[108]22.2289,[109]22.3068,[110]22.4265,[111]22.6983,[112]22.7445,[113]22.6043,[114]22.6991,[115]22.7464,[116]22.6789,[117]22.6637,[118]22.6117,[119]22.4801,[120]22.5271,[121]22.4859,[122]22.5401,[123]22.4884,[124]22.3738,[125]22.3296,[126]22.3120,[127]22.2808,[128]22.2113,[129]22.1820,[130]22.0967,[131]21.9649,[132]21.8758,[133]21.8106,[134]21.8190,[135]21.8055,[136]21.7746,[137]21.6757,[138]21.5688,[139]21.5918,[140]21.5352,[141]21.5365,[142]21.5313,[143]21.5830,[144]21.6334,[145]21.5820,[146]21.4404,[147]21.3221,[148]21.2254,[149]21.1610,[150]21.0275,[151]20.9862,[152]20.9807,[153]20.9462,[154]20.8430,[155]20.8845,[156]20.8231,[157]20.7761,[158]20.7030,[159]20.6539,[160]20.6175,[161]20.5961,[162]20.5945,[163]20.5933,[164]20.5829,[165]20.5549,[166]20.5706,[167]20.4945,[168]20.5321,[169]20.4991,[170]20.6300,[171]20.7073,[172]20.8090,[173]20.9257,[174]20.9546,[175]21.0459,[176]21.1389,[177]21.2396,[178]21.3148,[179]21.3779,[180]21.3999,[181]21.4899,[182]21.5729,[183]21.6508,[184]21.7454,[185]21.8018,[186]21.8150,[187]21.8148,[188]21.8192,[189]21.8321,[190]21.8946,[191]21.8988,[192]21.9099,[193]21.8954,[194]21.9487,[195]21.9786,[196]22.0066,[197]22.0122,[198]21.9362,[199]21.8940,[200]21.8842,[201]21.9256,[202]21.9574,[203]21.9939,[204]22.0111,[205]22.0078,[206]21.9754,[207]22.0273,[208]21.9489,[209]21.9375,[210]21.9351,[211]21.9303,[212]21.9306,[213]21.9356,[214]21.8914,[215]21.8245,[216]21.8248,[217]21.8215,[218]21.7893,[219]21.7209,[220]21.6726,[221]21.6308,[222]21.5844,[223]21.5850,[224]21.5932,[225]21.5312,[226]21.5377,[227]21.5125,[228]21.4446,[229]21.3643,[230]21.2985,[231]21.2502,[232]21.2138,[233]21.2159,[234]21.2235,[235]21.2078,[236]21.1555,[237]21.1226,[238]21.0472,[239]21.0126,[240]21.0364,[241]21.0142,[242]21.0357,[243]21.0522,[244]21.0883,[245]21.0862,[246]21.1484,[247]21.1422,[248]21.1551,[249]21.1873,[250]21.1885,[251]21.2482,[252]21.2487,[253]21.3056,[254]21.3377,[255]21.3458,[256]21.3951,[257]21.4012,[258]21.3516,[259]21.3046,[260]21.2711,[261]21.2378,[262]21.2172,[263]21.2020,[264]21.2051,[265]21.2514,[266]21.2689,[267]21.2769,[268]21.2512,[269]21.2495,[270]21.2604,[271]21.2247,[272]21.2362,[273]21.2306,[274]21.2257,[275]21.2204,[276]21.1653,[277]21.1383,[278]21.1568,[279]21.1522,[280]21.1416,[281]21.1318,[282]21.1935,[283]21.0992,[284]21.0066,[285]21.0192,[286]20.9604,[287]20.8883,[288]20.8982,[289]20.9033,[290]20.9771,[291]20.9773,[292]20.9730,[293]20.9790,[294]21.0244,[295]21.0750,[296]21.1267,[297]21.1729,[298]21.1655,[299]21.1226,[300]21.1190,[301]21.0966,[302]21.1031,[303]21.0925,[304]21.1167,[305]21.0969,[306]21.0828,[307]21.0929,[308]21.0756,[309]21.0630,[310]21.0965,[311]21.0983,[312]21.0566,[313]21.0474,[314]21.0640,[315]21.0245,[316]21.0772,[317]21.1347,[318]21.1309,[319]21.1085,[320]21.1258,[321]21.0809,[322]21.1015,[323]21.1350,[324]21.1671,[325]21.2013,[326]21.2158,[327]21.1780,[328]21.1717,[329]21.1124,[330]21.0911,[331]21.0487,[332]21.0369,[333]21.0330,[334]21.0004,[335]20.9572,[336]20.9416,[337]20.9568,[338]20.9846,[339]20.9619,[340]20.9265,[341]20.8980,[342]20.8948,[343]20.8895,[344]20.9131,[345]20.9252,[346]20.9290,[347]20.9197,[348]20.9258,[349]20.9131,[350]20.9128,[351]20.9369,[352]20.9426,[353]20.9421,[354]20.8916,[355]20.9323,[356]20.9483,[357]20.9563,[358]20.9323,[359]20.9294,[360]20.9247,[361]20.9310,[362]20.9537,[363]20.9492,[364]20.9858,[365]21.0167,[366]21.0832,[367]21.1329,[368]21.1995,[369]21.2535,[370]21.2927,[371]21.3428,[372]21.3929,[373]21.4149,[374]21.4419,[375]21.4981,[376]21.5366,[377]21.5578,[378]21.5972,[379]21.6336,[380]21.6756,[381]21.7027,[382]21.7344,[383]21.7624,[384]21.8092,[385]21.8688,[386]21.9063,[387]21.9056,[388]21.8845,[389]21.9224,[390]21.9712,[391]22.0114,[392]22.0110,[393]22.0082,[394]21.9857,[395]21.9901,[396]22.0089,[397]22.0145,[398]22.0209,[399]22.0226,[400]22.0624,[401]22.0581,[402]22.0635,[403]22.0402,[404]22.0360,[405]22.0157,[406]22.0130,[407]22.0166,[408]22.0269,[409]22.0285,[410]22.0292,[411]22.0655,[412]22.0758,[413]22.0852,[414]22.0784,[415]22.0755,[416]22.0653,[417]22.0752,[418]22.0854,[419]22.0784,[420]22.0697,[421]22.0786,[422]22.0485,[423]22.0611,[424]22.0714,[425]22.0802,[426]22.0978,[427]22.1304,[428]22.1741,[429]22.1771,[430]22.1545,[431]22.1257,[432]22.1264,[433]22.1198,[434]22.1060,[435]22.1215,[436]22.0598,[437]22.0524,[438]22.0553,[439]22.0294,[440]22.0559,[441]22.0642,[442]22.0520,[443]22.0423,[444]22.0538,[445]22.0201,[446]22.0275,[447]22.0219,[448]22.0103,[449]21.9999,[450]22.0159,[451]22.0078,[452]22.0011,[453]21.9813,[454]21.9633,[455]21.9674,[456]21.9618,[457]21.9761,[458]22.0112,[459]22.0188,[460]22.0148,[461]22.0200,[462]22.0289,[463]22.0540,[464]22.0557,[465]22.0626,[466]22.0644,[467]22.0768,[468]22.0997,[469]22.1154,[470]22.1372,[471]22.1276,[472]22.1486,[473]22.1078,[474]22.1035,[475]22.1146,[476]22.1303,[477]22.1183,[478]22.0800,[479]22.0767,[480]22.0991,[481]22.1168,[482]22.0800,[483]22.0907,[484]22.1102,[485]22.1115,[486]22.0988,[487]22.1144,[488]22.0801,[489]22.0539,[490]22.0500,[491]22.0238,[492]22.0158,[493]21.9776,[494]21.9670,[495]21.9318,[496]21.9298,[497]21.9507,[498]21.9690,[499]21.9480,[500]21.9542,[501]21.9444,[502]21.9511,[503]21.9913,[504]22.0109,[505]22.0135,[506]21.9989,[507]21.9777,[508]21.9843,[509]21.9597,[510]21.9612,[511]21.9710,[512]21.9596,[513]21.9746,[514]21.9712,[515]21.9681,[516]21.9676,[517]21.9635,[518]21.9487,[519]21.9364,[520]21.9210,[521]21.9166,[522]21.8963,[523]21.8973,[524]21.8896,[525]21.8972,[526]21.9068,[527]21.8947,[528]21.8852,[529]21.8729,[530]21.8620,[531]21.8654,[532]21.8605,[533]21.8717,[534]21.8707,[535]21.8708,[536]21.8580,[537]21.8774,[538]21.8935,[539]21.8927,[540]21.9267,[541]21.9310,[542]21.9031,[543]21.9037,[544]21.9193,[545]21.9088,[546]21.8992,[547]21.8840,[548]21.8580,[549]21.8684,[550]21.8225,[551]21.7793,[552]21.7472,[553]21.6456,[554]21.6317,[555]21.6413,[556]21.6386,[557]21.6279,[558]21.6138,[559]21.6158,[560]21.6295,[561]21.6331,[562]21.6619,[563]21.6847,[564]21.6771,[565]21.6962,[566]21.7037,[567]21.6830,[568]21.6831,[569]21.6660,[570]21.6846,[571]21.6875,[572]21.7139,[573]21.7223,[574]21.7289,[575]21.7254,[576]21.7478,[577]21.7466,[578]21.7535,[579]21.7866,[580]21.8215,[581]21.8317,[582]21.8666,[583]21.8466,[584]21.8581,
Final estimate: PPL = 21.8581 +/- 0.19091

llama_perf_context_print:        load time =    1669.20 ms
llama_perf_context_print: prompt eval time = 4846753.68 ms / 299008 tokens (   16.21 ms per token,    61.69 tokens per second)
llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_perf_context_print:       total time = 4898712.73 ms / 299009 tokens
llama_perf_context_print:    graphs reused =          0
状态: 完成
-------------------------------------------
执行时间: 2025-12-20 15:41:10
测试配置: [变体: base] [模型: Qwen3-1.7B-Q4_0.gguf] [线程: 64] [第 1 次运行]
命令: ./llama-perplexity -m /root/wmw/unsloth_riscv/Qwen3-1.7B-GGUF/Qwen3-1.7B-Q4_0.gguf -t 64 -f /root/wmw/hzeng_vw/riscv_kernels/e2e/rvv10/../wiki.test.raw
build: 0 (unknown) with riscv64-unknown-linux-gnu-gcc () 15.1.0 for riscv64-unknown-linux-gnu
llama_model_loader: loaded meta data with 32 key-value pairs and 310 tensors from /root/wmw/unsloth_riscv/Qwen3-1.7B-GGUF/Qwen3-1.7B-Q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3-1.7B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3-1.7B
llama_model_loader: - kv   4:                       general.quantized_by str              = Unsloth
llama_model_loader: - kv   5:                         general.size_label str              = 1.7B
llama_model_loader: - kv   6:                           general.repo_url str              = https://huggingface.co/unsloth
llama_model_loader: - kv   7:                          qwen3.block_count u32              = 28
llama_model_loader: - kv   8:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   9:                     qwen3.embedding_length u32              = 2048
llama_model_loader: - kv  10:                  qwen3.feed_forward_length u32              = 6144
llama_model_loader: - kv  11:                 qwen3.attention.head_count u32              = 16
llama_model_loader: - kv  12:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  13:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  14:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  15:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  16:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  22:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 151654
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 2
llama_model_loader: - kv  28:                      quantize.imatrix.file str              = Qwen3-1.7B-GGUF/imatrix_unsloth.dat
llama_model_loader: - kv  29:                   quantize.imatrix.dataset str              = unsloth_calibration_Qwen3-1.7B.txt
llama_model_loader: - kv  30:             quantize.imatrix.entries_count i32              = 196
llama_model_loader: - kv  31:              quantize.imatrix.chunks_count i32              = 685
llama_model_loader: - type  f32:  113 tensors
llama_model_loader: - type q4_0:  193 tensors
llama_model_loader: - type q4_1:    3 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 1002.15 MiB (4.89 BPW) 
load: printing all EOG tokens:
load:   - 151643 ('<|endoftext|>')
load:   - 151645 ('<|im_end|>')
load:   - 151662 ('<|fim_pad|>')
load:   - 151663 ('<|repo_name|>')
load:   - 151664 ('<|file_sep|>')
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 2048
print_info: n_layer          = 28
print_info: n_head           = 16
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 2
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 6144
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: model type       = 1.7B
print_info: model params     = 1.72 B
print_info: general.name     = Qwen3-1.7B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 11 ','
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151654 '<|vision_pad|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors:   CPU_Mapped model buffer size =  1002.15 MiB
.............................................................................
llama_context: constructing llama_context
llama_context: n_seq_max     = 4
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 512
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (512) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     2.32 MiB
llama_kv_cache_unified:        CPU KV buffer size =   224.00 MiB
llama_kv_cache_unified: size =  224.00 MiB (   512 cells,  28 layers,  4/4 seqs), K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:        CPU compute buffer size =   300.75 MiB
llama_context: graph nodes  = 1098
llama_context: graph splits = 1
common_init_from_params: added <|endoftext|> logit bias = -inf
common_init_from_params: added <|im_end|> logit bias = -inf
common_init_from_params: added <|fim_pad|> logit bias = -inf
common_init_from_params: added <|repo_name|> logit bias = -inf
common_init_from_params: added <|file_sep|> logit bias = -inf
common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)

system_info: n_threads = 64 (n_threads_batch = 64) / 64 | CPU : LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 2839.49 ms
perplexity: calculating perplexity over 584 chunks, n_ctx=512, batch_size=2048, n_seq=4
perplexity: 70.45 seconds per pass - ETA 2 hours 51.43 minutes
[1]13.0330,[2]17.6112,[3]17.0308,[4]17.5311,[5]17.8891,[6]17.7881,[7]18.0920,[8]19.0321,[9]20.2554,[10]20.7166,[11]20.7429,[12]20.7666,[13]21.3954,[14]21.0294,[15]20.7432,[16]21.2228,[17]19.9747,[18]20.1360,[19]19.8492,[20]19.8447,[21]19.4171,[22]19.4071,[23]18.6544,[24]17.6980,[25]17.2556,[26]16.7237,[27]16.1959,[28]16.1200,[29]16.2415,[30]16.1936,[31]15.9648,[32]15.9561,[33]15.6862,[34]15.8892,[35]15.9852,[36]16.2221,[37]16.4848,[38]16.5868,[39]16.4857,[40]16.5803,[41]16.5418,[42]16.4538,[43]16.6807,[44]16.7697,[45]16.7520,[46]16.7226,[47]17.0569,[48]17.2007,[49]17.1175,[50]17.2198,[51]17.2840,[52]17.3108,[53]17.3890,[54]17.4300,[55]17.4164,[56]17.5549,[57]17.4566,[58]17.4995,[59]17.5869,[60]17.6634,[61]17.6843,[62]17.7421,[63]17.8792,[64]18.0242,[65]18.2262,[66]18.3528,[67]18.5055,[68]18.4073,[69]18.3983,[70]18.3460,[71]18.3728,[72]18.4891,[73]18.5359,[74]18.5287,[75]18.3977,[76]18.3448,[77]18.3642,[78]18.3675,[79]18.2077,[80]18.1968,[81]18.1067,[82]18.1850,[83]18.1552,[84]18.1262,[85]18.2100,[86]18.3674,[87]18.4793,[88]18.4357,[89]18.4136,[90]18.3688,[91]18.4461,[92]18.3875,[93]18.5112,[94]18.5452,[95]18.4848,[96]18.4700,[97]18.4402,[98]18.4408,[99]18.4271,[100]18.5699,[101]18.6342,[102]18.5947,[103]18.6549,[104]18.5615,[105]18.6009,[106]18.5587,[107]18.6156,[108]18.6949,[109]18.7447,[110]18.8359,[111]19.0348,[112]19.0780,[113]18.9400,[114]18.9970,[115]19.0431,[116]18.9915,[117]18.9674,[118]18.9216,[119]18.7968,[120]18.8451,[121]18.7891,[122]18.8285,[123]18.8006,[124]18.7150,[125]18.6793,[126]18.6969,[127]18.6514,[128]18.6068,[129]18.5729,[130]18.5237,[131]18.4183,[132]18.3256,[133]18.2683,[134]18.2697,[135]18.2450,[136]18.2104,[137]18.1216,[138]18.0512,[139]18.0674,[140]18.0617,[141]18.0631,[142]18.0763,[143]18.1107,[144]18.1863,[145]18.1477,[146]18.0610,[147]17.9782,[148]17.9102,[149]17.8854,[150]17.7851,[151]17.7979,[152]17.7690,[153]17.7580,[154]17.6587,[155]17.7032,[156]17.6223,[157]17.5766,[158]17.5136,[159]17.4627,[160]17.3939,[161]17.3554,[162]17.3509,[163]17.3494,[164]17.3504,[165]17.3067,[166]17.2945,[167]17.2468,[168]17.2961,[169]17.2845,[170]17.3655,[171]17.4471,[172]17.5235,[173]17.6074,[174]17.6543,[175]17.7295,[176]17.8060,[177]17.9129,[178]17.9902,[179]18.0483,[180]18.0552,[181]18.1716,[182]18.2297,[183]18.2884,[184]18.3535,[185]18.4028,[186]18.4318,[187]18.4431,[188]18.4409,[189]18.4662,[190]18.5139,[191]18.5184,[192]18.5530,[193]18.5647,[194]18.6037,[195]18.6246,[196]18.6314,[197]18.6372,[198]18.5806,[199]18.5551,[200]18.5431,[201]18.6024,[202]18.6306,[203]18.6601,[204]18.6808,[205]18.6752,[206]18.6661,[207]18.7192,[208]18.6698,[209]18.6671,[210]18.6757,[211]18.6756,[212]18.6771,[213]18.7265,[214]18.7057,[215]18.6633,[216]18.6543,[217]18.6516,[218]18.6469,[219]18.5792,[220]18.5483,[221]18.5038,[222]18.4697,[223]18.4623,[224]18.4836,[225]18.4291,[226]18.4231,[227]18.3971,[228]18.3346,[229]18.2748,[230]18.2281,[231]18.1812,[232]18.1411,[233]18.1343,[234]18.1443,[235]18.1309,[236]18.1045,[237]18.0569,[238]17.9932,[239]17.9507,[240]17.9439,[241]17.9240,[242]17.9262,[243]17.9502,[244]17.9838,[245]17.9901,[246]18.0522,[247]18.0451,[248]18.0711,[249]18.0911,[250]18.0879,[251]18.1219,[252]18.1383,[253]18.1880,[254]18.2057,[255]18.1980,[256]18.2399,[257]18.2484,[258]18.2148,[259]18.1635,[260]18.1314,[261]18.0899,[262]18.0683,[263]18.0782,[264]18.0777,[265]18.1033,[266]18.1375,[267]18.1428,[268]18.1305,[269]18.1352,[270]18.1644,[271]18.1279,[272]18.1352,[273]18.1337,[274]18.1397,[275]18.1346,[276]18.0898,[277]18.0752,[278]18.0928,[279]18.0889,[280]18.0756,[281]18.0759,[282]18.1204,[283]18.0505,[284]17.9753,[285]17.9858,[286]17.9281,[287]17.8675,[288]17.8573,[289]17.8618,[290]17.9177,[291]17.9151,[292]17.9071,[293]17.9044,[294]17.9442,[295]18.0082,[296]18.0694,[297]18.1018,[298]18.0927,[299]18.0595,[300]18.0611,[301]18.0544,[302]18.0596,[303]18.0454,[304]18.0710,[305]18.0544,[306]18.0415,[307]18.0563,[308]18.0498,[309]18.0455,[310]18.0727,[311]18.0731,[312]18.0488,[313]18.0387,[314]18.0495,[315]18.0146,[316]18.0538,[317]18.1027,[318]18.1078,[319]18.0833,[320]18.0906,[321]18.0551,[322]18.0843,[323]18.1125,[324]18.1428,[325]18.1681,[326]18.1930,[327]18.1671,[328]18.1652,[329]18.1224,[330]18.1248,[331]18.0972,[332]18.0947,[333]18.0967,[334]18.0756,[335]18.0382,[336]18.0348,[337]18.0469,[338]18.0625,[339]18.0589,[340]18.0683,[341]18.0373,[342]18.0438,[343]18.0304,[344]18.0403,[345]18.0378,[346]18.0325,[347]18.0054,[348]18.0183,[349]18.0148,[350]18.0147,[351]18.0287,[352]18.0284,[353]18.0182,[354]17.9804,[355]18.0063,[356]18.0373,[357]18.0537,[358]18.0384,[359]18.0336,[360]18.0514,[361]18.0577,[362]18.0664,[363]18.0620,[364]18.1847,[365]18.2154,[366]18.2765,[367]18.3169,[368]18.3747,[369]18.4054,[370]18.4322,[371]18.4675,[372]18.5079,[373]18.5250,[374]18.5505,[375]18.5947,[376]18.6395,[377]18.6580,[378]18.6962,[379]18.7202,[380]18.7484,[381]18.7745,[382]18.7932,[383]18.8061,[384]18.8438,[385]18.8998,[386]18.9514,[387]18.9590,[388]18.9434,[389]18.9671,[390]19.0072,[391]19.0362,[392]19.0241,[393]19.0182,[394]19.0005,[395]19.0174,[396]19.0266,[397]19.0275,[398]19.0464,[399]19.0488,[400]19.0813,[401]19.0908,[402]19.0984,[403]19.0732,[404]19.0566,[405]19.0415,[406]19.0368,[407]19.0426,[408]19.0550,[409]19.0498,[410]19.0512,[411]19.0770,[412]19.0902,[413]19.0953,[414]19.0847,[415]19.0664,[416]19.0593,[417]19.0697,[418]19.0795,[419]19.0760,[420]19.0606,[421]19.0624,[422]19.0316,[423]19.0385,[424]19.0438,[425]19.0480,[426]19.0609,[427]19.0983,[428]19.1311,[429]19.1440,[430]19.1274,[431]19.1082,[432]19.1181,[433]19.1151,[434]19.1005,[435]19.1205,[436]19.0777,[437]19.0732,[438]19.0710,[439]19.0527,[440]19.0730,[441]19.0798,[442]19.0660,[443]19.0519,[444]19.0663,[445]19.0419,[446]19.0762,[447]19.0825,[448]19.0620,[449]19.0481,[450]19.0513,[451]19.0415,[452]19.0348,[453]19.0203,[454]19.0022,[455]19.0086,[456]19.0034,[457]19.0156,[458]19.0384,[459]19.0423,[460]19.0458,[461]19.0417,[462]19.0450,[463]19.0661,[464]19.0621,[465]19.0689,[466]19.0712,[467]19.0846,[468]19.1058,[469]19.1170,[470]19.1252,[471]19.1083,[472]19.1223,[473]19.0890,[474]19.0860,[475]19.0917,[476]19.1131,[477]19.0980,[478]19.0673,[479]19.0735,[480]19.0913,[481]19.1250,[482]19.0925,[483]19.1119,[484]19.1271,[485]19.1274,[486]19.1164,[487]19.1226,[488]19.0895,[489]19.0603,[490]19.0546,[491]19.0339,[492]19.0302,[493]18.9914,[494]18.9779,[495]18.9459,[496]18.9467,[497]18.9721,[498]18.9797,[499]18.9603,[500]18.9552,[501]18.9454,[502]18.9465,[503]18.9779,[504]18.9938,[505]18.9869,[506]18.9828,[507]18.9634,[508]18.9674,[509]18.9454,[510]18.9542,[511]18.9684,[512]18.9659,[513]18.9792,[514]18.9798,[515]19.0104,[516]19.0063,[517]19.0084,[518]19.0148,[519]19.0077,[520]18.9958,[521]18.9930,[522]18.9719,[523]18.9730,[524]18.9652,[525]18.9731,[526]18.9786,[527]18.9702,[528]18.9645,[529]18.9627,[530]18.9504,[531]18.9489,[532]18.9406,[533]18.9462,[534]18.9402,[535]18.9487,[536]18.9443,[537]18.9642,[538]18.9769,[539]18.9787,[540]19.0103,[541]19.0268,[542]18.9986,[543]19.0016,[544]19.0323,[545]19.0239,[546]19.0128,[547]18.9981,[548]18.9822,[549]18.9852,[550]18.9487,[551]18.9195,[552]18.8899,[553]18.8066,[554]18.7926,[555]18.8104,[556]18.8105,[557]18.7998,[558]18.7851,[559]18.7889,[560]18.7995,[561]18.8012,[562]18.8301,[563]18.8479,[564]18.8370,[565]18.8517,[566]18.8565,[567]18.8334,[568]18.8286,[569]18.8078,[570]18.8172,[571]18.8171,[572]18.8455,[573]18.8555,[574]18.8617,[575]18.8627,[576]18.8855,[577]18.8815,[578]18.8974,[579]18.9185,[580]18.9635,[581]18.9691,[582]18.9933,[583]18.9715,[584]18.9771,
Final estimate: PPL = 18.9771 +/- 0.18091

llama_perf_context_print:        load time =    1757.96 ms
llama_perf_context_print: prompt eval time = 10146329.87 ms / 299008 tokens (   33.93 ms per token,    29.47 tokens per second)
llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_perf_context_print:       total time = 10198288.33 ms / 299009 tokens
llama_perf_context_print:    graphs reused =          0
状态: 完成
-------------------------------------------
执行时间: 2025-12-20 18:31:11
测试配置: [变体: base] [模型: Qwen3-1.7B-Q4_0.gguf.aria2] [线程: 64] [第 1 次运行]
命令: ./llama-perplexity -m /root/wmw/unsloth_riscv/Qwen3-1.7B-GGUF/Qwen3-1.7B-Q4_0.gguf.aria2 -t 64 -f /root/wmw/hzeng_vw/riscv_kernels/e2e/rvv10/../wiki.test.raw
build: 0 (unknown) with riscv64-unknown-linux-gnu-gcc () 15.1.0 for riscv64-unknown-linux-gnu
gguf_init_from_file_impl: invalid magic characters: '????', expected 'GGUF'
llama_model_load: error loading model: llama_model_loader: failed to load model from /root/wmw/unsloth_riscv/Qwen3-1.7B-GGUF/Qwen3-1.7B-Q4_0.gguf.aria2
llama_model_load_from_file_impl: failed to load model
common_init_from_params: failed to load model '/root/wmw/unsloth_riscv/Qwen3-1.7B-GGUF/Qwen3-1.7B-Q4_0.gguf.aria2'
main: unable to load model
状态: 失败 (请查看日志详情)
-------------------------------------------
执行时间: 2025-12-20 18:31:11
测试配置: [变体: base] [模型: Qwen3-1.7B-Q8_0.gguf] [线程: 64] [第 1 次运行]
命令: ./llama-perplexity -m /root/wmw/unsloth_riscv/Qwen3-1.7B-GGUF/Qwen3-1.7B-Q8_0.gguf -t 64 -f /root/wmw/hzeng_vw/riscv_kernels/e2e/rvv10/../wiki.test.raw
build: 0 (unknown) with riscv64-unknown-linux-gnu-gcc () 15.1.0 for riscv64-unknown-linux-gnu
llama_model_loader: loaded meta data with 32 key-value pairs and 310 tensors from /root/wmw/unsloth_riscv/Qwen3-1.7B-GGUF/Qwen3-1.7B-Q8_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3-1.7B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3-1.7B
llama_model_loader: - kv   4:                       general.quantized_by str              = Unsloth
llama_model_loader: - kv   5:                         general.size_label str              = 1.7B
llama_model_loader: - kv   6:                           general.repo_url str              = https://huggingface.co/unsloth
llama_model_loader: - kv   7:                          qwen3.block_count u32              = 28
llama_model_loader: - kv   8:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   9:                     qwen3.embedding_length u32              = 2048
llama_model_loader: - kv  10:                  qwen3.feed_forward_length u32              = 6144
llama_model_loader: - kv  11:                 qwen3.attention.head_count u32              = 16
llama_model_loader: - kv  12:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  13:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  14:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  15:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  16:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  22:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 151654
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 7
llama_model_loader: - kv  28:                      quantize.imatrix.file str              = Qwen3-1.7B-GGUF/imatrix_unsloth.dat
llama_model_loader: - kv  29:                   quantize.imatrix.dataset str              = unsloth_calibration_Qwen3-1.7B.txt
llama_model_loader: - kv  30:             quantize.imatrix.entries_count i32              = 196
llama_model_loader: - kv  31:              quantize.imatrix.chunks_count i32              = 685
llama_model_loader: - type  f32:  113 tensors
llama_model_loader: - type q8_0:  197 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q8_0
print_info: file size   = 1.70 GiB (8.50 BPW) 
load: printing all EOG tokens:
load:   - 151643 ('<|endoftext|>')
load:   - 151645 ('<|im_end|>')
load:   - 151662 ('<|fim_pad|>')
load:   - 151663 ('<|repo_name|>')
load:   - 151664 ('<|file_sep|>')
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 2048
print_info: n_layer          = 28
print_info: n_head           = 16
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 2
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 6144
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: model type       = 1.7B
print_info: model params     = 1.72 B
print_info: general.name     = Qwen3-1.7B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 11 ','
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151654 '<|vision_pad|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors:   CPU_Mapped model buffer size =  1743.77 MiB
...................................................................................
llama_context: constructing llama_context
llama_context: n_seq_max     = 4
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 512
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (512) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     2.32 MiB
llama_kv_cache_unified:        CPU KV buffer size =   224.00 MiB
llama_kv_cache_unified: size =  224.00 MiB (   512 cells,  28 layers,  4/4 seqs), K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:        CPU compute buffer size =   300.75 MiB
llama_context: graph nodes  = 1098
llama_context: graph splits = 1
common_init_from_params: added <|endoftext|> logit bias = -inf
common_init_from_params: added <|im_end|> logit bias = -inf
common_init_from_params: added <|fim_pad|> logit bias = -inf
common_init_from_params: added <|repo_name|> logit bias = -inf
common_init_from_params: added <|file_sep|> logit bias = -inf
common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)

system_info: n_threads = 64 (n_threads_batch = 64) / 64 | CPU : LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 2978.95 ms
perplexity: calculating perplexity over 584 chunks, n_ctx=512, batch_size=2048, n_seq=4
perplexity: 84.95 seconds per pass - ETA 3 hours 26.70 minutes
[1]11.7780,[2]15.3230,[3]15.1188,[4]15.6493,[5]15.5926,[6]15.6866,[7]16.0785,[8]16.7759,[9]17.8863,[10]18.4172,[11]18.5169,[12]18.5908,[13]19.2280,[14]18.8388,[15]18.7701,[16]19.1443,[17]18.0559,[18]18.1981,[19]18.0057,[20]18.0111,[21]17.7395,[22]17.7861,[23]17.1154,[24]16.2070,[25]15.8134,[26]15.3757,[27]14.9202,[28]14.7555,[29]14.8208,[30]14.7613,[31]14.5624,[32]14.5486,[33]14.2982,[34]14.5685,[35]14.6610,[36]14.9011,[37]15.1325,[38]15.2285,[39]15.1827,[40]15.2878,[41]15.2461,[42]15.1561,[43]15.2822,[44]15.3320,[45]15.3165,[46]15.2658,[47]15.5714,[48]15.6913,[49]15.6044,[50]15.6981,[51]15.7702,[52]15.8111,[53]15.8979,[54]15.9426,[55]15.9447,[56]16.0682,[57]15.9823,[58]16.0364,[59]16.1127,[60]16.1907,[61]16.2292,[62]16.2771,[63]16.3599,[64]16.4848,[65]16.6709,[66]16.8002,[67]16.9432,[68]16.8533,[69]16.8312,[70]16.7948,[71]16.8126,[72]16.9135,[73]16.9495,[74]16.9493,[75]16.8332,[76]16.7967,[77]16.8406,[78]16.8391,[79]16.7085,[80]16.7195,[81]16.6521,[82]16.7291,[83]16.6897,[84]16.6491,[85]16.7024,[86]16.8645,[87]16.9671,[88]16.9391,[89]16.9191,[90]16.8816,[91]16.9469,[92]16.9025,[93]17.0221,[94]17.0622,[95]17.0119,[96]16.9944,[97]16.9578,[98]16.9608,[99]16.9268,[100]17.0525,[101]17.1118,[102]17.0727,[103]17.1138,[104]17.0392,[105]17.0693,[106]17.0374,[107]17.0754,[108]17.1424,[109]17.1954,[110]17.2808,[111]17.4776,[112]17.5057,[113]17.3702,[114]17.4256,[115]17.4639,[116]17.3982,[117]17.3749,[118]17.3354,[119]17.2192,[120]17.2694,[121]17.2250,[122]17.2440,[123]17.2131,[124]17.1215,[125]17.1001,[126]17.0977,[127]17.0629,[128]17.0229,[129]16.9806,[130]16.9432,[131]16.8434,[132]16.7574,[133]16.7074,[134]16.7159,[135]16.6928,[136]16.6534,[137]16.5739,[138]16.5071,[139]16.5203,[140]16.4987,[141]16.4827,[142]16.4950,[143]16.5194,[144]16.5905,[145]16.5511,[146]16.4656,[147]16.3801,[148]16.3068,[149]16.2913,[150]16.2085,[151]16.1941,[152]16.1799,[153]16.1697,[154]16.0931,[155]16.1389,[156]16.0759,[157]16.0354,[158]15.9848,[159]15.9449,[160]15.8797,[161]15.8478,[162]15.8466,[163]15.8517,[164]15.8628,[165]15.8350,[166]15.8183,[167]15.7777,[168]15.8152,[169]15.7947,[170]15.8687,[171]15.9352,[172]16.0059,[173]16.0843,[174]16.1153,[175]16.1908,[176]16.2553,[177]16.3452,[178]16.4139,[179]16.4690,[180]16.4732,[181]16.5322,[182]16.5862,[183]16.6246,[184]16.6860,[185]16.7290,[186]16.7512,[187]16.7597,[188]16.7643,[189]16.7852,[190]16.8295,[191]16.8350,[192]16.8619,[193]16.8690,[194]16.9056,[195]16.9258,[196]16.9369,[197]16.9440,[198]16.8908,[199]16.8694,[200]16.8530,[201]16.8924,[202]16.9190,[203]16.9526,[204]16.9725,[205]16.9737,[206]16.9652,[207]17.0151,[208]16.9785,[209]16.9791,[210]16.9757,[211]16.9723,[212]16.9705,[213]16.9868,[214]16.9555,[215]16.9168,[216]16.9112,[217]16.9051,[218]16.8988,[219]16.8379,[220]16.8125,[221]16.7734,[222]16.7368,[223]16.7263,[224]16.7389,[225]16.6962,[226]16.6857,[227]16.6593,[228]16.5981,[229]16.5397,[230]16.4939,[231]16.4530,[232]16.4141,[233]16.4115,[234]16.4252,[235]16.4123,[236]16.3798,[237]16.3428,[238]16.2894,[239]16.2556,[240]16.2577,[241]16.2423,[242]16.2510,[243]16.2712,[244]16.2946,[245]16.3007,[246]16.3540,[247]16.3468,[248]16.3619,[249]16.3826,[250]16.3804,[251]16.4094,[252]16.4215,[253]16.4641,[254]16.4868,[255]16.4814,[256]16.5175,[257]16.5192,[258]16.4948,[259]16.4484,[260]16.4199,[261]16.3797,[262]16.3626,[263]16.3711,[264]16.3759,[265]16.4061,[266]16.4363,[267]16.4428,[268]16.4340,[269]16.4428,[270]16.4651,[271]16.4315,[272]16.4443,[273]16.4433,[274]16.4475,[275]16.4505,[276]16.4195,[277]16.4067,[278]16.4170,[279]16.4148,[280]16.4008,[281]16.4048,[282]16.4478,[283]16.3924,[284]16.3288,[285]16.3415,[286]16.2930,[287]16.2416,[288]16.2368,[289]16.2438,[290]16.2959,[291]16.2997,[292]16.2930,[293]16.2950,[294]16.3295,[295]16.3837,[296]16.4528,[297]16.4886,[298]16.4808,[299]16.4520,[300]16.4510,[301]16.4446,[302]16.4499,[303]16.4367,[304]16.4600,[305]16.4500,[306]16.4388,[307]16.4519,[308]16.4407,[309]16.4334,[310]16.4545,[311]16.4547,[312]16.4331,[313]16.4233,[314]16.4313,[315]16.4020,[316]16.4339,[317]16.4820,[318]16.4841,[319]16.4638,[320]16.4679,[321]16.4362,[322]16.4587,[323]16.4855,[324]16.5129,[325]16.5360,[326]16.5493,[327]16.5254,[328]16.5246,[329]16.4821,[330]16.4750,[331]16.4505,[332]16.4490,[333]16.4473,[334]16.4268,[335]16.3923,[336]16.3880,[337]16.3989,[338]16.4118,[339]16.4064,[340]16.3966,[341]16.3718,[342]16.3698,[343]16.3574,[344]16.3697,[345]16.3709,[346]16.3684,[347]16.3446,[348]16.3579,[349]16.3541,[350]16.3516,[351]16.3644,[352]16.3676,[353]16.3623,[354]16.3327,[355]16.3571,[356]16.3741,[357]16.3863,[358]16.3676,[359]16.3621,[360]16.3755,[361]16.3851,[362]16.3911,[363]16.3834,[364]16.4573,[365]16.4873,[366]16.5406,[367]16.5781,[368]16.6288,[369]16.6612,[370]16.6856,[371]16.7209,[372]16.7559,[373]16.7736,[374]16.7968,[375]16.8355,[376]16.8687,[377]16.8838,[378]16.9048,[379]16.9239,[380]16.9492,[381]16.9726,[382]16.9942,[383]17.0096,[384]17.0407,[385]17.0881,[386]17.1330,[387]17.1364,[388]17.1258,[389]17.1519,[390]17.1924,[391]17.2216,[392]17.2119,[393]17.2050,[394]17.1914,[395]17.2036,[396]17.2144,[397]17.2151,[398]17.2258,[399]17.2266,[400]17.2588,[401]17.2679,[402]17.2747,[403]17.2518,[404]17.2411,[405]17.2237,[406]17.2190,[407]17.2237,[408]17.2338,[409]17.2314,[410]17.2341,[411]17.2581,[412]17.2701,[413]17.2726,[414]17.2641,[415]17.2472,[416]17.2397,[417]17.2465,[418]17.2517,[419]17.2542,[420]17.2416,[421]17.2425,[422]17.2124,[423]17.2238,[424]17.2281,[425]17.2330,[426]17.2440,[427]17.2727,[428]17.3064,[429]17.3157,[430]17.2990,[431]17.2825,[432]17.2943,[433]17.2904,[434]17.2747,[435]17.2910,[436]17.2531,[437]17.2468,[438]17.2442,[439]17.2261,[440]17.2433,[441]17.2493,[442]17.2344,[443]17.2238,[444]17.2329,[445]17.2059,[446]17.2283,[447]17.2289,[448]17.2107,[449]17.1988,[450]17.2010,[451]17.1902,[452]17.1803,[453]17.1637,[454]17.1478,[455]17.1533,[456]17.1451,[457]17.1574,[458]17.1812,[459]17.1825,[460]17.1867,[461]17.1857,[462]17.1900,[463]17.2075,[464]17.2077,[465]17.2131,[466]17.2165,[467]17.2288,[468]17.2458,[469]17.2556,[470]17.2627,[471]17.2467,[472]17.2613,[473]17.2335,[474]17.2315,[475]17.2383,[476]17.2557,[477]17.2424,[478]17.2182,[479]17.2238,[480]17.2424,[481]17.2624,[482]17.2326,[483]17.2508,[484]17.2673,[485]17.2698,[486]17.2625,[487]17.2697,[488]17.2418,[489]17.2195,[490]17.2155,[491]17.1974,[492]17.1900,[493]17.1585,[494]17.1456,[495]17.1196,[496]17.1179,[497]17.1375,[498]17.1451,[499]17.1276,[500]17.1250,[501]17.1169,[502]17.1212,[503]17.1493,[504]17.1653,[505]17.1601,[506]17.1590,[507]17.1411,[508]17.1442,[509]17.1232,[510]17.1291,[511]17.1404,[512]17.1366,[513]17.1472,[514]17.1460,[515]17.1526,[516]17.1504,[517]17.1507,[518]17.1484,[519]17.1412,[520]17.1320,[521]17.1312,[522]17.1142,[523]17.1154,[524]17.1096,[525]17.1165,[526]17.1221,[527]17.1171,[528]17.1144,[529]17.1102,[530]17.0965,[531]17.0946,[532]17.0853,[533]17.0903,[534]17.0857,[535]17.0928,[536]17.0909,[537]17.1091,[538]17.1212,[539]17.1242,[540]17.1534,[541]17.1587,[542]17.1341,[543]17.1386,[544]17.1599,[545]17.1528,[546]17.1443,[547]17.1281,[548]17.1124,[549]17.1154,[550]17.0839,[551]17.0579,[552]17.0326,[553]16.9598,[554]16.9493,[555]16.9634,[556]16.9636,[557]16.9542,[558]16.9412,[559]16.9452,[560]16.9567,[561]16.9595,[562]16.9852,[563]17.0008,[564]16.9918,[565]17.0068,[566]17.0095,[567]16.9913,[568]16.9872,[569]16.9702,[570]16.9731,[571]16.9744,[572]16.9957,[573]17.0047,[574]17.0082,[575]17.0098,[576]17.0315,[577]17.0304,[578]17.0434,[579]17.0631,[580]17.0895,[581]17.0946,[582]17.1175,[583]17.0975,[584]17.1029,
Final estimate: PPL = 17.1029 +/- 0.15983

llama_perf_context_print:        load time =    1917.09 ms
llama_perf_context_print: prompt eval time = 12269120.92 ms / 299008 tokens (   41.03 ms per token,    24.37 tokens per second)
llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_perf_context_print:       total time = 12329137.28 ms / 299009 tokens
llama_perf_context_print:    graphs reused =          0
状态: 完成
正在处理变体目录: vw
-------------------------------------------
执行时间: 2025-12-20 21:56:42
测试配置: [变体: vw] [模型: Qwen3-0.6B-Q4_0.gguf] [线程: 64] [第 1 次运行]
命令: ./llama-perplexity -m /root/wmw/unsloth_riscv/Qwen3-0.6B-GGUF/Qwen3-0.6B-Q4_0.gguf -t 64 -f /root/wmw/hzeng_vw/riscv_kernels/e2e/rvv10/../wiki.test.raw
build: 0 (unknown) with riscv64-unknown-linux-gnu-gcc () 15.1.0 for riscv64-unknown-linux-gnu
llama_model_loader: loaded meta data with 32 key-value pairs and 310 tensors from /root/wmw/unsloth_riscv/Qwen3-0.6B-GGUF/Qwen3-0.6B-Q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3-0.6B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3-0.6B
llama_model_loader: - kv   4:                       general.quantized_by str              = Unsloth
llama_model_loader: - kv   5:                         general.size_label str              = 0.6B
llama_model_loader: - kv   6:                           general.repo_url str              = https://huggingface.co/unsloth
llama_model_loader: - kv   7:                          qwen3.block_count u32              = 28
llama_model_loader: - kv   8:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   9:                     qwen3.embedding_length u32              = 1024
llama_model_loader: - kv  10:                  qwen3.feed_forward_length u32              = 3072
llama_model_loader: - kv  11:                 qwen3.attention.head_count u32              = 16
llama_model_loader: - kv  12:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  13:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  14:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  15:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  16:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  22:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 151654
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 2
llama_model_loader: - kv  28:                      quantize.imatrix.file str              = Qwen3-0.6B-GGUF/imatrix_unsloth.dat
llama_model_loader: - kv  29:                   quantize.imatrix.dataset str              = unsloth_calibration_Qwen3-0.6B.txt
llama_model_loader: - kv  30:             quantize.imatrix.entries_count u32              = 196
llama_model_loader: - kv  31:              quantize.imatrix.chunks_count u32              = 688
llama_model_loader: - type  f32:  113 tensors
llama_model_loader: - type q4_0:  193 tensors
llama_model_loader: - type q4_1:    3 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 358.78 MiB (5.05 BPW) 
load: printing all EOG tokens:
load:   - 151643 ('<|endoftext|>')
load:   - 151645 ('<|im_end|>')
load:   - 151662 ('<|fim_pad|>')
load:   - 151663 ('<|repo_name|>')
load:   - 151664 ('<|file_sep|>')
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 1024
print_info: n_layer          = 28
print_info: n_head           = 16
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 2
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 3072
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: model type       = 0.6B
print_info: model params     = 596.05 M
print_info: general.name     = Qwen3-0.6B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 11 ','
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151654 '<|vision_pad|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors:   CPU_Mapped model buffer size =   358.78 MiB
....................................................................
llama_context: constructing llama_context
llama_context: n_seq_max     = 4
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 512
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (512) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     2.32 MiB
llama_kv_cache_unified:        CPU KV buffer size =   224.00 MiB
llama_kv_cache_unified: size =  224.00 MiB (   512 cells,  28 layers,  4/4 seqs), K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:        CPU compute buffer size =   298.75 MiB
llama_context: graph nodes  = 1098
llama_context: graph splits = 1
common_init_from_params: added <|endoftext|> logit bias = -inf
common_init_from_params: added <|im_end|> logit bias = -inf
common_init_from_params: added <|fim_pad|> logit bias = -inf
common_init_from_params: added <|repo_name|> logit bias = -inf
common_init_from_params: added <|file_sep|> logit bias = -inf
common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)

system_info: n_threads = 64 (n_threads_batch = 64) / 64 | CPU : RISCV_V = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 2865.46 ms
perplexity: calculating perplexity over 584 chunks, n_ctx=512, batch_size=2048, n_seq=4
perplexity: 10.33 seconds per pass - ETA 25.13 minutes
[1]151936.0000,[2]151936.0000,[3]151936.0000,[4]151936.0000,[5]151936.0000,[6]151936.0000,[7]151936.0000,[8]151936.0000,[9]151936.0000,[10]151936.0000,[11]151936.0000,[12]151936.0000,[13]151936.0000,[14]151936.0000,[15]151936.0000,[16]151936.0000,[17]151936.0000,[18]151936.0000,[19]151936.0000,[20]151936.0000,[21]151936.0000,[22]151936.0000,[23]151936.0000,[24]151936.0000,[25]151936.0000,[26]151936.0000,[27]151936.0000,[28]151936.0000,[29]151936.0000,[30]151936.0000,[31]151936.0000,[32]151936.0000,[33]151936.0000,[34]151936.0000,[35]151936.0000,[36]151936.0000,[37]151936.0000,[38]151936.0000,[39]151936.0000,[40]151936.0000,[41]151936.0000,[42]151936.0000,[43]151936.0000,[44]151936.0000,[45]151936.0000,[46]151936.0000,[47]151936.0000,[48]151936.0000,[49]151936.0000,[50]151936.0000,[51]151936.0000,[52]151936.0000,[53]151936.0000,[54]151936.0000,[55]151936.0000,[56]151936.0000,[57]151936.0000,[58]151936.0000,[59]151936.0000,[60]151936.0000,[61]151936.0000,[62]151936.0000,[63]151936.0000,[64]151936.0000,[65]151936.0000,[66]151936.0000,[67]151936.0000,[68]151936.0000,[69]151936.0000,[70]151936.0000,[71]151936.0000,[72]151936.0000,[73]151936.0000,[74]151936.0000,[75]151936.0000,[76]151936.0000,[77]151936.0000,[78]151936.0000,[79]151936.0000,[80]151936.0000,[81]151936.0000,[82]151936.0000,[83]151936.0000,[84]151936.0000,[85]151936.0000,[86]151936.0000,[87]151936.0000,[88]151936.0000,[89]151936.0000,[90]151936.0000,[91]151936.0000,[92]151936.0000,[93]151936.0000,[94]151936.0000,[95]151936.0000,[96]151936.0000,[97]151936.0000,[98]151936.0000,[99]151936.0000,[100]151936.0000,[101]151936.0000,[102]151936.0000,[103]151936.0000,[104]151936.0000,[105]151936.0000,[106]151936.0000,[107]151936.0000,[108]151936.0000,[109]151936.0000,[110]151936.0000,[111]151936.0000,[112]151936.0000,[113]151936.0000,[114]151936.0000,[115]151936.0000,[116]151936.0000,[117]151936.0000,[118]151936.0000,[119]151936.0000,[120]151936.0000,[121]151936.0000,[122]151936.0000,[123]151936.0000,[124]151936.0000,[125]151936.0000,[126]151936.0000,[127]151936.0000,[128]151936.0000,[129]151936.0000,[130]151936.0000,[131]151936.0000,[132]151936.0000,[133]151936.0000,[134]151936.0000,[135]151936.0000,[136]151936.0000,[137]151936.0000,[138]151936.0000,[139]151936.0000,[140]151936.0000,[141]151936.0000,[142]151936.0000,[143]151936.0000,[144]151936.0000,[145]151936.0000,[146]151936.0000,[147]151936.0000,[148]151936.0000,[149]151936.0000,[150]151936.0000,[151]151936.0000,[152]151936.0000,[153]151936.0000,[154]151936.0000,[155]151936.0000,[156]151936.0000,[157]151936.0000,[158]151936.0000,[159]151936.0000,[160]151936.0000,[161]151936.0000,[162]151936.0000,[163]151936.0000,[164]151936.0000,[165]151936.0000,[166]151936.0000,[167]151936.0000,[168]151936.0000,[169]151936.0000,[170]151936.0000,[171]151936.0000,[172]151936.0000,[173]151936.0000,[174]151936.0000,[175]151936.0000,[176]151936.0000,[177]151936.0000,[178]151936.0000,[179]151936.0000,[180]151936.0000,[181]151936.0000,[182]151936.0000,[183]151936.0000,[184]151936.0000,[185]151936.0000,[186]151936.0000,[187]151936.0000,[188]151936.0000,[189]151936.0000,[190]151936.0000,[191]151936.0000,[192]151936.0000,[193]151936.0000,[194]151936.0000,[195]151936.0000,[196]151936.0000,[197]151936.0000,[198]151936.0000,[199]151936.0000,[200]151936.0000,[201]151936.0000,[202]151936.0000,[203]151936.0000,[204]151936.0000,[205]151936.0000,[206]151936.0000,[207]151936.0000,[208]151936.0000,[209]151936.0000,[210]151936.0000,[211]151936.0000,[212]151936.0000,[213]151936.0000,[214]151936.0000,[215]151936.0000,[216]151936.0000,[217]151936.0000,[218]151936.0000,[219]151936.0000,[220]151936.0000,[221]151936.0000,[222]151936.0000,[223]151936.0000,[224]151936.0000,[225]151936.0000,[226]151936.0000,[227]151936.0000,[228]151936.0000,[229]151936.0000,[230]151936.0000,[231]151936.0000,[232]151936.0000,[233]151936.0000,[234]151936.0000,[235]151936.0000,[236]151936.0000,[237]151936.0000,[238]151936.0000,[239]151936.0000,[240]151936.0000,[241]151936.0000,[242]151936.0000,[243]151936.0000,[244]151936.0000,[245]151936.0000,[246]151936.0000,[247]151936.0000,[248]151936.0000,[249]151936.0000,[250]151936.0000,[251]151936.0000,[252]151936.0000,[253]151936.0000,[254]151936.0000,[255]151936.0000,[256]151936.0000,[257]151936.0000,[258]151936.0000,[259]151936.0000,[260]151936.0000,[261]151936.0000,[262]151936.0000,[263]151936.0000,[264]151936.0000,[265]151936.0000,[266]151936.0000,[267]151936.0000,[268]151936.0000,[269]151936.0000,[270]151936.0000,[271]151936.0000,[272]151936.0000,[273]151936.0000,[274]151936.0000,[275]151936.0000,[276]151936.0000,[277]151936.0000,[278]151936.0000,[279]151936.0000,[280]151936.0000,[281]151936.0000,[282]151936.0000,[283]151936.0000,[284]151936.0000,[285]151936.0000,[286]151936.0000,[287]151936.0000,[288]151936.0000,[289]151936.0000,[290]151936.0000,[291]151936.0000,[292]151936.0000,[293]151936.0000,[294]151936.0000,[295]151936.0000,[296]151936.0000,[297]151936.0000,[298]151936.0000,[299]151936.0000,[300]151936.0000,[301]151936.0000,[302]151936.0000,[303]151936.0000,[304]151936.0000,[305]151936.0000,[306]151936.0000,[307]151936.0000,[308]151936.0000,[309]151936.0000,[310]151936.0000,[311]151936.0000,[312]151936.0000,[313]151936.0000,[314]151936.0000,[315]151936.0000,[316]151936.0000,[317]151936.0000,[318]151936.0000,[319]151936.0000,[320]151936.0000,[321]151936.0000,[322]151936.0000,[323]151936.0000,[324]151936.0000,[325]151936.0000,[326]151936.0000,[327]151936.0000,[328]151936.0000,[329]151936.0000,[330]151936.0000,[331]151936.0000,[332]151936.0000,[333]151936.0000,[334]151936.0000,[335]151936.0000,[336]151936.0000,[337]151936.0000,[338]151936.0000,[339]151936.0000,[340]151936.0000,[341]151936.0000,[342]151936.0000,[343]151936.0000,[344]151936.0000,[345]151936.0000,[346]151936.0000,[347]151936.0000,[348]151936.0000,[349]151936.0000,[350]151936.0000,[351]151936.0000,[352]151936.0000,[353]151936.0000,[354]151936.0000,[355]151936.0000,[356]151936.0000,[357]151936.0000,[358]151936.0000,[359]151936.0000,[360]151936.0000,[361]151936.0000,[362]151936.0000,[363]151936.0000,[364]151936.0000,[365]151936.0000,[366]151936.0000,[367]151936.0000,[368]151936.0000,[369]151936.0000,[370]151936.0000,[371]151936.0000,[372]151936.0000,[373]151936.0000,[374]151936.0000,[375]151936.0000,[376]151936.0000,[377]151936.0000,[378]151936.0000,[379]151936.0000,[380]151936.0000,[381]151936.0000,[382]151936.0000,[383]151936.0000,[384]151936.0000,[385]151936.0000,[386]151936.0000,[387]151936.0000,[388]151936.0000,[389]151936.0000,[390]151936.0000,[391]151936.0000,[392]151936.0000,[393]151936.0000,[394]151936.0000,[395]151936.0000,[396]151936.0000,[397]151936.0000,[398]151936.0000,[399]151936.0000,[400]151936.0000,[401]151936.0000,[402]151936.0000,[403]151936.0000,[404]151936.0000,[405]151936.0000,[406]151936.0000,[407]151936.0000,[408]151936.0000,[409]151936.0000,[410]151936.0000,[411]151936.0000,[412]151936.0000,[413]151936.0000,[414]151936.0000,[415]151936.0000,[416]151936.0000,[417]151936.0000,[418]151936.0000,[419]151936.0000,[420]151936.0000,[421]151936.0000,[422]151936.0000,[423]151936.0000,[424]151936.0000,[425]151936.0000,[426]151936.0000,[427]151936.0000,[428]151936.0000,[429]151936.0000,[430]151936.0000,[431]151936.0000,[432]151936.0000,[433]151936.0000,[434]151936.0000,[435]151936.0000,[436]151936.0000,[437]151936.0000,[438]151936.0000,[439]151936.0000,[440]151936.0000,[441]151936.0000,[442]151936.0000,[443]151936.0000,[444]151936.0000,[445]151936.0000,[446]151936.0000,[447]151936.0000,[448]151936.0000,[449]151936.0000,[450]151936.0000,[451]151936.0000,[452]151936.0000,[453]151936.0000,[454]151936.0000,[455]151936.0000,[456]151936.0000,[457]151936.0000,[458]151936.0000,[459]151936.0000,[460]151936.0000,[461]151936.0000,[462]151936.0000,[463]151936.0000,[464]151936.0000,[465]151936.0000,[466]151936.0000,[467]151936.0000,[468]151936.0000,[469]151936.0000,[470]151936.0000,[471]151936.0000,[472]151936.0000,[473]151936.0000,[474]151936.0000,[475]151936.0000,[476]151936.0000,[477]151936.0000,[478]151936.0000,[479]151936.0000,[480]151936.0000,[481]151936.0000,[482]151936.0000,[483]151936.0000,[484]151936.0000,[485]151936.0000,[486]151936.0000,[487]151936.0000,[488]151936.0000,[489]151936.0000,[490]151936.0000,[491]151936.0000,[492]151936.0000,[493]151936.0000,[494]151936.0000,[495]151936.0000,[496]151936.0000,[497]151936.0000,[498]151936.0000,[499]151936.0000,[500]151936.0000,[501]151936.0000,[502]151936.0000,[503]151936.0000,[504]151936.0000,[505]151936.0000,[506]151936.0000,[507]151936.0000,[508]151936.0000,[509]151936.0000,[510]151936.0000,[511]151936.0000,[512]151936.0000,[513]151936.0000,[514]151936.0000,[515]151936.0000,[516]151936.0000,[517]151936.0000,[518]151936.0000,[519]151936.0000,[520]151936.0000,[521]151936.0000,[522]151936.0000,[523]151936.0000,[524]151936.0000,[525]151936.0000,[526]151936.0000,[527]151936.0000,[528]151936.0000,[529]151936.0000,[530]151936.0000,[531]151936.0000,[532]151936.0000,[533]151936.0000,[534]151936.0000,[535]151936.0000,[536]151936.0000,[537]151936.0000,[538]151936.0000,[539]151936.0000,[540]151936.0000,[541]151936.0000,[542]151936.0000,[543]151936.0000,[544]151936.0000,[545]151936.0000,[546]151936.0000,[547]151936.0000,[548]151936.0000,[549]151936.0000,[550]151936.0000,[551]151936.0000,[552]151936.0000,[553]151936.0000,[554]151936.0000,[555]151936.0000,[556]151936.0000,[557]151936.0000,[558]151936.0000,[559]151936.0000,[560]151936.0000,[561]151936.0000,[562]151936.0000,[563]151936.0000,[564]151936.0000,[565]151936.0000,[566]151936.0000,[567]151936.0000,[568]151936.0000,[569]151936.0000,[570]151936.0000,[571]151936.0000,[572]151936.0000,[573]151936.0000,[574]151936.0000,[575]151936.0000,[576]151936.0000,[577]151936.0000,[578]151936.0000,[579]151936.0000,[580]151936.0000,[581]151936.0000,[582]151936.0000,[583]151936.0000,[584]151936.0000,
Unexpected negative standard deviation of log(prob)

llama_perf_context_print:        load time =    1585.43 ms
llama_perf_context_print: prompt eval time = 1302725.41 ms / 299008 tokens (    4.36 ms per token,   229.52 tokens per second)
llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_perf_context_print:       total time = 1362776.09 ms / 299009 tokens
llama_perf_context_print:    graphs reused =          0
状态: 完成
-------------------------------------------
执行时间: 2025-12-20 22:19:27
测试配置: [变体: vw] [模型: Qwen3-0.6B-Q8_0.gguf] [线程: 64] [第 1 次运行]
命令: ./llama-perplexity -m /root/wmw/unsloth_riscv/Qwen3-0.6B-GGUF/Qwen3-0.6B-Q8_0.gguf -t 64 -f /root/wmw/hzeng_vw/riscv_kernels/e2e/rvv10/../wiki.test.raw
build: 0 (unknown) with riscv64-unknown-linux-gnu-gcc () 15.1.0 for riscv64-unknown-linux-gnu
llama_model_loader: loaded meta data with 32 key-value pairs and 310 tensors from /root/wmw/unsloth_riscv/Qwen3-0.6B-GGUF/Qwen3-0.6B-Q8_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3-0.6B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3-0.6B
llama_model_loader: - kv   4:                       general.quantized_by str              = Unsloth
llama_model_loader: - kv   5:                         general.size_label str              = 0.6B
llama_model_loader: - kv   6:                           general.repo_url str              = https://huggingface.co/unsloth
llama_model_loader: - kv   7:                          qwen3.block_count u32              = 28
llama_model_loader: - kv   8:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   9:                     qwen3.embedding_length u32              = 1024
llama_model_loader: - kv  10:                  qwen3.feed_forward_length u32              = 3072
llama_model_loader: - kv  11:                 qwen3.attention.head_count u32              = 16
llama_model_loader: - kv  12:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  13:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  14:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  15:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  16:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  22:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 151654
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 7
llama_model_loader: - kv  28:                      quantize.imatrix.file str              = Qwen3-0.6B-GGUF/imatrix_unsloth.dat
llama_model_loader: - kv  29:                   quantize.imatrix.dataset str              = unsloth_calibration_Qwen3-0.6B.txt
llama_model_loader: - kv  30:             quantize.imatrix.entries_count u32              = 196
llama_model_loader: - kv  31:              quantize.imatrix.chunks_count u32              = 688
llama_model_loader: - type  f32:  113 tensors
llama_model_loader: - type q8_0:  197 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q8_0
print_info: file size   = 604.15 MiB (8.50 BPW) 
load: printing all EOG tokens:
load:   - 151643 ('<|endoftext|>')
load:   - 151645 ('<|im_end|>')
load:   - 151662 ('<|fim_pad|>')
load:   - 151663 ('<|repo_name|>')
load:   - 151664 ('<|file_sep|>')
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 1024
print_info: n_layer          = 28
print_info: n_head           = 16
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 2
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 3072
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: model type       = 0.6B
print_info: model params     = 596.05 M
print_info: general.name     = Qwen3-0.6B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 11 ','
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151654 '<|vision_pad|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors:   CPU_Mapped model buffer size =   604.15 MiB
...........................................................................
llama_context: constructing llama_context
llama_context: n_seq_max     = 4
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 512
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (512) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     2.32 MiB
llama_kv_cache_unified:        CPU KV buffer size =   224.00 MiB
llama_kv_cache_unified: size =  224.00 MiB (   512 cells,  28 layers,  4/4 seqs), K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:        CPU compute buffer size =   298.75 MiB
llama_context: graph nodes  = 1098
llama_context: graph splits = 1
common_init_from_params: added <|endoftext|> logit bias = -inf
common_init_from_params: added <|im_end|> logit bias = -inf
common_init_from_params: added <|fim_pad|> logit bias = -inf
common_init_from_params: added <|repo_name|> logit bias = -inf
common_init_from_params: added <|file_sep|> logit bias = -inf
common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)

system_info: n_threads = 64 (n_threads_batch = 64) / 64 | CPU : RISCV_V = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 2897.98 ms
perplexity: calculating perplexity over 584 chunks, n_ctx=512, batch_size=2048, n_seq=4
perplexity: 10.12 seconds per pass - ETA 24.62 minutes
[1]12.7985,[2]18.1217,[3]18.2742,[4]19.2390,[5]19.5312,[6]19.9603,[7]20.1241,[8]21.6986,[9]22.9975,[10]23.7583,[11]24.1909,[12]24.4451,[13]25.2513,[14]24.8614,[15]24.5424,[16]25.1449,[17]23.3015,[18]23.7694,[19]23.4599,[20]23.6478,[21]23.0898,[22]23.0567,[23]22.0545,[24]20.8396,[25]20.3861,[26]19.8978,[27]19.2056,[28]18.7961,[29]18.9760,[30]18.9941,[31]18.7543,[32]18.8264,[33]18.4695,[34]18.6492,[35]18.7952,[36]19.1142,[37]19.5061,[38]19.6519,[39]19.4689,[40]19.5818,[41]19.5586,[42]19.4193,[43]19.6136,[44]19.6667,[45]19.7465,[46]19.6938,[47]20.1134,[48]20.3400,[49]20.2010,[50]20.3986,[51]20.4537,[52]20.4849,[53]20.5591,[54]20.7102,[55]20.6742,[56]20.8056,[57]20.7327,[58]20.8011,[59]20.9385,[60]21.0527,[61]21.0372,[62]21.1024,[63]21.2638,[64]21.4272,[65]21.6729,[66]21.8597,[67]22.0708,[68]21.9847,[69]21.9950,[70]22.0177,[71]22.0414,[72]22.1923,[73]22.2391,[74]22.2456,[75]22.1146,[76]22.0196,[77]22.0369,[78]21.9857,[79]21.8166,[80]21.7800,[81]21.6512,[82]21.7461,[83]21.6987,[84]21.6119,[85]21.6182,[86]21.8175,[87]21.9795,[88]21.9244,[89]21.8561,[90]21.7913,[91]21.8874,[92]21.8146,[93]21.9496,[94]22.0617,[95]21.9961,[96]22.0297,[97]21.9848,[98]22.0014,[99]21.9564,[100]22.1106,[101]22.1997,[102]22.1678,[103]22.2354,[104]22.1421,[105]22.1781,[106]22.0805,[107]22.1088,[108]22.2305,[109]22.3079,[110]22.4278,[111]22.6990,[112]22.7461,[113]22.6042,[114]22.6984,[115]22.7452,[116]22.6762,[117]22.6603,[118]22.6082,[119]22.4768,[120]22.5229,[121]22.4804,[122]22.5342,[123]22.4828,[124]22.3680,[125]22.3241,[126]22.3074,[127]22.2762,[128]22.2062,[129]22.1771,[130]22.0901,[131]21.9581,[132]21.8685,[133]21.8040,[134]21.8136,[135]21.8006,[136]21.7698,[137]21.6698,[138]21.5629,[139]21.5862,[140]21.5297,[141]21.5319,[142]21.5263,[143]21.5781,[144]21.6284,[145]21.5774,[146]21.4360,[147]21.3172,[148]21.2200,[149]21.1566,[150]21.0244,[151]20.9818,[152]20.9749,[153]20.9405,[154]20.8369,[155]20.8784,[156]20.8182,[157]20.7716,[158]20.6985,[159]20.6484,[160]20.6122,[161]20.5901,[162]20.5903,[163]20.5886,[164]20.5778,[165]20.5516,[166]20.5677,[167]20.4920,[168]20.5289,[169]20.4954,[170]20.6266,[171]20.7040,[172]20.8062,[173]20.9228,[174]20.9529,[175]21.0448,[176]21.1378,[177]21.2380,[178]21.3132,[179]21.3767,[180]21.3983,[181]21.4884,[182]21.5715,[183]21.6494,[184]21.7444,[185]21.8014,[186]21.8135,[187]21.8134,[188]21.8175,[189]21.8307,[190]21.8946,[191]21.8995,[192]21.9104,[193]21.8953,[194]21.9488,[195]21.9787,[196]22.0068,[197]22.0126,[198]21.9365,[199]21.8937,[200]21.8839,[201]21.9266,[202]21.9583,[203]21.9949,[204]22.0116,[205]22.0079,[206]21.9760,[207]22.0285,[208]21.9500,[209]21.9390,[210]21.9370,[211]21.9310,[212]21.9313,[213]21.9374,[214]21.8925,[215]21.8259,[216]21.8256,[217]21.8235,[218]21.7913,[219]21.7228,[220]21.6737,[221]21.6319,[222]21.5858,[223]21.5861,[224]21.5935,[225]21.5315,[226]21.5381,[227]21.5134,[228]21.4449,[229]21.3658,[230]21.2996,[231]21.2520,[232]21.2162,[233]21.2178,[234]21.2245,[235]21.2089,[236]21.1564,[237]21.1228,[238]21.0473,[239]21.0140,[240]21.0375,[241]21.0148,[242]21.0370,[243]21.0539,[244]21.0897,[245]21.0882,[246]21.1500,[247]21.1442,[248]21.1570,[249]21.1885,[250]21.1894,[251]21.2493,[252]21.2493,[253]21.3065,[254]21.3389,[255]21.3468,[256]21.3960,[257]21.4024,[258]21.3527,[259]21.3062,[260]21.2730,[261]21.2397,[262]21.2200,[263]21.2044,[264]21.2072,[265]21.2548,[266]21.2726,[267]21.2808,[268]21.2555,[269]21.2537,[270]21.2637,[271]21.2280,[272]21.2395,[273]21.2343,[274]21.2293,[275]21.2241,[276]21.1688,[277]21.1411,[278]21.1602,[279]21.1559,[280]21.1454,[281]21.1355,[282]21.1973,[283]21.1031,[284]21.0107,[285]21.0233,[286]20.9639,[287]20.8921,[288]20.9021,[289]20.9070,[290]20.9811,[291]20.9816,[292]20.9777,[293]20.9840,[294]21.0289,[295]21.0792,[296]21.1311,[297]21.1769,[298]21.1690,[299]21.1263,[300]21.1227,[301]21.1004,[302]21.1071,[303]21.0965,[304]21.1206,[305]21.1016,[306]21.0877,[307]21.0976,[308]21.0802,[309]21.0674,[310]21.1003,[311]21.1023,[312]21.0603,[313]21.0520,[314]21.0687,[315]21.0297,[316]21.0829,[317]21.1401,[318]21.1360,[319]21.1131,[320]21.1301,[321]21.0853,[322]21.1060,[323]21.1394,[324]21.1712,[325]21.2056,[326]21.2206,[327]21.1831,[328]21.1770,[329]21.1174,[330]21.0959,[331]21.0534,[332]21.0418,[333]21.0374,[334]21.0049,[335]20.9619,[336]20.9464,[337]20.9613,[338]20.9892,[339]20.9664,[340]20.9312,[341]20.9028,[342]20.8994,[343]20.8941,[344]20.9179,[345]20.9301,[346]20.9338,[347]20.9249,[348]20.9313,[349]20.9187,[350]20.9185,[351]20.9424,[352]20.9482,[353]20.9473,[354]20.8969,[355]20.9376,[356]20.9537,[357]20.9617,[358]20.9380,[359]20.9351,[360]20.9304,[361]20.9369,[362]20.9597,[363]20.9553,[364]20.9919,[365]21.0234,[366]21.0898,[367]21.1393,[368]21.2057,[369]21.2600,[370]21.2993,[371]21.3490,[372]21.3984,[373]21.4205,[374]21.4479,[375]21.5038,[376]21.5426,[377]21.5638,[378]21.6042,[379]21.6410,[380]21.6829,[381]21.7102,[382]21.7422,[383]21.7702,[384]21.8167,[385]21.8761,[386]21.9139,[387]21.9132,[388]21.8924,[389]21.9301,[390]21.9790,[391]22.0190,[392]22.0187,[393]22.0161,[394]21.9930,[395]21.9972,[396]22.0163,[397]22.0219,[398]22.0279,[399]22.0292,[400]22.0691,[401]22.0649,[402]22.0702,[403]22.0467,[404]22.0425,[405]22.0225,[406]22.0195,[407]22.0231,[408]22.0334,[409]22.0351,[410]22.0356,[411]22.0719,[412]22.0822,[413]22.0917,[414]22.0851,[415]22.0817,[416]22.0711,[417]22.0814,[418]22.0914,[419]22.0844,[420]22.0757,[421]22.0843,[422]22.0538,[423]22.0663,[424]22.0764,[425]22.0852,[426]22.1032,[427]22.1354,[428]22.1794,[429]22.1822,[430]22.1597,[431]22.1313,[432]22.1319,[433]22.1249,[434]22.1112,[435]22.1266,[436]22.0649,[437]22.0574,[438]22.0604,[439]22.0342,[440]22.0606,[441]22.0687,[442]22.0566,[443]22.0471,[444]22.0584,[445]22.0248,[446]22.0322,[447]22.0269,[448]22.0155,[449]22.0053,[450]22.0213,[451]22.0134,[452]22.0069,[453]21.9869,[454]21.9689,[455]21.9731,[456]21.9675,[457]21.9822,[458]22.0170,[459]22.0247,[460]22.0204,[461]22.0257,[462]22.0348,[463]22.0597,[464]22.0615,[465]22.0685,[466]22.0702,[467]22.0823,[468]22.1051,[469]22.1210,[470]22.1432,[471]22.1337,[472]22.1546,[473]22.1140,[474]22.1097,[475]22.1207,[476]22.1365,[477]22.1245,[478]22.0861,[479]22.0826,[480]22.1057,[481]22.1234,[482]22.0866,[483]22.0974,[484]22.1167,[485]22.1176,[486]22.1051,[487]22.1208,[488]22.0864,[489]22.0602,[490]22.0563,[491]22.0303,[492]22.0222,[493]21.9841,[494]21.9736,[495]21.9382,[496]21.9359,[497]21.9565,[498]21.9746,[499]21.9535,[500]21.9598,[501]21.9499,[502]21.9565,[503]21.9966,[504]22.0163,[505]22.0192,[506]22.0043,[507]21.9831,[508]21.9898,[509]21.9652,[510]21.9666,[511]21.9764,[512]21.9651,[513]21.9803,[514]21.9765,[515]21.9731,[516]21.9723,[517]21.9684,[518]21.9536,[519]21.9414,[520]21.9260,[521]21.9217,[522]21.9012,[523]21.9024,[524]21.8946,[525]21.9021,[526]21.9121,[527]21.8999,[528]21.8907,[529]21.8785,[530]21.8673,[531]21.8707,[532]21.8660,[533]21.8768,[534]21.8756,[535]21.8757,[536]21.8632,[537]21.8824,[538]21.8986,[539]21.8980,[540]21.9318,[541]21.9361,[542]21.9083,[543]21.9087,[544]21.9243,[545]21.9140,[546]21.9044,[547]21.8894,[548]21.8633,[549]21.8733,[550]21.8275,[551]21.7842,[552]21.7520,[553]21.6503,[554]21.6364,[555]21.6461,[556]21.6432,[557]21.6326,[558]21.6184,[559]21.6206,[560]21.6346,[561]21.6385,[562]21.6673,[563]21.6902,[564]21.6826,[565]21.7011,[566]21.7087,[567]21.6877,[568]21.6876,[569]21.6707,[570]21.6894,[571]21.6922,[572]21.7188,[573]21.7269,[574]21.7336,[575]21.7299,[576]21.7527,[577]21.7515,[578]21.7582,[579]21.7914,[580]21.8263,[581]21.8368,[582]21.8714,[583]21.8514,[584]21.8626,
Final estimate: PPL = 21.8626 +/- 0.19096

llama_perf_context_print:        load time =    1636.27 ms
llama_perf_context_print: prompt eval time = 1226934.59 ms / 299008 tokens (    4.10 ms per token,   243.70 tokens per second)
llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_perf_context_print:       total time = 1287065.41 ms / 299009 tokens
llama_perf_context_print:    graphs reused =          0
状态: 完成
-------------------------------------------
执行时间: 2025-12-20 22:40:56
测试配置: [变体: vw] [模型: Qwen3-1.7B-Q4_0.gguf] [线程: 64] [第 1 次运行]
命令: ./llama-perplexity -m /root/wmw/unsloth_riscv/Qwen3-1.7B-GGUF/Qwen3-1.7B-Q4_0.gguf -t 64 -f /root/wmw/hzeng_vw/riscv_kernels/e2e/rvv10/../wiki.test.raw
build: 0 (unknown) with riscv64-unknown-linux-gnu-gcc () 15.1.0 for riscv64-unknown-linux-gnu
llama_model_loader: loaded meta data with 32 key-value pairs and 310 tensors from /root/wmw/unsloth_riscv/Qwen3-1.7B-GGUF/Qwen3-1.7B-Q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3-1.7B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3-1.7B
llama_model_loader: - kv   4:                       general.quantized_by str              = Unsloth
llama_model_loader: - kv   5:                         general.size_label str              = 1.7B
llama_model_loader: - kv   6:                           general.repo_url str              = https://huggingface.co/unsloth
llama_model_loader: - kv   7:                          qwen3.block_count u32              = 28
llama_model_loader: - kv   8:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   9:                     qwen3.embedding_length u32              = 2048
llama_model_loader: - kv  10:                  qwen3.feed_forward_length u32              = 6144
llama_model_loader: - kv  11:                 qwen3.attention.head_count u32              = 16
llama_model_loader: - kv  12:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  13:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  14:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  15:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  16:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  22:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 151654
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 2
llama_model_loader: - kv  28:                      quantize.imatrix.file str              = Qwen3-1.7B-GGUF/imatrix_unsloth.dat
llama_model_loader: - kv  29:                   quantize.imatrix.dataset str              = unsloth_calibration_Qwen3-1.7B.txt
llama_model_loader: - kv  30:             quantize.imatrix.entries_count i32              = 196
llama_model_loader: - kv  31:              quantize.imatrix.chunks_count i32              = 685
llama_model_loader: - type  f32:  113 tensors
llama_model_loader: - type q4_0:  193 tensors
llama_model_loader: - type q4_1:    3 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 1002.15 MiB (4.89 BPW) 
load: printing all EOG tokens:
load:   - 151643 ('<|endoftext|>')
load:   - 151645 ('<|im_end|>')
load:   - 151662 ('<|fim_pad|>')
load:   - 151663 ('<|repo_name|>')
load:   - 151664 ('<|file_sep|>')
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 2048
print_info: n_layer          = 28
print_info: n_head           = 16
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 2
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 6144
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: model type       = 1.7B
print_info: model params     = 1.72 B
print_info: general.name     = Qwen3-1.7B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 11 ','
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151654 '<|vision_pad|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors:   CPU_Mapped model buffer size =  1002.15 MiB
.............................................................................
llama_context: constructing llama_context
llama_context: n_seq_max     = 4
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 512
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (512) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     2.32 MiB
llama_kv_cache_unified:        CPU KV buffer size =   224.00 MiB
llama_kv_cache_unified: size =  224.00 MiB (   512 cells,  28 layers,  4/4 seqs), K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:        CPU compute buffer size =   300.75 MiB
llama_context: graph nodes  = 1098
llama_context: graph splits = 1
common_init_from_params: added <|endoftext|> logit bias = -inf
common_init_from_params: added <|im_end|> logit bias = -inf
common_init_from_params: added <|fim_pad|> logit bias = -inf
common_init_from_params: added <|repo_name|> logit bias = -inf
common_init_from_params: added <|file_sep|> logit bias = -inf
common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)

system_info: n_threads = 64 (n_threads_batch = 64) / 64 | CPU : RISCV_V = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 2897.54 ms
perplexity: calculating perplexity over 584 chunks, n_ctx=512, batch_size=2048, n_seq=4
perplexity: 19.69 seconds per pass - ETA 47.92 minutes
[1]151936.0000,[2]151936.0000,[3]151936.0000,[4]151936.0000,[5]151936.0000,[6]151936.0000,[7]151936.0000,[8]151936.0000,[9]151936.0000,[10]151936.0000,[11]151936.0000,[12]151936.0000,[13]151936.0000,[14]151936.0000,[15]151936.0000,[16]151936.0000,[17]151936.0000,[18]151936.0000,[19]151936.0000,[20]151936.0000,[21]151936.0000,[22]151936.0000,[23]151936.0000,[24]151936.0000,[25]151936.0000,[26]151936.0000,[27]151936.0000,[28]151936.0000,[29]151936.0000,[30]151936.0000,[31]151936.0000,[32]151936.0000,[33]151936.0000,[34]151936.0000,[35]151936.0000,[36]151936.0000,[37]151936.0000,[38]151936.0000,[39]151936.0000,[40]151936.0000,[41]151936.0000,[42]151936.0000,[43]151936.0000,[44]151936.0000,[45]151936.0000,[46]151936.0000,[47]151936.0000,[48]151936.0000,[49]151936.0000,[50]151936.0000,[51]151936.0000,[52]151936.0000,[53]151936.0000,[54]151936.0000,[55]151936.0000,[56]151936.0000,[57]151936.0000,[58]151936.0000,[59]151936.0000,[60]151936.0000,[61]151936.0000,[62]151936.0000,[63]151936.0000,[64]151936.0000,[65]151936.0000,[66]151936.0000,[67]151936.0000,[68]151936.0000,[69]151936.0000,[70]151936.0000,[71]151936.0000,[72]151936.0000,[73]151936.0000,[74]151936.0000,[75]151936.0000,[76]151936.0000,[77]151936.0000,[78]151936.0000,[79]151936.0000,[80]151936.0000,[81]151936.0000,[82]151936.0000,[83]151936.0000,[84]151936.0000,[85]151936.0000,[86]151936.0000,[87]151936.0000,[88]151936.0000,[89]151936.0000,[90]151936.0000,[91]151936.0000,[92]151936.0000,[93]151936.0000,[94]151936.0000,[95]151936.0000,[96]151936.0000,[97]151936.0000,[98]151936.0000,[99]151936.0000,[100]151936.0000,[101]151936.0000,[102]151936.0000,[103]151936.0000,[104]151936.0000,[105]151936.0000,[106]151936.0000,[107]151936.0000,[108]151936.0000,[109]151936.0000,[110]151936.0000,[111]151936.0000,[112]151936.0000,[113]151936.0000,[114]151936.0000,[115]151936.0000,[116]151936.0000,[117]151936.0000,[118]151936.0000,[119]151936.0000,[120]151936.0000,[121]151936.0000,[122]151936.0000,[123]151936.0000,[124]151936.0000,[125]151936.0000,[126]151936.0000,[127]151936.0000,[128]151936.0000,[129]151936.0000,[130]151936.0000,[131]151936.0000,[132]151936.0000,[133]151936.0000,[134]151936.0000,[135]151936.0000,[136]151936.0000,[137]151936.0000,[138]151936.0000,[139]151936.0000,[140]151936.0000,[141]151936.0000,[142]151936.0000,[143]151936.0000,[144]151936.0000,[145]151936.0000,[146]151936.0000,[147]151936.0000,[148]151936.0000,[149]151936.0000,[150]151936.0000,[151]151936.0000,[152]151936.0000,[153]151936.0000,[154]151936.0000,[155]151936.0000,[156]151936.0000,[157]151936.0000,[158]151936.0000,[159]151936.0000,[160]151936.0000,[161]151936.0000,[162]151936.0000,[163]151936.0000,[164]151936.0000,[165]151936.0000,[166]151936.0000,[167]151936.0000,[168]151936.0000,[169]151936.0000,[170]151936.0000,[171]151936.0000,[172]151936.0000,[173]151936.0000,[174]151936.0000,[175]151936.0000,[176]151936.0000,[177]151936.0000,[178]151936.0000,[179]151936.0000,[180]151936.0000,[181]151936.0000,[182]151936.0000,[183]151936.0000,[184]151936.0000,[185]151936.0000,[186]151936.0000,[187]151936.0000,[188]151936.0000,[189]151936.0000,[190]151936.0000,[191]151936.0000,[192]151936.0000,[193]151936.0000,[194]151936.0000,[195]151936.0000,[196]151936.0000,[197]151936.0000,[198]151936.0000,[199]151936.0000,[200]151936.0000,[201]151936.0000,[202]151936.0000,[203]151936.0000,[204]151936.0000,[205]151936.0000,[206]151936.0000,[207]151936.0000,[208]151936.0000,[209]151936.0000,[210]151936.0000,[211]151936.0000,[212]151936.0000,[213]151936.0000,[214]151936.0000,[215]151936.0000,[216]151936.0000,[217]151936.0000,[218]151936.0000,[219]151936.0000,[220]151936.0000,[221]151936.0000,[222]151936.0000,[223]151936.0000,[224]151936.0000,[225]151936.0000,[226]151936.0000,[227]151936.0000,[228]151936.0000,[229]151936.0000,[230]151936.0000,[231]151936.0000,[232]151936.0000,[233]151936.0000,[234]151936.0000,[235]151936.0000,[236]151936.0000,[237]151936.0000,[238]151936.0000,[239]151936.0000,[240]151936.0000,[241]151936.0000,[242]151936.0000,[243]151936.0000,[244]151936.0000,[245]151936.0000,[246]151936.0000,[247]151936.0000,[248]151936.0000,[249]151936.0000,[250]151936.0000,[251]151936.0000,[252]151936.0000,[253]151936.0000,[254]151936.0000,[255]151936.0000,[256]151936.0000,[257]151936.0000,[258]151936.0000,[259]151936.0000,[260]151936.0000,[261]151936.0000,[262]151936.0000,[263]151936.0000,[264]151936.0000,[265]151936.0000,[266]151936.0000,[267]151936.0000,[268]151936.0000,[269]151936.0000,[270]151936.0000,[271]151936.0000,[272]151936.0000,[273]151936.0000,[274]151936.0000,[275]151936.0000,[276]151936.0000,[277]151936.0000,[278]151936.0000,[279]151936.0000,[280]151936.0000,[281]151936.0000,[282]151936.0000,[283]151936.0000,[284]151936.0000,[285]151936.0000,[286]151936.0000,[287]151936.0000,[288]151936.0000,[289]151936.0000,[290]151936.0000,[291]151936.0000,[292]151936.0000,[293]151936.0000,[294]151936.0000,[295]151936.0000,[296]151936.0000,[297]151936.0000,[298]151936.0000,[299]151936.0000,[300]151936.0000,[301]151936.0000,[302]151936.0000,[303]151936.0000,[304]151936.0000,[305]151936.0000,[306]151936.0000,[307]151936.0000,[308]151936.0000,[309]151936.0000,[310]151936.0000,[311]151936.0000,[312]151936.0000,[313]151936.0000,[314]151936.0000,[315]151936.0000,[316]151936.0000,[317]151936.0000,[318]151936.0000,[319]151936.0000,[320]151936.0000,[321]151936.0000,[322]151936.0000,[323]151936.0000,[324]151936.0000,[325]151936.0000,[326]151936.0000,[327]151936.0000,[328]151936.0000,[329]151936.0000,[330]151936.0000,[331]151936.0000,[332]151936.0000,[333]151936.0000,[334]151936.0000,[335]151936.0000,[336]151936.0000,[337]151936.0000,[338]151936.0000,[339]151936.0000,[340]151936.0000,[341]151936.0000,[342]151936.0000,[343]151936.0000,[344]151936.0000,[345]151936.0000,[346]151936.0000,[347]151936.0000,[348]151936.0000,[349]151936.0000,[350]151936.0000,[351]151936.0000,[352]151936.0000,[353]151936.0000,[354]151936.0000,[355]151936.0000,[356]151936.0000,[357]151936.0000,[358]151936.0000,[359]151936.0000,[360]151936.0000,[361]151936.0000,[362]151936.0000,[363]151936.0000,[364]151936.0000,[365]151936.0000,[366]151936.0000,[367]151936.0000,[368]151936.0000,[369]151936.0000,[370]151936.0000,[371]151936.0000,[372]151936.0000,[373]151936.0000,[374]151936.0000,[375]151936.0000,[376]151936.0000,[377]151936.0000,[378]151936.0000,[379]151936.0000,[380]151936.0000,[381]151936.0000,[382]151936.0000,[383]151936.0000,[384]151936.0000,[385]151936.0000,[386]151936.0000,[387]151936.0000,[388]151936.0000,[389]151936.0000,[390]151936.0000,[391]151936.0000,[392]151936.0000,[393]151936.0000,[394]151936.0000,[395]151936.0000,[396]151936.0000,[397]151936.0000,[398]151936.0000,[399]151936.0000,[400]151936.0000,[401]151936.0000,[402]151936.0000,[403]151936.0000,[404]151936.0000,[405]151936.0000,[406]151936.0000,[407]151936.0000,[408]151936.0000,[409]151936.0000,[410]151936.0000,[411]151936.0000,[412]151936.0000,[413]151936.0000,[414]151936.0000,[415]151936.0000,[416]151936.0000,[417]151936.0000,[418]151936.0000,[419]151936.0000,[420]151936.0000,[421]151936.0000,[422]151936.0000,[423]151936.0000,[424]151936.0000,[425]151936.0000,[426]151936.0000,[427]151936.0000,[428]151936.0000,[429]151936.0000,[430]151936.0000,[431]151936.0000,[432]151936.0000,[433]151936.0000,[434]151936.0000,[435]151936.0000,[436]151936.0000,[437]151936.0000,[438]151936.0000,[439]151936.0000,[440]151936.0000,[441]151936.0000,[442]151936.0000,[443]151936.0000,[444]151936.0000,[445]151936.0000,[446]151936.0000,[447]151936.0000,[448]151936.0000,[449]151936.0000,[450]151936.0000,[451]151936.0000,[452]151936.0000,[453]151936.0000,[454]151936.0000,[455]151936.0000,[456]151936.0000,[457]151936.0000,[458]151936.0000,[459]151936.0000,[460]151936.0000,[461]151936.0000,[462]151936.0000,[463]151936.0000,[464]151936.0000,[465]151936.0000,[466]151936.0000,[467]151936.0000,[468]151936.0000,[469]151936.0000,[470]151936.0000,[471]151936.0000,[472]151936.0000,[473]151936.0000,[474]151936.0000,[475]151936.0000,[476]151936.0000,[477]151936.0000,[478]151936.0000,[479]151936.0000,[480]151936.0000,[481]151936.0000,[482]151936.0000,[483]151936.0000,[484]151936.0000,[485]151936.0000,[486]151936.0000,[487]151936.0000,[488]151936.0000,[489]151936.0000,[490]151936.0000,[491]151936.0000,[492]151936.0000,[493]151936.0000,[494]151936.0000,[495]151936.0000,[496]151936.0000,[497]151936.0000,[498]151936.0000,[499]151936.0000,[500]151936.0000,[501]151936.0000,[502]151936.0000,[503]151936.0000,[504]151936.0000,[505]151936.0000,[506]151936.0000,[507]151936.0000,[508]151936.0000,[509]151936.0000,[510]151936.0000,[511]151936.0000,[512]151936.0000,[513]151936.0000,[514]151936.0000,[515]151936.0000,[516]151936.0000,[517]151936.0000,[518]151936.0000,[519]151936.0000,[520]151936.0000,[521]151936.0000,[522]151936.0000,[523]151936.0000,[524]151936.0000,[525]151936.0000,[526]151936.0000,[527]151936.0000,[528]151936.0000,[529]151936.0000,[530]151936.0000,[531]151936.0000,[532]151936.0000,[533]151936.0000,[534]151936.0000,[535]151936.0000,[536]151936.0000,[537]151936.0000,[538]151936.0000,[539]151936.0000,[540]151936.0000,[541]151936.0000,[542]151936.0000,[543]151936.0000,[544]151936.0000,[545]151936.0000,[546]151936.0000,[547]151936.0000,[548]151936.0000,[549]151936.0000,[550]151936.0000,[551]151936.0000,[552]151936.0000,[553]151936.0000,[554]151936.0000,[555]151936.0000,[556]151936.0000,[557]151936.0000,[558]151936.0000,[559]151936.0000,[560]151936.0000,[561]151936.0000,[562]151936.0000,[563]151936.0000,[564]151936.0000,[565]151936.0000,[566]151936.0000,[567]151936.0000,[568]151936.0000,[569]151936.0000,[570]151936.0000,[571]151936.0000,[572]151936.0000,[573]151936.0000,[574]151936.0000,[575]151936.0000,[576]151936.0000,[577]151936.0000,[578]151936.0000,[579]151936.0000,[580]151936.0000,[581]151936.0000,[582]151936.0000,[583]151936.0000,[584]151936.0000,
Unexpected negative standard deviation of log(prob)

llama_perf_context_print:        load time =    1729.84 ms
llama_perf_context_print: prompt eval time = 2691781.74 ms / 299008 tokens (    9.00 ms per token,   111.08 tokens per second)
llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_perf_context_print:       total time = 2750614.12 ms / 299009 tokens
llama_perf_context_print:    graphs reused =          0
状态: 完成
-------------------------------------------
执行时间: 2025-12-20 23:26:49
测试配置: [变体: vw] [模型: Qwen3-1.7B-Q4_0.gguf.aria2] [线程: 64] [第 1 次运行]
命令: ./llama-perplexity -m /root/wmw/unsloth_riscv/Qwen3-1.7B-GGUF/Qwen3-1.7B-Q4_0.gguf.aria2 -t 64 -f /root/wmw/hzeng_vw/riscv_kernels/e2e/rvv10/../wiki.test.raw
build: 0 (unknown) with riscv64-unknown-linux-gnu-gcc () 15.1.0 for riscv64-unknown-linux-gnu
gguf_init_from_file_impl: invalid magic characters: '????', expected 'GGUF'
llama_model_load: error loading model: llama_model_loader: failed to load model from /root/wmw/unsloth_riscv/Qwen3-1.7B-GGUF/Qwen3-1.7B-Q4_0.gguf.aria2
llama_model_load_from_file_impl: failed to load model
common_init_from_params: failed to load model '/root/wmw/unsloth_riscv/Qwen3-1.7B-GGUF/Qwen3-1.7B-Q4_0.gguf.aria2'
main: unable to load model
状态: 失败 (请查看日志详情)
-------------------------------------------
执行时间: 2025-12-20 23:26:49
测试配置: [变体: vw] [模型: Qwen3-1.7B-Q8_0.gguf] [线程: 64] [第 1 次运行]
命令: ./llama-perplexity -m /root/wmw/unsloth_riscv/Qwen3-1.7B-GGUF/Qwen3-1.7B-Q8_0.gguf -t 64 -f /root/wmw/hzeng_vw/riscv_kernels/e2e/rvv10/../wiki.test.raw
build: 0 (unknown) with riscv64-unknown-linux-gnu-gcc () 15.1.0 for riscv64-unknown-linux-gnu
llama_model_loader: loaded meta data with 32 key-value pairs and 310 tensors from /root/wmw/unsloth_riscv/Qwen3-1.7B-GGUF/Qwen3-1.7B-Q8_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3-1.7B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3-1.7B
llama_model_loader: - kv   4:                       general.quantized_by str              = Unsloth
llama_model_loader: - kv   5:                         general.size_label str              = 1.7B
llama_model_loader: - kv   6:                           general.repo_url str              = https://huggingface.co/unsloth
llama_model_loader: - kv   7:                          qwen3.block_count u32              = 28
llama_model_loader: - kv   8:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   9:                     qwen3.embedding_length u32              = 2048
llama_model_loader: - kv  10:                  qwen3.feed_forward_length u32              = 6144
llama_model_loader: - kv  11:                 qwen3.attention.head_count u32              = 16
llama_model_loader: - kv  12:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  13:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  14:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  15:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  16:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  22:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 151654
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 7
llama_model_loader: - kv  28:                      quantize.imatrix.file str              = Qwen3-1.7B-GGUF/imatrix_unsloth.dat
llama_model_loader: - kv  29:                   quantize.imatrix.dataset str              = unsloth_calibration_Qwen3-1.7B.txt
llama_model_loader: - kv  30:             quantize.imatrix.entries_count i32              = 196
llama_model_loader: - kv  31:              quantize.imatrix.chunks_count i32              = 685
llama_model_loader: - type  f32:  113 tensors
llama_model_loader: - type q8_0:  197 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q8_0
print_info: file size   = 1.70 GiB (8.50 BPW) 
load: printing all EOG tokens:
load:   - 151643 ('<|endoftext|>')
load:   - 151645 ('<|im_end|>')
load:   - 151662 ('<|fim_pad|>')
load:   - 151663 ('<|repo_name|>')
load:   - 151664 ('<|file_sep|>')
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 2048
print_info: n_layer          = 28
print_info: n_head           = 16
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 2
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 6144
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: model type       = 1.7B
print_info: model params     = 1.72 B
print_info: general.name     = Qwen3-1.7B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 11 ','
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151654 '<|vision_pad|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors:   CPU_Mapped model buffer size =  1743.77 MiB
...................................................................................
llama_context: constructing llama_context
llama_context: n_seq_max     = 4
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 512
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (512) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     2.32 MiB
llama_kv_cache_unified:        CPU KV buffer size =   224.00 MiB
llama_kv_cache_unified: size =  224.00 MiB (   512 cells,  28 layers,  4/4 seqs), K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:        CPU compute buffer size =   300.75 MiB
llama_context: graph nodes  = 1098
llama_context: graph splits = 1
common_init_from_params: added <|endoftext|> logit bias = -inf
common_init_from_params: added <|im_end|> logit bias = -inf
common_init_from_params: added <|fim_pad|> logit bias = -inf
common_init_from_params: added <|repo_name|> logit bias = -inf
common_init_from_params: added <|file_sep|> logit bias = -inf
common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)

system_info: n_threads = 64 (n_threads_batch = 64) / 64 | CPU : RISCV_V = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 2971.39 ms
perplexity: calculating perplexity over 584 chunks, n_ctx=512, batch_size=2048, n_seq=4
perplexity: 18.32 seconds per pass - ETA 44.58 minutes
[1]11.8368,[2]15.3671,[3]15.1026,[4]15.6064,[5]15.5295,[6]15.6455,[7]16.0590,[8]16.7701,[9]17.8751,[10]18.4006,[11]18.5094,[12]18.5864,[13]19.2209,[14]18.8204,[15]18.7518,[16]19.1184,[17]18.0270,[18]18.1704,[19]17.9949,[20]17.9972,[21]17.7160,[22]17.7518,[23]17.0802,[24]16.1796,[25]15.7896,[26]15.3564,[27]14.8967,[28]14.7395,[29]14.8021,[30]14.7385,[31]14.5429,[32]14.5270,[33]14.2741,[34]14.5500,[35]14.6424,[36]14.8855,[37]15.1174,[38]15.2105,[39]15.1698,[40]15.2735,[41]15.2375,[42]15.1458,[43]15.2709,[44]15.3182,[45]15.3048,[46]15.2543,[47]15.5633,[48]15.6840,[49]15.5986,[50]15.6926,[51]15.7639,[52]15.8063,[53]15.8919,[54]15.9334,[55]15.9355,[56]16.0592,[57]15.9753,[58]16.0306,[59]16.1059,[60]16.1856,[61]16.2227,[62]16.2715,[63]16.3512,[64]16.4756,[65]16.6622,[66]16.7926,[67]16.9340,[68]16.8439,[69]16.8229,[70]16.7900,[71]16.8086,[72]16.9072,[73]16.9443,[74]16.9440,[75]16.8276,[76]16.7918,[77]16.8364,[78]16.8332,[79]16.7018,[80]16.7089,[81]16.6394,[82]16.7158,[83]16.6776,[84]16.6368,[85]16.6922,[86]16.8539,[87]16.9562,[88]16.9272,[89]16.9070,[90]16.8701,[91]16.9367,[92]16.8936,[93]17.0113,[94]17.0516,[95]17.0003,[96]16.9826,[97]16.9455,[98]16.9494,[99]16.9156,[100]17.0421,[101]17.1016,[102]17.0626,[103]17.1015,[104]17.0263,[105]17.0561,[106]17.0244,[107]17.0623,[108]17.1290,[109]17.1835,[110]17.2669,[111]17.4643,[112]17.4914,[113]17.3568,[114]17.4116,[115]17.4511,[116]17.3862,[117]17.3638,[118]17.3251,[119]17.2102,[120]17.2600,[121]17.2159,[122]17.2344,[123]17.2038,[124]17.1129,[125]17.0932,[126]17.0898,[127]17.0542,[128]17.0149,[129]16.9738,[130]16.9368,[131]16.8374,[132]16.7515,[133]16.7018,[134]16.7115,[135]16.6879,[136]16.6482,[137]16.5692,[138]16.5032,[139]16.5162,[140]16.4953,[141]16.4802,[142]16.4919,[143]16.5171,[144]16.5893,[145]16.5497,[146]16.4644,[147]16.3794,[148]16.3054,[149]16.2900,[150]16.2076,[151]16.1933,[152]16.1793,[153]16.1698,[154]16.0923,[155]16.1392,[156]16.0754,[157]16.0347,[158]15.9843,[159]15.9439,[160]15.8795,[161]15.8476,[162]15.8468,[163]15.8511,[164]15.8619,[165]15.8342,[166]15.8175,[167]15.7768,[168]15.8138,[169]15.7933,[170]15.8677,[171]15.9350,[172]16.0058,[173]16.0839,[174]16.1148,[175]16.1898,[176]16.2538,[177]16.3445,[178]16.4129,[179]16.4691,[180]16.4737,[181]16.5337,[182]16.5867,[183]16.6268,[184]16.6881,[185]16.7320,[186]16.7540,[187]16.7625,[188]16.7673,[189]16.7875,[190]16.8322,[191]16.8361,[192]16.8630,[193]16.8698,[194]16.9048,[195]16.9246,[196]16.9364,[197]16.9440,[198]16.8909,[199]16.8703,[200]16.8541,[201]16.8938,[202]16.9204,[203]16.9536,[204]16.9732,[205]16.9741,[206]16.9658,[207]17.0153,[208]16.9785,[209]16.9787,[210]16.9748,[211]16.9704,[212]16.9689,[213]16.9855,[214]16.9532,[215]16.9139,[216]16.9079,[217]16.9023,[218]16.8957,[219]16.8348,[220]16.8090,[221]16.7694,[222]16.7326,[223]16.7227,[224]16.7357,[225]16.6933,[226]16.6836,[227]16.6572,[228]16.5957,[229]16.5373,[230]16.4928,[231]16.4519,[232]16.4130,[233]16.4107,[234]16.4245,[235]16.4114,[236]16.3793,[237]16.3425,[238]16.2887,[239]16.2551,[240]16.2572,[241]16.2411,[242]16.2495,[243]16.2700,[244]16.2942,[245]16.3006,[246]16.3528,[247]16.3456,[248]16.3597,[249]16.3793,[250]16.3770,[251]16.4058,[252]16.4175,[253]16.4598,[254]16.4824,[255]16.4774,[256]16.5137,[257]16.5153,[258]16.4903,[259]16.4449,[260]16.4159,[261]16.3754,[262]16.3588,[263]16.3674,[264]16.3724,[265]16.4024,[266]16.4330,[267]16.4392,[268]16.4311,[269]16.4394,[270]16.4638,[271]16.4304,[272]16.4423,[273]16.4409,[274]16.4448,[275]16.4478,[276]16.4168,[277]16.4042,[278]16.4145,[279]16.4120,[280]16.3985,[281]16.4026,[282]16.4465,[283]16.3916,[284]16.3280,[285]16.3407,[286]16.2926,[287]16.2412,[288]16.2363,[289]16.2431,[290]16.2955,[291]16.2989,[292]16.2924,[293]16.2939,[294]16.3279,[295]16.3826,[296]16.4512,[297]16.4869,[298]16.4790,[299]16.4504,[300]16.4497,[301]16.4434,[302]16.4490,[303]16.4359,[304]16.4589,[305]16.4485,[306]16.4371,[307]16.4506,[308]16.4392,[309]16.4319,[310]16.4529,[311]16.4532,[312]16.4315,[313]16.4221,[314]16.4296,[315]16.4004,[316]16.4321,[317]16.4799,[318]16.4816,[319]16.4612,[320]16.4656,[321]16.4341,[322]16.4569,[323]16.4834,[324]16.5108,[325]16.5338,[326]16.5467,[327]16.5226,[328]16.5218,[329]16.4792,[330]16.4723,[331]16.4484,[332]16.4469,[333]16.4452,[334]16.4245,[335]16.3903,[336]16.3857,[337]16.3966,[338]16.4098,[339]16.4048,[340]16.3950,[341]16.3703,[342]16.3682,[343]16.3558,[344]16.3683,[345]16.3698,[346]16.3675,[347]16.3436,[348]16.3570,[349]16.3525,[350]16.3495,[351]16.3624,[352]16.3658,[353]16.3607,[354]16.3311,[355]16.3554,[356]16.3722,[357]16.3845,[358]16.3658,[359]16.3612,[360]16.3750,[361]16.3847,[362]16.3904,[363]16.3825,[364]16.4568,[365]16.4864,[366]16.5397,[367]16.5771,[368]16.6277,[369]16.6599,[370]16.6843,[371]16.7191,[372]16.7544,[373]16.7716,[374]16.7944,[375]16.8334,[376]16.8664,[377]16.8815,[378]16.9022,[379]16.9216,[380]16.9463,[381]16.9697,[382]16.9906,[383]17.0059,[384]17.0374,[385]17.0845,[386]17.1297,[387]17.1336,[388]17.1227,[389]17.1488,[390]17.1894,[391]17.2187,[392]17.2090,[393]17.2023,[394]17.1887,[395]17.2011,[396]17.2118,[397]17.2127,[398]17.2235,[399]17.2240,[400]17.2564,[401]17.2656,[402]17.2722,[403]17.2495,[404]17.2386,[405]17.2211,[406]17.2167,[407]17.2211,[408]17.2311,[409]17.2289,[410]17.2316,[411]17.2557,[412]17.2677,[413]17.2702,[414]17.2619,[415]17.2448,[416]17.2378,[417]17.2447,[418]17.2499,[419]17.2525,[420]17.2403,[421]17.2409,[422]17.2111,[423]17.2221,[424]17.2264,[425]17.2314,[426]17.2425,[427]17.2709,[428]17.3046,[429]17.3141,[430]17.2979,[431]17.2813,[432]17.2933,[433]17.2893,[434]17.2738,[435]17.2901,[436]17.2520,[437]17.2454,[438]17.2430,[439]17.2251,[440]17.2426,[441]17.2489,[442]17.2340,[443]17.2233,[444]17.2326,[445]17.2058,[446]17.2282,[447]17.2289,[448]17.2109,[449]17.1986,[450]17.2007,[451]17.1903,[452]17.1804,[453]17.1638,[454]17.1477,[455]17.1530,[456]17.1449,[457]17.1573,[458]17.1813,[459]17.1825,[460]17.1866,[461]17.1855,[462]17.1895,[463]17.2068,[464]17.2074,[465]17.2130,[466]17.2159,[467]17.2283,[468]17.2452,[469]17.2551,[470]17.2624,[471]17.2464,[472]17.2609,[473]17.2331,[474]17.2314,[475]17.2382,[476]17.2560,[477]17.2426,[478]17.2186,[479]17.2243,[480]17.2429,[481]17.2633,[482]17.2335,[483]17.2519,[484]17.2684,[485]17.2710,[486]17.2640,[487]17.2713,[488]17.2435,[489]17.2213,[490]17.2176,[491]17.1994,[492]17.1918,[493]17.1603,[494]17.1475,[495]17.1217,[496]17.1199,[497]17.1397,[498]17.1476,[499]17.1301,[500]17.1270,[501]17.1189,[502]17.1234,[503]17.1516,[504]17.1677,[505]17.1627,[506]17.1616,[507]17.1437,[508]17.1468,[509]17.1261,[510]17.1320,[511]17.1434,[512]17.1399,[513]17.1503,[514]17.1493,[515]17.1555,[516]17.1534,[517]17.1537,[518]17.1513,[519]17.1444,[520]17.1351,[521]17.1342,[522]17.1169,[523]17.1179,[524]17.1120,[525]17.1188,[526]17.1242,[527]17.1192,[528]17.1168,[529]17.1124,[530]17.0987,[531]17.0969,[532]17.0878,[533]17.0930,[534]17.0884,[535]17.0958,[536]17.0944,[537]17.1128,[538]17.1251,[539]17.1283,[540]17.1575,[541]17.1632,[542]17.1385,[543]17.1431,[544]17.1640,[545]17.1569,[546]17.1484,[547]17.1322,[548]17.1169,[549]17.1201,[550]17.0883,[551]17.0623,[552]17.0370,[553]16.9641,[554]16.9537,[555]16.9675,[556]16.9678,[557]16.9583,[558]16.9453,[559]16.9493,[560]16.9608,[561]16.9634,[562]16.9891,[563]17.0049,[564]16.9961,[565]17.0109,[566]17.0135,[567]16.9955,[568]16.9915,[569]16.9747,[570]16.9778,[571]16.9790,[572]17.0001,[573]17.0093,[574]17.0129,[575]17.0141,[576]17.0358,[577]17.0345,[578]17.0474,[579]17.0669,[580]17.0934,[581]17.0984,[582]17.1211,[583]17.1007,[584]17.1064,
Final estimate: PPL = 17.1064 +/- 0.15987

llama_perf_context_print:        load time =    1858.80 ms
llama_perf_context_print: prompt eval time = 2473513.95 ms / 299008 tokens (    8.27 ms per token,   120.88 tokens per second)
llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_perf_context_print:       total time = 2533764.62 ms / 299009 tokens
llama_perf_context_print:    graphs reused =          0
状态: 完成
正在处理变体目录: 4
-------------------------------------------
执行时间: 2025-12-21 00:09:05
测试配置: [变体: 4] [模型: Qwen3-0.6B-Q4_0.gguf] [线程: 64] [第 1 次运行]
命令: ./llama-perplexity -m /root/wmw/unsloth_riscv/Qwen3-0.6B-GGUF/Qwen3-0.6B-Q4_0.gguf -t 64 -f /root/wmw/hzeng_vw/riscv_kernels/e2e/rvv10/../wiki.test.raw
build: 0 (unknown) with riscv64-unknown-linux-gnu-gcc () 15.1.0 for riscv64-unknown-linux-gnu
llama_model_loader: loaded meta data with 32 key-value pairs and 310 tensors from /root/wmw/unsloth_riscv/Qwen3-0.6B-GGUF/Qwen3-0.6B-Q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3-0.6B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3-0.6B
llama_model_loader: - kv   4:                       general.quantized_by str              = Unsloth
llama_model_loader: - kv   5:                         general.size_label str              = 0.6B
llama_model_loader: - kv   6:                           general.repo_url str              = https://huggingface.co/unsloth
llama_model_loader: - kv   7:                          qwen3.block_count u32              = 28
llama_model_loader: - kv   8:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   9:                     qwen3.embedding_length u32              = 1024
llama_model_loader: - kv  10:                  qwen3.feed_forward_length u32              = 3072
llama_model_loader: - kv  11:                 qwen3.attention.head_count u32              = 16
llama_model_loader: - kv  12:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  13:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  14:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  15:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  16:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  22:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 151654
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 2
llama_model_loader: - kv  28:                      quantize.imatrix.file str              = Qwen3-0.6B-GGUF/imatrix_unsloth.dat
llama_model_loader: - kv  29:                   quantize.imatrix.dataset str              = unsloth_calibration_Qwen3-0.6B.txt
llama_model_loader: - kv  30:             quantize.imatrix.entries_count u32              = 196
llama_model_loader: - kv  31:              quantize.imatrix.chunks_count u32              = 688
llama_model_loader: - type  f32:  113 tensors
llama_model_loader: - type q4_0:  193 tensors
llama_model_loader: - type q4_1:    3 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 358.78 MiB (5.05 BPW) 
load: printing all EOG tokens:
load:   - 151643 ('<|endoftext|>')
load:   - 151645 ('<|im_end|>')
load:   - 151662 ('<|fim_pad|>')
load:   - 151663 ('<|repo_name|>')
load:   - 151664 ('<|file_sep|>')
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 1024
print_info: n_layer          = 28
print_info: n_head           = 16
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 2
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 3072
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: model type       = 0.6B
print_info: model params     = 596.05 M
print_info: general.name     = Qwen3-0.6B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 11 ','
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151654 '<|vision_pad|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors:   CPU_Mapped model buffer size =   358.78 MiB
....................................................................
llama_context: constructing llama_context
llama_context: n_seq_max     = 4
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 512
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (512) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     2.32 MiB
llama_kv_cache_unified:        CPU KV buffer size =   224.00 MiB
llama_kv_cache_unified: size =  224.00 MiB (   512 cells,  28 layers,  4/4 seqs), K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:        CPU compute buffer size =   298.75 MiB
llama_context: graph nodes  = 1098
llama_context: graph splits = 1
common_init_from_params: added <|endoftext|> logit bias = -inf
common_init_from_params: added <|im_end|> logit bias = -inf
common_init_from_params: added <|fim_pad|> logit bias = -inf
common_init_from_params: added <|repo_name|> logit bias = -inf
common_init_from_params: added <|file_sep|> logit bias = -inf
common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)

system_info: n_threads = 64 (n_threads_batch = 64) / 64 | CPU : RISCV_V = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 2804.98 ms
perplexity: calculating perplexity over 584 chunks, n_ctx=512, batch_size=2048, n_seq=4
perplexity: 10.36 seconds per pass - ETA 25.22 minutes
[1]151936.0000,[2]151936.0000,[3]151936.0000,[4]151936.0000,[5]151936.0000,[6]151936.0000,[7]151936.0000,[8]151936.0000,[9]151936.0000,[10]151936.0000,[11]151936.0000,[12]151936.0000,[13]151936.0000,[14]151936.0000,[15]151936.0000,[16]151936.0000,[17]151936.0000,[18]151936.0000,[19]151936.0000,[20]151936.0000,[21]151936.0000,[22]151936.0000,[23]151936.0000,[24]151936.0000,[25]151936.0000,[26]151936.0000,[27]151936.0000,[28]151936.0000,[29]151936.0000,[30]151936.0000,[31]151936.0000,[32]151936.0000,[33]151936.0000,[34]151936.0000,[35]151936.0000,[36]151936.0000,[37]151936.0000,[38]151936.0000,[39]151936.0000,[40]151936.0000,[41]151936.0000,[42]151936.0000,[43]151936.0000,[44]151936.0000,[45]151936.0000,[46]151936.0000,[47]151936.0000,[48]151936.0000,[49]151936.0000,[50]151936.0000,[51]151936.0000,[52]151936.0000,[53]151936.0000,[54]151936.0000,[55]151936.0000,[56]151936.0000,[57]151936.0000,[58]151936.0000,[59]151936.0000,[60]151936.0000,[61]151936.0000,[62]151936.0000,[63]151936.0000,[64]151936.0000,[65]151936.0000,[66]151936.0000,[67]151936.0000,[68]151936.0000,[69]151936.0000,[70]151936.0000,[71]151936.0000,[72]151936.0000,[73]151936.0000,[74]151936.0000,[75]151936.0000,[76]151936.0000,[77]151936.0000,[78]151936.0000,[79]151936.0000,[80]151936.0000,[81]151936.0000,[82]151936.0000,[83]151936.0000,[84]151936.0000,[85]151936.0000,[86]151936.0000,[87]151936.0000,[88]151936.0000,[89]151936.0000,[90]151936.0000,[91]151936.0000,[92]151936.0000,[93]151936.0000,[94]151936.0000,[95]151936.0000,[96]151936.0000,[97]151936.0000,[98]151936.0000,[99]151936.0000,[100]151936.0000,[101]151936.0000,[102]151936.0000,[103]151936.0000,[104]151936.0000,[105]151936.0000,[106]151936.0000,[107]151936.0000,[108]151936.0000,[109]151936.0000,[110]151936.0000,[111]151936.0000,[112]151936.0000,[113]151936.0000,[114]151936.0000,[115]151936.0000,[116]151936.0000,[117]151936.0000,[118]151936.0000,[119]151936.0000,[120]151936.0000,[121]151936.0000,[122]151936.0000,[123]151936.0000,[124]151936.0000,[125]151936.0000,[126]151936.0000,[127]151936.0000,[128]151936.0000,[129]151936.0000,[130]151936.0000,[131]151936.0000,[132]151936.0000,[133]151936.0000,[134]151936.0000,[135]151936.0000,[136]151936.0000,[137]151936.0000,[138]151936.0000,[139]151936.0000,[140]151936.0000,[141]151936.0000,[142]151936.0000,[143]151936.0000,[144]151936.0000,[145]151936.0000,[146]151936.0000,[147]151936.0000,[148]151936.0000,[149]151936.0000,[150]151936.0000,[151]151936.0000,[152]151936.0000,[153]151936.0000,[154]151936.0000,[155]151936.0000,[156]151936.0000,[157]151936.0000,[158]151936.0000,[159]151936.0000,[160]151936.0000,[161]151936.0000,[162]151936.0000,[163]151936.0000,[164]151936.0000,[165]151936.0000,[166]151936.0000,[167]151936.0000,[168]151936.0000,[169]151936.0000,[170]151936.0000,[171]151936.0000,[172]151936.0000,[173]151936.0000,[174]151936.0000,[175]151936.0000,[176]151936.0000,[177]151936.0000,[178]151936.0000,[179]151936.0000,[180]151936.0000,[181]151936.0000,[182]151936.0000,[183]151936.0000,[184]151936.0000,[185]151936.0000,[186]151936.0000,[187]151936.0000,[188]151936.0000,[189]151936.0000,[190]151936.0000,[191]151936.0000,[192]151936.0000,[193]151936.0000,[194]151936.0000,[195]151936.0000,[196]151936.0000,[197]151936.0000,[198]151936.0000,[199]151936.0000,[200]151936.0000,[201]151936.0000,[202]151936.0000,[203]151936.0000,[204]151936.0000,[205]151936.0000,[206]151936.0000,[207]151936.0000,[208]151936.0000,[209]151936.0000,[210]151936.0000,[211]151936.0000,[212]151936.0000,[213]151936.0000,[214]151936.0000,[215]151936.0000,[216]151936.0000,[217]151936.0000,[218]151936.0000,[219]151936.0000,[220]151936.0000,[221]151936.0000,[222]151936.0000,[223]151936.0000,[224]151936.0000,[225]151936.0000,[226]151936.0000,[227]151936.0000,[228]151936.0000,[229]151936.0000,[230]151936.0000,[231]151936.0000,[232]151936.0000,[233]151936.0000,[234]151936.0000,[235]151936.0000,[236]151936.0000,[237]151936.0000,[238]151936.0000,[239]151936.0000,[240]151936.0000,[241]151936.0000,[242]151936.0000,[243]151936.0000,[244]151936.0000,[245]151936.0000,[246]151936.0000,[247]151936.0000,[248]151936.0000,[249]151936.0000,[250]151936.0000,[251]151936.0000,[252]151936.0000,[253]151936.0000,[254]151936.0000,[255]151936.0000,[256]151936.0000,[257]151936.0000,[258]151936.0000,[259]151936.0000,[260]151936.0000,[261]151936.0000,[262]151936.0000,[263]151936.0000,[264]151936.0000,[265]151936.0000,[266]151936.0000,[267]151936.0000,[268]151936.0000,[269]151936.0000,[270]151936.0000,[271]151936.0000,[272]151936.0000,[273]151936.0000,[274]151936.0000,[275]151936.0000,[276]151936.0000,[277]151936.0000,[278]151936.0000,[279]151936.0000,[280]151936.0000,[281]151936.0000,[282]151936.0000,[283]151936.0000,[284]151936.0000,[285]151936.0000,[286]151936.0000,[287]151936.0000,[288]151936.0000,[289]151936.0000,[290]151936.0000,[291]151936.0000,[292]151936.0000,[293]151936.0000,[294]151936.0000,[295]151936.0000,[296]151936.0000,[297]151936.0000,[298]151936.0000,[299]151936.0000,[300]151936.0000,[301]151936.0000,[302]151936.0000,[303]151936.0000,[304]151936.0000,[305]151936.0000,[306]151936.0000,[307]151936.0000,[308]151936.0000,[309]151936.0000,[310]151936.0000,[311]151936.0000,[312]151936.0000,[313]151936.0000,[314]151936.0000,[315]151936.0000,[316]151936.0000,[317]151936.0000,[318]151936.0000,[319]151936.0000,[320]151936.0000,[321]151936.0000,[322]151936.0000,[323]151936.0000,[324]151936.0000,[325]151936.0000,[326]151936.0000,[327]151936.0000,[328]151936.0000,[329]151936.0000,[330]151936.0000,[331]151936.0000,[332]151936.0000,[333]151936.0000,[334]151936.0000,[335]151936.0000,[336]151936.0000,[337]151936.0000,[338]151936.0000,[339]151936.0000,[340]151936.0000,[341]151936.0000,[342]151936.0000,[343]151936.0000,[344]151936.0000,[345]151936.0000,[346]151936.0000,[347]151936.0000,[348]151936.0000,[349]151936.0000,[350]151936.0000,[351]151936.0000,[352]151936.0000,[353]151936.0000,[354]151936.0000,[355]151936.0000,[356]151936.0000,[357]151936.0000,[358]151936.0000,[359]151936.0000,[360]151936.0000,[361]151936.0000,[362]151936.0000,[363]151936.0000,[364]151936.0000,[365]151936.0000,[366]151936.0000,[367]151936.0000,[368]151936.0000,[369]151936.0000,[370]151936.0000,[371]151936.0000,[372]151936.0000,[373]151936.0000,[374]151936.0000,[375]151936.0000,[376]151936.0000,[377]151936.0000,[378]151936.0000,[379]151936.0000,[380]151936.0000,[381]151936.0000,[382]151936.0000,[383]151936.0000,[384]151936.0000,[385]151936.0000,[386]151936.0000,[387]151936.0000,[388]151936.0000,[389]151936.0000,[390]151936.0000,[391]151936.0000,[392]151936.0000,[393]151936.0000,[394]151936.0000,[395]151936.0000,[396]151936.0000,[397]151936.0000,[398]151936.0000,[399]151936.0000,[400]151936.0000,[401]151936.0000,[402]151936.0000,[403]151936.0000,[404]151936.0000,[405]151936.0000,[406]151936.0000,[407]151936.0000,[408]151936.0000,[409]151936.0000,[410]151936.0000,[411]151936.0000,[412]151936.0000,[413]151936.0000,[414]151936.0000,[415]151936.0000,[416]151936.0000,[417]151936.0000,[418]151936.0000,[419]151936.0000,[420]151936.0000,[421]151936.0000,[422]151936.0000,[423]151936.0000,[424]151936.0000,[425]151936.0000,[426]151936.0000,[427]151936.0000,[428]151936.0000,[429]151936.0000,[430]151936.0000,[431]151936.0000,[432]151936.0000,[433]151936.0000,[434]151936.0000,[435]151936.0000,[436]151936.0000,[437]151936.0000,[438]151936.0000,[439]151936.0000,[440]151936.0000,[441]151936.0000,[442]151936.0000,[443]151936.0000,[444]151936.0000,[445]151936.0000,[446]151936.0000,[447]151936.0000,[448]151936.0000,[449]151936.0000,[450]151936.0000,[451]151936.0000,[452]151936.0000,[453]151936.0000,[454]151936.0000,[455]151936.0000,[456]151936.0000,[457]151936.0000,[458]151936.0000,[459]151936.0000,[460]151936.0000,[461]151936.0000,[462]151936.0000,[463]151936.0000,[464]151936.0000,[465]151936.0000,[466]151936.0000,[467]151936.0000,[468]151936.0000,[469]151936.0000,[470]151936.0000,[471]151936.0000,[472]151936.0000,[473]151936.0000,[474]151936.0000,[475]151936.0000,[476]151936.0000,[477]151936.0000,[478]151936.0000,[479]151936.0000,[480]151936.0000,[481]151936.0000,[482]151936.0000,[483]151936.0000,[484]151936.0000,[485]151936.0000,[486]151936.0000,[487]151936.0000,[488]151936.0000,[489]151936.0000,[490]151936.0000,[491]151936.0000,[492]151936.0000,[493]151936.0000,[494]151936.0000,[495]151936.0000,[496]151936.0000,[497]151936.0000,[498]151936.0000,[499]151936.0000,[500]151936.0000,[501]151936.0000,[502]151936.0000,[503]151936.0000,[504]151936.0000,[505]151936.0000,[506]151936.0000,[507]151936.0000,[508]151936.0000,[509]151936.0000,[510]151936.0000,[511]151936.0000,[512]151936.0000,[513]151936.0000,[514]151936.0000,[515]151936.0000,[516]151936.0000,[517]151936.0000,[518]151936.0000,[519]151936.0000,[520]151936.0000,[521]151936.0000,[522]151936.0000,[523]151936.0000,[524]151936.0000,[525]151936.0000,[526]151936.0000,[527]151936.0000,[528]151936.0000,[529]151936.0000,[530]151936.0000,[531]151936.0000,[532]151936.0000,[533]151936.0000,[534]151936.0000,[535]151936.0000,[536]151936.0000,[537]151936.0000,[538]151936.0000,[539]151936.0000,[540]151936.0000,[541]151936.0000,[542]151936.0000,[543]151936.0000,[544]151936.0000,[545]151936.0000,[546]151936.0000,[547]151936.0000,[548]151936.0000,[549]151936.0000,[550]151936.0000,[551]151936.0000,[552]151936.0000,[553]151936.0000,[554]151936.0000,[555]151936.0000,[556]151936.0000,[557]151936.0000,[558]151936.0000,[559]151936.0000,[560]151936.0000,[561]151936.0000,[562]151936.0000,[563]151936.0000,[564]151936.0000,[565]151936.0000,[566]151936.0000,[567]151936.0000,[568]151936.0000,[569]151936.0000,[570]151936.0000,[571]151936.0000,[572]151936.0000,[573]151936.0000,[574]151936.0000,[575]151936.0000,[576]151936.0000,[577]151936.0000,[578]151936.0000,[579]151936.0000,[580]151936.0000,[581]151936.0000,[582]151936.0000,[583]151936.0000,[584]151936.0000,
Unexpected negative standard deviation of log(prob)

llama_perf_context_print:        load time =    1555.24 ms
llama_perf_context_print: prompt eval time = 1299502.67 ms / 299008 tokens (    4.35 ms per token,   230.09 tokens per second)
llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_perf_context_print:       total time = 1359404.49 ms / 299009 tokens
llama_perf_context_print:    graphs reused =          0
状态: 完成
-------------------------------------------
执行时间: 2025-12-21 00:31:47
测试配置: [变体: 4] [模型: Qwen3-0.6B-Q8_0.gguf] [线程: 64] [第 1 次运行]
命令: ./llama-perplexity -m /root/wmw/unsloth_riscv/Qwen3-0.6B-GGUF/Qwen3-0.6B-Q8_0.gguf -t 64 -f /root/wmw/hzeng_vw/riscv_kernels/e2e/rvv10/../wiki.test.raw
build: 0 (unknown) with riscv64-unknown-linux-gnu-gcc () 15.1.0 for riscv64-unknown-linux-gnu
llama_model_loader: loaded meta data with 32 key-value pairs and 310 tensors from /root/wmw/unsloth_riscv/Qwen3-0.6B-GGUF/Qwen3-0.6B-Q8_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3-0.6B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3-0.6B
llama_model_loader: - kv   4:                       general.quantized_by str              = Unsloth
llama_model_loader: - kv   5:                         general.size_label str              = 0.6B
llama_model_loader: - kv   6:                           general.repo_url str              = https://huggingface.co/unsloth
llama_model_loader: - kv   7:                          qwen3.block_count u32              = 28
llama_model_loader: - kv   8:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   9:                     qwen3.embedding_length u32              = 1024
llama_model_loader: - kv  10:                  qwen3.feed_forward_length u32              = 3072
llama_model_loader: - kv  11:                 qwen3.attention.head_count u32              = 16
llama_model_loader: - kv  12:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  13:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  14:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  15:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  16:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  22:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 151654
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 7
llama_model_loader: - kv  28:                      quantize.imatrix.file str              = Qwen3-0.6B-GGUF/imatrix_unsloth.dat
llama_model_loader: - kv  29:                   quantize.imatrix.dataset str              = unsloth_calibration_Qwen3-0.6B.txt
llama_model_loader: - kv  30:             quantize.imatrix.entries_count u32              = 196
llama_model_loader: - kv  31:              quantize.imatrix.chunks_count u32              = 688
llama_model_loader: - type  f32:  113 tensors
llama_model_loader: - type q8_0:  197 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q8_0
print_info: file size   = 604.15 MiB (8.50 BPW) 
load: printing all EOG tokens:
load:   - 151643 ('<|endoftext|>')
load:   - 151645 ('<|im_end|>')
load:   - 151662 ('<|fim_pad|>')
load:   - 151663 ('<|repo_name|>')
load:   - 151664 ('<|file_sep|>')
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 1024
print_info: n_layer          = 28
print_info: n_head           = 16
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 2
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 3072
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: model type       = 0.6B
print_info: model params     = 596.05 M
print_info: general.name     = Qwen3-0.6B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 11 ','
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151654 '<|vision_pad|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors:   CPU_Mapped model buffer size =   604.15 MiB
...........................................................................
llama_context: constructing llama_context
llama_context: n_seq_max     = 4
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 512
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (512) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     2.32 MiB
llama_kv_cache_unified:        CPU KV buffer size =   224.00 MiB
llama_kv_cache_unified: size =  224.00 MiB (   512 cells,  28 layers,  4/4 seqs), K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:        CPU compute buffer size =   298.75 MiB
llama_context: graph nodes  = 1098
llama_context: graph splits = 1
common_init_from_params: added <|endoftext|> logit bias = -inf
common_init_from_params: added <|im_end|> logit bias = -inf
common_init_from_params: added <|fim_pad|> logit bias = -inf
common_init_from_params: added <|repo_name|> logit bias = -inf
common_init_from_params: added <|file_sep|> logit bias = -inf
common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)

system_info: n_threads = 64 (n_threads_batch = 64) / 64 | CPU : RISCV_V = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 2909.78 ms
perplexity: calculating perplexity over 584 chunks, n_ctx=512, batch_size=2048, n_seq=4
perplexity: 10.03 seconds per pass - ETA 24.40 minutes
[1]12.7218,[2]18.1444,[3]18.2077,[4]19.2079,[5]19.5436,[6]19.9973,[7]20.1821,[8]21.7043,[9]22.9801,[10]23.7626,[11]24.1758,[12]24.4121,[13]25.2263,[14]24.8251,[15]24.5261,[16]25.1405,[17]23.3032,[18]23.7863,[19]23.4752,[20]23.6572,[21]23.1021,[22]23.0789,[23]22.0705,[24]20.8533,[25]20.3940,[26]19.9129,[27]19.2291,[28]18.8129,[29]18.9986,[30]19.0134,[31]18.7715,[32]18.8456,[33]18.4917,[34]18.6750,[35]18.8333,[36]19.1501,[37]19.5518,[38]19.6925,[39]19.5036,[40]19.6187,[41]19.5964,[42]19.4557,[43]19.6451,[44]19.6970,[45]19.7818,[46]19.7389,[47]20.1608,[48]20.4007,[49]20.2608,[50]20.4624,[51]20.5095,[52]20.5432,[53]20.6179,[54]20.7694,[55]20.7347,[56]20.8617,[57]20.7854,[58]20.8561,[59]20.9944,[60]21.1158,[61]21.1048,[62]21.1688,[63]21.3300,[64]21.5021,[65]21.7539,[66]21.9431,[67]22.1562,[68]22.0724,[69]22.0862,[70]22.1172,[71]22.1389,[72]22.2928,[73]22.3431,[74]22.3471,[75]22.2139,[76]22.1214,[77]22.1398,[78]22.0902,[79]21.9197,[80]21.8839,[81]21.7541,[82]21.8507,[83]21.8049,[84]21.7212,[85]21.7294,[86]21.9304,[87]22.0946,[88]22.0394,[89]21.9675,[90]21.8996,[91]21.9968,[92]21.9257,[93]22.0600,[94]22.1725,[95]22.1062,[96]22.1388,[97]22.0943,[98]22.1091,[99]22.0636,[100]22.2165,[101]22.3066,[102]22.2732,[103]22.3379,[104]22.2427,[105]22.2789,[106]22.1780,[107]22.2088,[108]22.3313,[109]22.4093,[110]22.5291,[111]22.8008,[112]22.8452,[113]22.7011,[114]22.7976,[115]22.8462,[116]22.7774,[117]22.7633,[118]22.7138,[119]22.5813,[120]22.6302,[121]22.5927,[122]22.6472,[123]22.5972,[124]22.4815,[125]22.4383,[126]22.4229,[127]22.3923,[128]22.3188,[129]22.2914,[130]22.2080,[131]22.0739,[132]21.9860,[133]21.9197,[134]21.9301,[135]21.9186,[136]21.8864,[137]21.7876,[138]21.6774,[139]21.7004,[140]21.6411,[141]21.6423,[142]21.6336,[143]21.6832,[144]21.7350,[145]21.6811,[146]21.5371,[147]21.4179,[148]21.3209,[149]21.2576,[150]21.1238,[151]21.0814,[152]21.0752,[153]21.0400,[154]20.9374,[155]20.9789,[156]20.9191,[157]20.8730,[158]20.8010,[159]20.7510,[160]20.7156,[161]20.6933,[162]20.6919,[163]20.6888,[164]20.6776,[165]20.6457,[166]20.6640,[167]20.5865,[168]20.6244,[169]20.5905,[170]20.7223,[171]20.8003,[172]20.9036,[173]21.0220,[174]21.0494,[175]21.1419,[176]21.2358,[177]21.3374,[178]21.4125,[179]21.4766,[180]21.4987,[181]21.5897,[182]21.6743,[183]21.7517,[184]21.8452,[185]21.9027,[186]21.9152,[187]21.9161,[188]21.9197,[189]21.9325,[190]21.9950,[191]21.9994,[192]22.0111,[193]21.9959,[194]22.0498,[195]22.0791,[196]22.1070,[197]22.1126,[198]22.0359,[199]21.9933,[200]21.9822,[201]22.0239,[202]22.0552,[203]22.0936,[204]22.1102,[205]22.1070,[206]22.0753,[207]22.1288,[208]22.0502,[209]22.0384,[210]22.0359,[211]22.0294,[212]22.0293,[213]22.0352,[214]21.9896,[215]21.9233,[216]21.9237,[217]21.9211,[218]21.8888,[219]21.8206,[220]21.7721,[221]21.7291,[222]21.6832,[223]21.6840,[224]21.6924,[225]21.6291,[226]21.6344,[227]21.6103,[228]21.5412,[229]21.4626,[230]21.3965,[231]21.3488,[232]21.3126,[233]21.3157,[234]21.3245,[235]21.3092,[236]21.2571,[237]21.2251,[238]21.1498,[239]21.1158,[240]21.1384,[241]21.1164,[242]21.1388,[243]21.1551,[244]21.1914,[245]21.1898,[246]21.2528,[247]21.2464,[248]21.2597,[249]21.2918,[250]21.2921,[251]21.3524,[252]21.3532,[253]21.4114,[254]21.4443,[255]21.4527,[256]21.5011,[257]21.5064,[258]21.4563,[259]21.4095,[260]21.3767,[261]21.3424,[262]21.3237,[263]21.3084,[264]21.3121,[265]21.3597,[266]21.3778,[267]21.3867,[268]21.3616,[269]21.3590,[270]21.3706,[271]21.3349,[272]21.3461,[273]21.3413,[274]21.3375,[275]21.3317,[276]21.2760,[277]21.2487,[278]21.2665,[279]21.2616,[280]21.2509,[281]21.2411,[282]21.3022,[283]21.2067,[284]21.1140,[285]21.1266,[286]21.0675,[287]20.9947,[288]21.0046,[289]21.0094,[290]21.0840,[291]21.0854,[292]21.0816,[293]21.0876,[294]21.1332,[295]21.1834,[296]21.2348,[297]21.2813,[298]21.2731,[299]21.2310,[300]21.2274,[301]21.2055,[302]21.2115,[303]21.2002,[304]21.2248,[305]21.2049,[306]21.1904,[307]21.2004,[308]21.1824,[309]21.1701,[310]21.2034,[311]21.2048,[312]21.1631,[313]21.1551,[314]21.1727,[315]21.1327,[316]21.1858,[317]21.2425,[318]21.2384,[319]21.2156,[320]21.2326,[321]21.1876,[322]21.2074,[323]21.2403,[324]21.2719,[325]21.3061,[326]21.3208,[327]21.2833,[328]21.2773,[329]21.2178,[330]21.1958,[331]21.1529,[332]21.1411,[333]21.1369,[334]21.1032,[335]21.0601,[336]21.0439,[337]21.0588,[338]21.0863,[339]21.0642,[340]21.0282,[341]20.9997,[342]20.9972,[343]20.9920,[344]21.0157,[345]21.0277,[346]21.0308,[347]21.0211,[348]21.0273,[349]21.0146,[350]21.0137,[351]21.0374,[352]21.0433,[353]21.0422,[354]20.9922,[355]21.0332,[356]21.0492,[357]21.0573,[358]21.0332,[359]21.0300,[360]21.0249,[361]21.0306,[362]21.0538,[363]21.0496,[364]21.0874,[365]21.1187,[366]21.1855,[367]21.2364,[368]21.3032,[369]21.3587,[370]21.3972,[371]21.4472,[372]21.4966,[373]21.5189,[374]21.5460,[375]21.6030,[376]21.6413,[377]21.6625,[378]21.7020,[379]21.7385,[380]21.7808,[381]21.8087,[382]21.8401,[383]21.8687,[384]21.9156,[385]21.9761,[386]22.0142,[387]22.0138,[388]21.9930,[389]22.0315,[390]22.0804,[391]22.1212,[392]22.1211,[393]22.1193,[394]22.0964,[395]22.1005,[396]22.1194,[397]22.1251,[398]22.1315,[399]22.1332,[400]22.1738,[401]22.1700,[402]22.1750,[403]22.1513,[404]22.1472,[405]22.1263,[406]22.1241,[407]22.1273,[408]22.1381,[409]22.1401,[410]22.1409,[411]22.1779,[412]22.1880,[413]22.1974,[414]22.1907,[415]22.1877,[416]22.1766,[417]22.1865,[418]22.1968,[419]22.1904,[420]22.1815,[421]22.1904,[422]22.1600,[423]22.1716,[424]22.1810,[425]22.1895,[426]22.2079,[427]22.2402,[428]22.2850,[429]22.2877,[430]22.2656,[431]22.2366,[432]22.2372,[433]22.2298,[434]22.2154,[435]22.2309,[436]22.1685,[437]22.1611,[438]22.1643,[439]22.1375,[440]22.1642,[441]22.1727,[442]22.1608,[443]22.1509,[444]22.1624,[445]22.1287,[446]22.1361,[447]22.1311,[448]22.1194,[449]22.1087,[450]22.1250,[451]22.1166,[452]22.1099,[453]22.0895,[454]22.0717,[455]22.0762,[456]22.0704,[457]22.0846,[458]22.1195,[459]22.1263,[460]22.1220,[461]22.1274,[462]22.1368,[463]22.1623,[464]22.1644,[465]22.1722,[466]22.1737,[467]22.1859,[468]22.2093,[469]22.2253,[470]22.2470,[471]22.2382,[472]22.2597,[473]22.2184,[474]22.2138,[475]22.2243,[476]22.2392,[477]22.2269,[478]22.1880,[479]22.1843,[480]22.2077,[481]22.2251,[482]22.1877,[483]22.1983,[484]22.2181,[485]22.2193,[486]22.2069,[487]22.2225,[488]22.1880,[489]22.1618,[490]22.1587,[491]22.1319,[492]22.1234,[493]22.0854,[494]22.0753,[495]22.0393,[496]22.0372,[497]22.0583,[498]22.0769,[499]22.0557,[500]22.0616,[501]22.0515,[502]22.0583,[503]22.0997,[504]22.1196,[505]22.1220,[506]22.1069,[507]22.0853,[508]22.0915,[509]22.0674,[510]22.0689,[511]22.0787,[512]22.0671,[513]22.0818,[514]22.0786,[515]22.0753,[516]22.0754,[517]22.0715,[518]22.0569,[519]22.0447,[520]22.0294,[521]22.0251,[522]22.0053,[523]22.0059,[524]21.9984,[525]22.0062,[526]22.0155,[527]22.0031,[528]21.9935,[529]21.9811,[530]21.9698,[531]21.9731,[532]21.9684,[533]21.9804,[534]21.9795,[535]21.9798,[536]21.9671,[537]21.9864,[538]22.0029,[539]22.0021,[540]22.0364,[541]22.0407,[542]22.0125,[543]22.0131,[544]22.0289,[545]22.0185,[546]22.0092,[547]21.9937,[548]21.9670,[549]21.9769,[550]21.9308,[551]21.8877,[552]21.8558,[553]21.7534,[554]21.7396,[555]21.7492,[556]21.7461,[557]21.7358,[558]21.7215,[559]21.7234,[560]21.7371,[561]21.7410,[562]21.7698,[563]21.7927,[564]21.7849,[565]21.8041,[566]21.8120,[567]21.7911,[568]21.7909,[569]21.7737,[570]21.7922,[571]21.7949,[572]21.8214,[573]21.8298,[574]21.8363,[575]21.8324,[576]21.8549,[577]21.8532,[578]21.8604,[579]21.8933,[580]21.9284,[581]21.9386,[582]21.9731,[583]21.9531,[584]21.9645,
Final estimate: PPL = 21.9645 +/- 0.19168

llama_perf_context_print:        load time =    1589.06 ms
llama_perf_context_print: prompt eval time = 1222519.74 ms / 299008 tokens (    4.09 ms per token,   244.58 tokens per second)
llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_perf_context_print:       total time = 1282868.97 ms / 299009 tokens
llama_perf_context_print:    graphs reused =          0
状态: 完成
-------------------------------------------
执行时间: 2025-12-21 00:53:11
测试配置: [变体: 4] [模型: Qwen3-1.7B-Q4_0.gguf] [线程: 64] [第 1 次运行]
命令: ./llama-perplexity -m /root/wmw/unsloth_riscv/Qwen3-1.7B-GGUF/Qwen3-1.7B-Q4_0.gguf -t 64 -f /root/wmw/hzeng_vw/riscv_kernels/e2e/rvv10/../wiki.test.raw
build: 0 (unknown) with riscv64-unknown-linux-gnu-gcc () 15.1.0 for riscv64-unknown-linux-gnu
llama_model_loader: loaded meta data with 32 key-value pairs and 310 tensors from /root/wmw/unsloth_riscv/Qwen3-1.7B-GGUF/Qwen3-1.7B-Q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3-1.7B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3-1.7B
llama_model_loader: - kv   4:                       general.quantized_by str              = Unsloth
llama_model_loader: - kv   5:                         general.size_label str              = 1.7B
llama_model_loader: - kv   6:                           general.repo_url str              = https://huggingface.co/unsloth
llama_model_loader: - kv   7:                          qwen3.block_count u32              = 28
llama_model_loader: - kv   8:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   9:                     qwen3.embedding_length u32              = 2048
llama_model_loader: - kv  10:                  qwen3.feed_forward_length u32              = 6144
llama_model_loader: - kv  11:                 qwen3.attention.head_count u32              = 16
llama_model_loader: - kv  12:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  13:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  14:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  15:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  16:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  22:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 151654
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 2
llama_model_loader: - kv  28:                      quantize.imatrix.file str              = Qwen3-1.7B-GGUF/imatrix_unsloth.dat
llama_model_loader: - kv  29:                   quantize.imatrix.dataset str              = unsloth_calibration_Qwen3-1.7B.txt
llama_model_loader: - kv  30:             quantize.imatrix.entries_count i32              = 196
llama_model_loader: - kv  31:              quantize.imatrix.chunks_count i32              = 685
llama_model_loader: - type  f32:  113 tensors
llama_model_loader: - type q4_0:  193 tensors
llama_model_loader: - type q4_1:    3 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 1002.15 MiB (4.89 BPW) 
load: printing all EOG tokens:
load:   - 151643 ('<|endoftext|>')
load:   - 151645 ('<|im_end|>')
load:   - 151662 ('<|fim_pad|>')
load:   - 151663 ('<|repo_name|>')
load:   - 151664 ('<|file_sep|>')
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 2048
print_info: n_layer          = 28
print_info: n_head           = 16
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 2
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 6144
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: model type       = 1.7B
print_info: model params     = 1.72 B
print_info: general.name     = Qwen3-1.7B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 11 ','
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151654 '<|vision_pad|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors:   CPU_Mapped model buffer size =  1002.15 MiB
.............................................................................
llama_context: constructing llama_context
llama_context: n_seq_max     = 4
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 512
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (512) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     2.32 MiB
llama_kv_cache_unified:        CPU KV buffer size =   224.00 MiB
llama_kv_cache_unified: size =  224.00 MiB (   512 cells,  28 layers,  4/4 seqs), K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:        CPU compute buffer size =   300.75 MiB
llama_context: graph nodes  = 1098
llama_context: graph splits = 1
common_init_from_params: added <|endoftext|> logit bias = -inf
common_init_from_params: added <|im_end|> logit bias = -inf
common_init_from_params: added <|fim_pad|> logit bias = -inf
common_init_from_params: added <|repo_name|> logit bias = -inf
common_init_from_params: added <|file_sep|> logit bias = -inf
common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)

system_info: n_threads = 64 (n_threads_batch = 64) / 64 | CPU : RISCV_V = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 2940.96 ms
perplexity: calculating perplexity over 584 chunks, n_ctx=512, batch_size=2048, n_seq=4
perplexity: 19.82 seconds per pass - ETA 48.22 minutes
[1]151936.0000,[2]151936.0000,[3]151936.0000,[4]151936.0000,[5]151936.0000,[6]151936.0000,[7]151936.0000,[8]151936.0000,[9]151936.0000,[10]151936.0000,[11]151936.0000,[12]151936.0000,[13]151936.0000,[14]151936.0000,[15]151936.0000,[16]151936.0000,[17]151936.0000,[18]151936.0000,[19]151936.0000,[20]151936.0000,[21]151936.0000,[22]151936.0000,[23]151936.0000,[24]151936.0000,[25]151936.0000,[26]151936.0000,[27]151936.0000,[28]151936.0000,[29]151936.0000,[30]151936.0000,[31]151936.0000,[32]151936.0000,[33]151936.0000,[34]151936.0000,[35]151936.0000,[36]151936.0000,[37]151936.0000,[38]151936.0000,[39]151936.0000,[40]151936.0000,[41]151936.0000,[42]151936.0000,[43]151936.0000,[44]151936.0000,[45]151936.0000,[46]151936.0000,[47]151936.0000,[48]151936.0000,[49]151936.0000,[50]151936.0000,[51]151936.0000,[52]151936.0000,[53]151936.0000,[54]151936.0000,[55]151936.0000,[56]151936.0000,[57]151936.0000,[58]151936.0000,[59]151936.0000,[60]151936.0000,[61]151936.0000,[62]151936.0000,[63]151936.0000,[64]151936.0000,[65]151936.0000,[66]151936.0000,[67]151936.0000,[68]151936.0000,[69]151936.0000,[70]151936.0000,[71]151936.0000,[72]151936.0000,[73]151936.0000,[74]151936.0000,[75]151936.0000,[76]151936.0000,[77]151936.0000,[78]151936.0000,[79]151936.0000,[80]151936.0000,[81]151936.0000,[82]151936.0000,[83]151936.0000,[84]151936.0000,[85]151936.0000,[86]151936.0000,[87]151936.0000,[88]151936.0000,[89]151936.0000,[90]151936.0000,[91]151936.0000,[92]151936.0000,[93]151936.0000,[94]151936.0000,[95]151936.0000,[96]151936.0000,[97]151936.0000,[98]151936.0000,[99]151936.0000,[100]151936.0000,[101]151936.0000,[102]151936.0000,[103]151936.0000,[104]151936.0000,[105]151936.0000,[106]151936.0000,[107]151936.0000,[108]151936.0000,[109]151936.0000,[110]151936.0000,[111]151936.0000,[112]151936.0000,[113]151936.0000,[114]151936.0000,[115]151936.0000,[116]151936.0000,[117]151936.0000,[118]151936.0000,[119]151936.0000,[120]151936.0000,[121]151936.0000,[122]151936.0000,[123]151936.0000,[124]151936.0000,[125]151936.0000,[126]151936.0000,[127]151936.0000,[128]151936.0000,[129]151936.0000,[130]151936.0000,[131]151936.0000,[132]151936.0000,[133]151936.0000,[134]151936.0000,[135]151936.0000,[136]151936.0000,[137]151936.0000,[138]151936.0000,[139]151936.0000,[140]151936.0000,[141]151936.0000,[142]151936.0000,[143]151936.0000,[144]151936.0000,[145]151936.0000,[146]151936.0000,[147]151936.0000,[148]151936.0000,[149]151936.0000,[150]151936.0000,[151]151936.0000,[152]151936.0000,[153]151936.0000,[154]151936.0000,[155]151936.0000,[156]151936.0000,[157]151936.0000,[158]151936.0000,[159]151936.0000,[160]151936.0000,[161]151936.0000,[162]151936.0000,[163]151936.0000,[164]151936.0000,[165]151936.0000,[166]151936.0000,[167]151936.0000,[168]151936.0000,[169]151936.0000,[170]151936.0000,[171]151936.0000,[172]151936.0000,[173]151936.0000,[174]151936.0000,[175]151936.0000,[176]151936.0000,[177]151936.0000,[178]151936.0000,[179]151936.0000,[180]151936.0000,[181]151936.0000,[182]151936.0000,[183]151936.0000,[184]151936.0000,[185]151936.0000,[186]151936.0000,[187]151936.0000,[188]151936.0000,[189]151936.0000,[190]151936.0000,[191]151936.0000,[192]151936.0000,[193]151936.0000,[194]151936.0000,[195]151936.0000,[196]151936.0000,[197]151936.0000,[198]151936.0000,[199]151936.0000,[200]151936.0000,[201]151936.0000,[202]151936.0000,[203]151936.0000,[204]151936.0000,[205]151936.0000,[206]151936.0000,[207]151936.0000,[208]151936.0000,[209]151936.0000,[210]151936.0000,[211]151936.0000,[212]151936.0000,[213]151936.0000,[214]151936.0000,[215]151936.0000,[216]151936.0000,[217]151936.0000,[218]151936.0000,[219]151936.0000,[220]151936.0000,[221]151936.0000,[222]151936.0000,[223]151936.0000,[224]151936.0000,[225]151936.0000,[226]151936.0000,[227]151936.0000,[228]151936.0000,[229]151936.0000,[230]151936.0000,[231]151936.0000,[232]151936.0000,[233]151936.0000,[234]151936.0000,[235]151936.0000,[236]151936.0000,[237]151936.0000,[238]151936.0000,[239]151936.0000,[240]151936.0000,[241]151936.0000,[242]151936.0000,[243]151936.0000,[244]151936.0000,[245]151936.0000,[246]151936.0000,[247]151936.0000,[248]151936.0000,[249]151936.0000,[250]151936.0000,[251]151936.0000,[252]151936.0000,[253]151936.0000,[254]151936.0000,[255]151936.0000,[256]151936.0000,[257]151936.0000,[258]151936.0000,[259]151936.0000,[260]151936.0000,[261]151936.0000,[262]151936.0000,[263]151936.0000,[264]151936.0000,[265]151936.0000,[266]151936.0000,[267]151936.0000,[268]151936.0000,[269]151936.0000,[270]151936.0000,[271]151936.0000,[272]151936.0000,[273]151936.0000,[274]151936.0000,[275]151936.0000,[276]151936.0000,[277]151936.0000,[278]151936.0000,[279]151936.0000,[280]151936.0000,[281]151936.0000,[282]151936.0000,[283]151936.0000,[284]151936.0000,[285]151936.0000,[286]151936.0000,[287]151936.0000,[288]151936.0000,[289]151936.0000,[290]151936.0000,[291]151936.0000,[292]151936.0000,[293]151936.0000,[294]151936.0000,[295]151936.0000,[296]151936.0000,[297]151936.0000,[298]151936.0000,[299]151936.0000,[300]151936.0000,[301]151936.0000,[302]151936.0000,[303]151936.0000,[304]151936.0000,[305]151936.0000,[306]151936.0000,[307]151936.0000,[308]151936.0000,[309]151936.0000,[310]151936.0000,[311]151936.0000,[312]151936.0000,[313]151936.0000,[314]151936.0000,[315]151936.0000,[316]151936.0000,[317]151936.0000,[318]151936.0000,[319]151936.0000,[320]151936.0000,[321]151936.0000,[322]151936.0000,[323]151936.0000,[324]151936.0000,[325]151936.0000,[326]151936.0000,[327]151936.0000,[328]151936.0000,[329]151936.0000,[330]151936.0000,[331]151936.0000,[332]151936.0000,[333]151936.0000,[334]151936.0000,[335]151936.0000,[336]151936.0000,[337]151936.0000,[338]151936.0000,[339]151936.0000,[340]151936.0000,[341]151936.0000,[342]151936.0000,[343]151936.0000,[344]151936.0000,[345]151936.0000,[346]151936.0000,[347]151936.0000,[348]151936.0000,[349]151936.0000,[350]151936.0000,[351]151936.0000,[352]151936.0000,[353]151936.0000,[354]151936.0000,[355]151936.0000,[356]151936.0000,[357]151936.0000,[358]151936.0000,[359]151936.0000,[360]151936.0000,[361]151936.0000,[362]151936.0000,[363]151936.0000,[364]151936.0000,[365]151936.0000,[366]151936.0000,[367]151936.0000,[368]151936.0000,[369]151936.0000,[370]151936.0000,[371]151936.0000,[372]151936.0000,[373]151936.0000,[374]151936.0000,[375]151936.0000,[376]151936.0000,[377]151936.0000,[378]151936.0000,[379]151936.0000,[380]151936.0000,[381]151936.0000,[382]151936.0000,[383]151936.0000,[384]151936.0000,[385]151936.0000,[386]151936.0000,[387]151936.0000,[388]151936.0000,[389]151936.0000,[390]151936.0000,[391]151936.0000,[392]151936.0000,[393]151936.0000,[394]151936.0000,[395]151936.0000,[396]151936.0000,[397]151936.0000,[398]151936.0000,[399]151936.0000,[400]151936.0000,[401]151936.0000,[402]151936.0000,[403]151936.0000,[404]151936.0000,[405]151936.0000,[406]151936.0000,[407]151936.0000,[408]151936.0000,[409]151936.0000,[410]151936.0000,[411]151936.0000,[412]151936.0000,[413]151936.0000,[414]151936.0000,[415]151936.0000,[416]151936.0000,[417]151936.0000,[418]151936.0000,[419]151936.0000,[420]151936.0000,[421]151936.0000,[422]151936.0000,[423]151936.0000,[424]151936.0000,[425]151936.0000,[426]151936.0000,[427]151936.0000,[428]151936.0000,[429]151936.0000,[430]151936.0000,[431]151936.0000,[432]151936.0000,[433]151936.0000,[434]151936.0000,[435]151936.0000,[436]151936.0000,[437]151936.0000,[438]151936.0000,[439]151936.0000,[440]151936.0000,[441]151936.0000,[442]151936.0000,[443]151936.0000,[444]151936.0000,[445]151936.0000,[446]151936.0000,[447]151936.0000,[448]151936.0000,[449]151936.0000,[450]151936.0000,[451]151936.0000,[452]151936.0000,[453]151936.0000,[454]151936.0000,[455]151936.0000,[456]151936.0000,[457]151936.0000,[458]151936.0000,[459]151936.0000,[460]151936.0000,[461]151936.0000,[462]151936.0000,[463]151936.0000,[464]151936.0000,[465]151936.0000,[466]151936.0000,[467]151936.0000,[468]151936.0000,[469]151936.0000,[470]151936.0000,[471]151936.0000,[472]151936.0000,[473]151936.0000,[474]151936.0000,[475]151936.0000,[476]151936.0000,[477]151936.0000,[478]151936.0000,[479]151936.0000,[480]151936.0000,[481]151936.0000,[482]151936.0000,[483]151936.0000,[484]151936.0000,[485]151936.0000,[486]151936.0000,[487]151936.0000,[488]151936.0000,[489]151936.0000,[490]151936.0000,[491]151936.0000,[492]151936.0000,[493]151936.0000,[494]151936.0000,[495]151936.0000,[496]151936.0000,[497]151936.0000,[498]151936.0000,[499]151936.0000,[500]151936.0000,[501]151936.0000,[502]151936.0000,[503]151936.0000,[504]151936.0000,[505]151936.0000,[506]151936.0000,[507]151936.0000,[508]151936.0000,[509]151936.0000,[510]151936.0000,[511]151936.0000,[512]151936.0000,[513]151936.0000,[514]151936.0000,[515]151936.0000,[516]151936.0000,[517]151936.0000,[518]151936.0000,[519]151936.0000,[520]151936.0000,[521]151936.0000,[522]151936.0000,[523]151936.0000,[524]151936.0000,[525]151936.0000,[526]151936.0000,[527]151936.0000,[528]151936.0000,[529]151936.0000,[530]151936.0000,[531]151936.0000,[532]151936.0000,[533]151936.0000,[534]151936.0000,[535]151936.0000,[536]151936.0000,[537]151936.0000,[538]151936.0000,[539]151936.0000,[540]151936.0000,[541]151936.0000,[542]151936.0000,[543]151936.0000,[544]151936.0000,[545]151936.0000,[546]151936.0000,[547]151936.0000,[548]151936.0000,[549]151936.0000,[550]151936.0000,[551]151936.0000,[552]151936.0000,[553]151936.0000,[554]151936.0000,[555]151936.0000,[556]151936.0000,[557]151936.0000,[558]151936.0000,[559]151936.0000,[560]151936.0000,[561]151936.0000,[562]151936.0000,[563]151936.0000,[564]151936.0000,[565]151936.0000,[566]151936.0000,[567]151936.0000,[568]151936.0000,[569]151936.0000,[570]151936.0000,[571]151936.0000,[572]151936.0000,[573]151936.0000,[574]151936.0000,[575]151936.0000,[576]151936.0000,[577]151936.0000,[578]151936.0000,[579]151936.0000,[580]151936.0000,[581]151936.0000,[582]151936.0000,[583]151936.0000,[584]151936.0000,
Unexpected negative standard deviation of log(prob)

llama_perf_context_print:        load time =    1741.83 ms
llama_perf_context_print: prompt eval time = 2683557.83 ms / 299008 tokens (    8.97 ms per token,   111.42 tokens per second)
llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_perf_context_print:       total time = 2744009.89 ms / 299009 tokens
llama_perf_context_print:    graphs reused =          0
状态: 完成
-------------------------------------------
执行时间: 2025-12-21 01:38:58
测试配置: [变体: 4] [模型: Qwen3-1.7B-Q4_0.gguf.aria2] [线程: 64] [第 1 次运行]
命令: ./llama-perplexity -m /root/wmw/unsloth_riscv/Qwen3-1.7B-GGUF/Qwen3-1.7B-Q4_0.gguf.aria2 -t 64 -f /root/wmw/hzeng_vw/riscv_kernels/e2e/rvv10/../wiki.test.raw
build: 0 (unknown) with riscv64-unknown-linux-gnu-gcc () 15.1.0 for riscv64-unknown-linux-gnu
gguf_init_from_file_impl: invalid magic characters: '????', expected 'GGUF'
llama_model_load: error loading model: llama_model_loader: failed to load model from /root/wmw/unsloth_riscv/Qwen3-1.7B-GGUF/Qwen3-1.7B-Q4_0.gguf.aria2
llama_model_load_from_file_impl: failed to load model
common_init_from_params: failed to load model '/root/wmw/unsloth_riscv/Qwen3-1.7B-GGUF/Qwen3-1.7B-Q4_0.gguf.aria2'
main: unable to load model
状态: 失败 (请查看日志详情)
-------------------------------------------
执行时间: 2025-12-21 01:38:58
测试配置: [变体: 4] [模型: Qwen3-1.7B-Q8_0.gguf] [线程: 64] [第 1 次运行]
命令: ./llama-perplexity -m /root/wmw/unsloth_riscv/Qwen3-1.7B-GGUF/Qwen3-1.7B-Q8_0.gguf -t 64 -f /root/wmw/hzeng_vw/riscv_kernels/e2e/rvv10/../wiki.test.raw
build: 0 (unknown) with riscv64-unknown-linux-gnu-gcc () 15.1.0 for riscv64-unknown-linux-gnu
llama_model_loader: loaded meta data with 32 key-value pairs and 310 tensors from /root/wmw/unsloth_riscv/Qwen3-1.7B-GGUF/Qwen3-1.7B-Q8_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen3-1.7B
llama_model_loader: - kv   3:                           general.basename str              = Qwen3-1.7B
llama_model_loader: - kv   4:                       general.quantized_by str              = Unsloth
llama_model_loader: - kv   5:                         general.size_label str              = 1.7B
llama_model_loader: - kv   6:                           general.repo_url str              = https://huggingface.co/unsloth
llama_model_loader: - kv   7:                          qwen3.block_count u32              = 28
llama_model_loader: - kv   8:                       qwen3.context_length u32              = 40960
llama_model_loader: - kv   9:                     qwen3.embedding_length u32              = 2048
llama_model_loader: - kv  10:                  qwen3.feed_forward_length u32              = 6144
llama_model_loader: - kv  11:                 qwen3.attention.head_count u32              = 16
llama_model_loader: - kv  12:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  13:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  14:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  15:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  16:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  22:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 151654
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  26:               general.quantization_version u32              = 2
llama_model_loader: - kv  27:                          general.file_type u32              = 7
llama_model_loader: - kv  28:                      quantize.imatrix.file str              = Qwen3-1.7B-GGUF/imatrix_unsloth.dat
llama_model_loader: - kv  29:                   quantize.imatrix.dataset str              = unsloth_calibration_Qwen3-1.7B.txt
llama_model_loader: - kv  30:             quantize.imatrix.entries_count i32              = 196
llama_model_loader: - kv  31:              quantize.imatrix.chunks_count i32              = 685
llama_model_loader: - type  f32:  113 tensors
llama_model_loader: - type q8_0:  197 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q8_0
print_info: file size   = 1.70 GiB (8.50 BPW) 
load: printing all EOG tokens:
load:   - 151643 ('<|endoftext|>')
load:   - 151645 ('<|im_end|>')
load:   - 151662 ('<|fim_pad|>')
load:   - 151663 ('<|repo_name|>')
load:   - 151664 ('<|file_sep|>')
load: special tokens cache size = 26
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 40960
print_info: n_embd           = 2048
print_info: n_layer          = 28
print_info: n_head           = 16
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 2
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 6144
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 40960
print_info: rope_finetuned   = unknown
print_info: model type       = 1.7B
print_info: model params     = 1.72 B
print_info: general.name     = Qwen3-1.7B
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 11 ','
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151654 '<|vision_pad|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors:   CPU_Mapped model buffer size =  1743.77 MiB
...................................................................................
llama_context: constructing llama_context
llama_context: n_seq_max     = 4
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 512
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = 0
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (512) < n_ctx_train (40960) -- the full capacity of the model will not be utilized
llama_context:        CPU  output buffer size =     2.32 MiB
llama_kv_cache_unified:        CPU KV buffer size =   224.00 MiB
llama_kv_cache_unified: size =  224.00 MiB (   512 cells,  28 layers,  4/4 seqs), K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:        CPU compute buffer size =   300.75 MiB
llama_context: graph nodes  = 1098
llama_context: graph splits = 1
common_init_from_params: added <|endoftext|> logit bias = -inf
common_init_from_params: added <|im_end|> logit bias = -inf
common_init_from_params: added <|fim_pad|> logit bias = -inf
common_init_from_params: added <|repo_name|> logit bias = -inf
common_init_from_params: added <|file_sep|> logit bias = -inf
common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)

system_info: n_threads = 64 (n_threads_batch = 64) / 64 | CPU : RISCV_V = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 2866.54 ms
perplexity: calculating perplexity over 584 chunks, n_ctx=512, batch_size=2048, n_seq=4
perplexity: 18.12 seconds per pass - ETA 44.07 minutes
[1]11.4300,[2]15.1327,[3]14.8006,[4]15.4354,[5]15.4249,[6]15.5606,[7]15.9786,[8]16.6940,[9]17.8151,[10]18.3606,[11]18.4929,[12]18.5778,[13]19.2155,[14]18.8063,[15]18.7245,[16]19.1096,[17]18.0380,[18]18.2215,[19]18.0615,[20]18.0681,[21]17.7998,[22]17.8383,[23]17.1712,[24]16.2608,[25]15.8719,[26]15.4447,[27]14.9863,[28]14.8290,[29]14.8865,[30]14.8325,[31]14.6325,[32]14.6189,[33]14.3625,[34]14.6623,[35]14.7609,[36]14.9975,[37]15.2270,[38]15.3233,[39]15.2859,[40]15.3908,[41]15.3581,[42]15.2717,[43]15.4074,[44]15.4618,[45]15.4464,[46]15.3920,[47]15.6972,[48]15.8175,[49]15.7314,[50]15.8216,[51]15.8929,[52]15.9329,[53]16.0170,[54]16.0576,[55]16.0570,[56]16.1825,[57]16.0928,[58]16.1493,[59]16.2282,[60]16.3110,[61]16.3507,[62]16.4003,[63]16.4805,[64]16.6034,[65]16.7950,[66]16.9232,[67]17.0663,[68]16.9766,[69]16.9580,[70]16.9225,[71]16.9411,[72]17.0394,[73]17.0798,[74]17.0801,[75]16.9612,[76]16.9308,[77]16.9695,[78]16.9646,[79]16.8308,[80]16.8353,[81]16.7610,[82]16.8395,[83]16.8022,[84]16.7599,[85]16.8150,[86]16.9767,[87]17.0770,[88]17.0476,[89]17.0261,[90]16.9853,[91]17.0513,[92]17.0083,[93]17.1305,[94]17.1734,[95]17.1222,[96]17.1024,[97]17.0654,[98]17.0713,[99]17.0400,[100]17.1664,[101]17.2276,[102]17.1905,[103]17.2305,[104]17.1536,[105]17.1839,[106]17.1507,[107]17.1875,[108]17.2561,[109]17.3092,[110]17.3926,[111]17.5914,[112]17.6175,[113]17.4810,[114]17.5370,[115]17.5777,[116]17.5137,[117]17.4921,[118]17.4532,[119]17.3347,[120]17.3858,[121]17.3428,[122]17.3596,[123]17.3324,[124]17.2391,[125]17.2197,[126]17.2196,[127]17.1841,[128]17.1475,[129]17.1062,[130]17.0696,[131]16.9700,[132]16.8833,[133]16.8343,[134]16.8438,[135]16.8217,[136]16.7809,[137]16.7026,[138]16.6370,[139]16.6492,[140]16.6269,[141]16.6120,[142]16.6225,[143]16.6482,[144]16.7220,[145]16.6800,[146]16.5952,[147]16.5074,[148]16.4328,[149]16.4156,[150]16.3315,[151]16.3213,[152]16.3073,[153]16.2970,[154]16.2187,[155]16.2674,[156]16.2037,[157]16.1636,[158]16.1122,[159]16.0727,[160]16.0075,[161]15.9776,[162]15.9765,[163]15.9826,[164]15.9927,[165]15.9645,[166]15.9474,[167]15.9053,[168]15.9429,[169]15.9215,[170]15.9971,[171]16.0658,[172]16.1364,[173]16.2152,[174]16.2486,[175]16.3244,[176]16.3895,[177]16.4793,[178]16.5498,[179]16.6060,[180]16.6106,[181]16.6693,[182]16.7225,[183]16.7667,[184]16.8272,[185]16.8705,[186]16.8916,[187]16.9007,[188]16.9046,[189]16.9247,[190]16.9694,[191]16.9744,[192]17.0018,[193]17.0071,[194]17.0433,[195]17.0628,[196]17.0736,[197]17.0786,[198]17.0261,[199]17.0045,[200]16.9897,[201]17.0312,[202]17.0583,[203]17.0930,[204]17.1131,[205]17.1135,[206]17.1082,[207]17.1591,[208]17.1236,[209]17.1245,[210]17.1192,[211]17.1146,[212]17.1133,[213]17.1298,[214]17.0986,[215]17.0582,[216]17.0523,[217]17.0463,[218]17.0410,[219]16.9786,[220]16.9539,[221]16.9143,[222]16.8772,[223]16.8671,[224]16.8806,[225]16.8377,[226]16.8270,[227]16.8010,[228]16.7383,[229]16.6805,[230]16.6340,[231]16.5915,[232]16.5509,[233]16.5496,[234]16.5629,[235]16.5497,[236]16.5169,[237]16.4814,[238]16.4273,[239]16.3926,[240]16.3942,[241]16.3795,[242]16.3882,[243]16.4079,[244]16.4325,[245]16.4386,[246]16.4913,[247]16.4829,[248]16.4986,[249]16.5199,[250]16.5175,[251]16.5464,[252]16.5590,[253]16.6015,[254]16.6255,[255]16.6208,[256]16.6562,[257]16.6578,[258]16.6337,[259]16.5875,[260]16.5592,[261]16.5175,[262]16.5011,[263]16.5111,[264]16.5173,[265]16.5476,[266]16.5792,[267]16.5860,[268]16.5778,[269]16.5856,[270]16.6090,[271]16.5754,[272]16.5886,[273]16.5886,[274]16.5913,[275]16.5942,[276]16.5627,[277]16.5496,[278]16.5603,[279]16.5595,[280]16.5456,[281]16.5504,[282]16.5962,[283]16.5407,[284]16.4754,[285]16.4886,[286]16.4402,[287]16.3875,[288]16.3824,[289]16.3889,[290]16.4407,[291]16.4444,[292]16.4375,[293]16.4398,[294]16.4724,[295]16.5279,[296]16.5997,[297]16.6348,[298]16.6272,[299]16.5990,[300]16.5989,[301]16.5939,[302]16.5994,[303]16.5859,[304]16.6092,[305]16.5995,[306]16.5882,[307]16.6011,[308]16.5890,[309]16.5814,[310]16.6023,[311]16.6032,[312]16.5811,[313]16.5721,[314]16.5802,[315]16.5518,[316]16.5839,[317]16.6315,[318]16.6351,[319]16.6149,[320]16.6194,[321]16.5890,[322]16.6115,[323]16.6379,[324]16.6656,[325]16.6889,[326]16.7037,[327]16.6793,[328]16.6794,[329]16.6373,[330]16.6315,[331]16.6080,[332]16.6065,[333]16.6047,[334]16.5836,[335]16.5492,[336]16.5450,[337]16.5569,[338]16.5704,[339]16.5665,[340]16.5602,[341]16.5358,[342]16.5343,[343]16.5213,[344]16.5344,[345]16.5346,[346]16.5323,[347]16.5086,[348]16.5219,[349]16.5182,[350]16.5159,[351]16.5290,[352]16.5324,[353]16.5273,[354]16.4984,[355]16.5230,[356]16.5397,[357]16.5529,[358]16.5337,[359]16.5282,[360]16.5428,[361]16.5524,[362]16.5599,[363]16.5524,[364]16.6343,[365]16.6644,[366]16.7188,[367]16.7564,[368]16.8074,[369]16.8398,[370]16.8637,[371]16.8991,[372]16.9344,[373]16.9513,[374]16.9752,[375]17.0147,[376]17.0483,[377]17.0648,[378]17.0876,[379]17.1064,[380]17.1322,[381]17.1559,[382]17.1776,[383]17.1929,[384]17.2252,[385]17.2728,[386]17.3177,[387]17.3212,[388]17.3107,[389]17.3374,[390]17.3787,[391]17.4085,[392]17.3981,[393]17.3912,[394]17.3776,[395]17.3906,[396]17.4016,[397]17.4023,[398]17.4135,[399]17.4144,[400]17.4476,[401]17.4564,[402]17.4627,[403]17.4395,[404]17.4295,[405]17.4125,[406]17.4071,[407]17.4118,[408]17.4219,[409]17.4195,[410]17.4227,[411]17.4470,[412]17.4597,[413]17.4618,[414]17.4539,[415]17.4361,[416]17.4291,[417]17.4359,[418]17.4413,[419]17.4445,[420]17.4318,[421]17.4330,[422]17.4022,[423]17.4132,[424]17.4174,[425]17.4218,[426]17.4333,[427]17.4626,[428]17.4969,[429]17.5059,[430]17.4896,[431]17.4729,[432]17.4854,[433]17.4817,[434]17.4661,[435]17.4819,[436]17.4431,[437]17.4365,[438]17.4334,[439]17.4156,[440]17.4335,[441]17.4384,[442]17.4229,[443]17.4119,[444]17.4210,[445]17.3940,[446]17.4185,[447]17.4182,[448]17.3999,[449]17.3877,[450]17.3896,[451]17.3793,[452]17.3691,[453]17.3525,[454]17.3363,[455]17.3419,[456]17.3330,[457]17.3448,[458]17.3688,[459]17.3702,[460]17.3741,[461]17.3733,[462]17.3776,[463]17.3954,[464]17.3958,[465]17.4003,[466]17.4033,[467]17.4159,[468]17.4325,[469]17.4421,[470]17.4494,[471]17.4336,[472]17.4493,[473]17.4215,[474]17.4194,[475]17.4262,[476]17.4432,[477]17.4304,[478]17.4053,[479]17.4121,[480]17.4306,[481]17.4518,[482]17.4221,[483]17.4403,[484]17.4571,[485]17.4594,[486]17.4521,[487]17.4593,[488]17.4307,[489]17.4083,[490]17.4045,[491]17.3860,[492]17.3784,[493]17.3465,[494]17.3329,[495]17.3060,[496]17.3039,[497]17.3243,[498]17.3324,[499]17.3143,[500]17.3113,[501]17.3028,[502]17.3071,[503]17.3352,[504]17.3513,[505]17.3459,[506]17.3445,[507]17.3262,[508]17.3298,[509]17.3084,[510]17.3146,[511]17.3268,[512]17.3239,[513]17.3348,[514]17.3334,[515]17.3409,[516]17.3387,[517]17.3391,[518]17.3370,[519]17.3303,[520]17.3208,[521]17.3201,[522]17.3031,[523]17.3051,[524]17.2994,[525]17.3064,[526]17.3118,[527]17.3064,[528]17.3039,[529]17.2998,[530]17.2858,[531]17.2835,[532]17.2742,[533]17.2788,[534]17.2741,[535]17.2810,[536]17.2785,[537]17.2970,[538]17.3099,[539]17.3124,[540]17.3423,[541]17.3475,[542]17.3225,[543]17.3269,[544]17.3486,[545]17.3419,[546]17.3333,[547]17.3168,[548]17.3007,[549]17.3039,[550]17.2722,[551]17.2462,[552]17.2206,[553]17.1469,[554]17.1364,[555]17.1506,[556]17.1511,[557]17.1414,[558]17.1276,[559]17.1311,[560]17.1434,[561]17.1462,[562]17.1723,[563]17.1876,[564]17.1784,[565]17.1931,[566]17.1960,[567]17.1775,[568]17.1731,[569]17.1561,[570]17.1588,[571]17.1604,[572]17.1819,[573]17.1912,[574]17.1946,[575]17.1957,[576]17.2172,[577]17.2160,[578]17.2305,[579]17.2504,[580]17.2778,[581]17.2831,[582]17.3061,[583]17.2859,[584]17.2915,
Final estimate: PPL = 17.2915 +/- 0.16206

llama_perf_context_print:        load time =    1841.27 ms
llama_perf_context_print: prompt eval time = 2463021.89 ms / 299008 tokens (    8.24 ms per token,   121.40 tokens per second)
llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_perf_context_print:       total time = 2523000.01 ms / 299009 tokens
llama_perf_context_print:    graphs reused =          0
状态: 完成
===========================================
所有测试已结束。
